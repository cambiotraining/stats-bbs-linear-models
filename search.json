[
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "BBS - Linear models",
    "section": "Overview",
    "text": "Overview\nThese materials are part of the NST Part II BBS Bioinformatics course at the University of Cambridge, UK.\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nTBD\n\n\n\n\nTarget Audience\nUndergraduate students on the NST Part II BBS Bioinformatics course.\n\n\nPrerequisites\nProficient in use of R.\n\n\n\nExercises\nExercises in these materials are labelled according to their level of difficulty:\n\n\n\n\n\n\n\nLevel\nDescription\n\n\n\n\n  \nExercises in level 1 are simpler and designed to get you familiar with the concepts and syntax covered in the course.\n\n\n  \nExercises in level 2 combine different concepts together and apply it to a given task.\n\n\n  \nExercises in level 3 require going beyond the concepts and syntax introduced to solve new problems."
  },
  {
    "objectID": "index.html#authors",
    "href": "index.html#authors",
    "title": "BBS - Linear models",
    "section": "Authors",
    "text": "Authors\n\nAbout the author(s):\n\nVicki Hodgson  \nAffiliation: Bioinformatics Training Facility, University of Cambridge\nRoles: writing - review & editing; conceptualisation; coding\nMartin van Rongen  \nAffiliation: Bioinformatics Training Facility, University of Cambridge\nRoles: writing - review & editing; conceptualisation; coding"
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "BBS - Linear models",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\n\nList any other sources of materials that were used.\nOr other people that may have advised during the material development (but are not authors)."
  },
  {
    "objectID": "setup.html#data",
    "href": "setup.html#data",
    "title": "2  Data & Setup",
    "section": "Data",
    "text": "Data\nThe data used in these materials is provided as a zip file. Download and unzip the folder to your Desktop to follow along with the materials.\n\n  Download"
  },
  {
    "objectID": "setup.html#software",
    "href": "setup.html#software",
    "title": "2  Data & Setup",
    "section": "Software",
    "text": "Software\n\nR and RStudio\n\n\nWindows\nDownload and install all these using default options:\n\nR\nRTools\nRStudio\n\n\n\nMac OS\nDownload and install all these using default options:\n\nR\nRStudio\n\n\n\nLinux\n\nGo to the R installation folder and look at the instructions for your distribution.\nDownload the RStudio installer for your distribution and install it using your package manager."
  },
  {
    "objectID": "materials/research-questions.html#criteria-for-a-good-research-question",
    "href": "materials/research-questions.html#criteria-for-a-good-research-question",
    "title": "3  Research questions",
    "section": "3.1 Criteria for a good research question",
    "text": "3.1 Criteria for a good research question\nFocused on a single topic\nThe scope of the question is important. If your research question is too broad, or tries to tackle multiple topics at once, you’ll struggle to design an experiment that can actually answer it. Even if you succeed, your experiment will necessarily be more complicated and take more time and resources to conduct, and may require compromising on some of your\nUses specific, well-defined concepts\nOr, alternatively, be prepared with your own definition of the concepts/variables that you’re studying! It should be clear to you in advance of collecting data precisely what your variables of interest are and how you’re going to measure them. This should also be clear to other researchers who may be reading your work or using your data, since your experiment should also be repeatable by others. This is all particularly true if you’re studying something that is a little abstract or can be defined in multiple ways. (See the section on operationalising variables for more on this.)\nRelevant and addressing a current research need\nYour research question should be focused on something that is of use to the scientific community or the public more broadly, either by increasing our understanding of basic science or by promoting translation to clinical or industry settings. It should, ideally, be motivated by the existing literature, or more specifically, by gaps or outstanding questions in the existing literature. We could get very philosophical here about the role of researchers in society, but the focus of this course is much more practical than that, and from a practical perspective (and a cynical one): these are the studies that get funded and published!\nResearchable\nThis perhaps seems obvious, but let’s unpack it a bit. It should be possible to answer your research question either by collecting original data, or using credible existing sources (e.g., a meta-analysis), and crucially, your research question shouldn’t depend on any subjective opinions or value judgements. For instance, it’s a good idea to avoid words like “best” or “worst” in your questions.\nOriginal\nIn short: what you’re trying to find out, shouldn’t be possible to find out elsewhere already. An important caveat here is that this does not mean that attempts to replicate prior experiments aren’t valid - they absolutely are. The research question for a replication study is different to the earlier study, because it is specifically asking whether a previously observed effect or phenomenon can be replicated.\nComplex and insightful\nBy “complex”, we don’t necessarily mean that the research question has to be difficult or convoluted for the sake of it - but it should have more than a simple yes or no answer. There are situations where a yes or no answer forms part of your overall conclusion - e.g., can tartigrades survive in the vacuum of space? Will compound X react with compound Y? Can humans remember their dreams? - but in all of these situations, a well-designed experiment should also give additional information, like the degree to which a particular phenomenon is observed, or details about the conditions under which it occurs. If you’re going to the effort of designing and conducting an experiment, you may as well be getting some detailed insight!"
  },
  {
    "objectID": "materials/research-questions.html#some-examples",
    "href": "materials/research-questions.html#some-examples",
    "title": "3  Research questions",
    "section": "3.2 Some examples",
    "text": "3.2 Some examples\nBelow are a list of research questions that don’t quite meet all of the above criteria. Have a read through them, and see if you can identify the issues, and how you might refine the question to improve it.\n\nDoes owning a dog make you more likeable?\nWhy do programmers make typos in their code?\nHow does nuclear radiation affect humans?\nDo human beings have a soul that continues to exist after death?\nIs bureaucracy in the University bad or good?\nDoes spending a lot of time on social media affect children’s development?\nCan pet parrots live more than 50 years?"
  },
  {
    "objectID": "materials/research-questions.html#summary",
    "href": "materials/research-questions.html#summary",
    "title": "3  Research questions",
    "section": "3.3 Summary",
    "text": "3.3 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nWithin the statistical inference and hypothesis testing framework, it is essential to set a good research question\nSetting the question is the first step to designing the experiment that will answer it\nA good research question should be focused, researchable, relevant, feasible, original and complex"
  },
  {
    "objectID": "materials/glm-intro-lm.html#data",
    "href": "materials/glm-intro-lm.html#data",
    "title": "4  Linear models",
    "section": "4.1 Data",
    "text": "4.1 Data\nFor this example, we’ll be using the several data sets about Darwin’s finches. They are part of a long-term genetic and phenotypic study on the evolution of several species of finches. The exact details are less important for now, but there are data on multiple species where several phenotypic characteristics were measured (see Figure 4.1).\n\n\n\nFigure 4.1: Finches phenotypes (courtesy of HHMI BioInteractive)"
  },
  {
    "objectID": "materials/glm-intro-lm.html#exploring-data",
    "href": "materials/glm-intro-lm.html#exploring-data",
    "title": "4  Linear models",
    "section": "4.2 Exploring data",
    "text": "4.2 Exploring data\nIt’s always a good idea to explore your data visually. Here we are focussing on the (potential) relationship between beak length (blength) and beak depth (bdepth).\nOur data contains measurements from two years (year) and two species (species). If we plot beak depth against beak length, colour our data by species and look across the two time points (1975 and 2012), we get the following graph:\n\n\n\n\n\nFigure 4.2: Beak depth and length for G. fortis and G. scandens\n\n\n\n\nIt seems that there is a potential linear relationship between beak depth and beak length. There are some differences between the two species and two time points with, what seems, more spread in the data in 2012. The data for both species also seem to be less separated than in 1975.\nFor the current purpose, we’ll focus on one group of data: those of Geospiza fortis in 1975."
  },
  {
    "objectID": "materials/glm-intro-lm.html#linear-model",
    "href": "materials/glm-intro-lm.html#linear-model",
    "title": "4  Linear models",
    "section": "4.3 Linear model",
    "text": "4.3 Linear model\nLet’s look at the G. fortis data more closely, assuming that the have a linear relationship. We can visualise that as follows:\n\n\n\n\n\nFigure 4.3: Beak depth vs beak length G. fortis (1975)\n\n\n\n\nIf you recall from the Core statistics linear regression session, what we’re doing here is assuming that there is a linear relationship between the response variable (in this case bdepth) and predictor variable (here, blength).\nWe can get more information on this linear relationship by defining a linear model, which has the form of:\n\\[\nY = \\beta_0 + \\beta_1X\n\\]\nwhere \\(Y\\) is the response variable (the thing we’re interested in), \\(X\\) the predictor variable and \\(\\beta_0\\) and \\(\\beta_1\\) are model coefficients. More explicitly for our data, we get:\n\\[\nbeak\\ depth = \\beta_0 + \\beta_1 \\times beak\\ length\n\\]\nBut how do we find this model? The computer uses a method called least-squares regression. There are several steps involved in this.\n\n4.3.1 Line of best fit\nThe computer tries to find the line of best fit. This is a linear line that best describes your data. We could draw a linear line through our cloud of data points in many ways, but the least-squares method converges to a single solution, where the sum of squared residual deviations is at its smallest.\nTo understand this a bit better, it’s helpful to realise that each data point consists of a fitted value (the beak depth predicted by the model at a given beak length), combined with the error. The error is the difference between the fitted value and the data point.\nLet’s look at this for one of the observations, for example finch 473:\n\n\n\n\n\nFigure 4.4: Beak depth vs beak length (finch 473, 1975)\n\n\n\n\nObtaining the fitted value and error happens for each data point. All these residuals are then squared (to ensure that they are positive), and added together. This is the so-called sum-of-squares.\nYou can imagine that if you draw a line through the data that doesn’t fit the data all that well, the error associated with each data point increases. The sum-of-squares then also increases. Equally, the closer the data are to the line, the smaller the error. This results in a smaller sum-of-squares.\nThe linear line where the sum-of-squares is at its smallest, is called the line of best fit. This line acts as a model for our data.\nFor finch 473 we have the following values:\n\nthe observed beak depth is 9.5 mm\nthe observed beak length is 10.5 mm\nthe fitted value is 9.11 mm\nthe error is 0.39 mm\n\n\n\n4.3.2 Linear regression\nOnce we have the line of best fit, we can perform a linear regression. What we’re doing with the regression, is asking:\n\nIs the line of best fit a better predictor of our data than a horizontal line across the average value?\n\nVisually, that looks like this:\n\n\n\n\n\nFigure 4.5: Regression: is the slope different from zero?\n\n\n\n\nWhat we’re actually testing is whether the slope (\\(\\beta_1\\)) of the line of best fit is any different from zero.\nTo find the answer, we perform an ANOVA. This gives us a p-value of 1.68e-78.\nNeedless to say, this p-value is extremely small, and definitely smaller than any common significance threshold, such as \\(p &lt; 0.05\\). This suggests that beak length is a statistically significant predictor of beak depth.\nIn this case the model has an intercept (\\(\\beta_0\\)) of -0.34 and a slope (\\(\\beta_1\\)) of 0.9. We can use this to write a simple linear equation, describing our data. Remember that this takes the form of:\n\\[\nY = \\beta_0 + \\beta_1X\n\\]\nwhich in our case is\n\\[\nbeak\\ depth = \\beta_0 + \\beta_1 \\times beak\\ length\n\\]\nand gives us\n\\[\nbeak\\ depth = -0.34 + 0.90 \\times beak\\ length\n\\]\n\n\n4.3.3 Assumptions\nIn example above we just got on with things once we suspected that there was a linear relationship between beak depth and beak length. However, for the linear regression to be valid, several assumptions need to be met. If any of those assumptions are violated, we can’t trust the results. The following four assumptions need to be met, with a 5th point being a case of good scientific practice:\n\nData should be linear\nResiduals are normally distributed\nEquality of variance\nThe residuals are independent\n(no influential points)\n\nAs we did many times during the Core statistics sessions, we mainly rely on diagnostic plots to check these assumptions. For this particular model they look as follows:\n\n\n\n\n\nFigure 4.6: Diagnostic plots for G. fortis (1975) model\n\n\n\n\nThese plots look very good to me. For a recap on how to interpret these plots, see CS2: ANOVA.\nTaken together, we can see the relationship between beak depth and beak length as a linear one, described by a (linear) model that has a predicted value for each data point, and an associated error."
  },
  {
    "objectID": "materials/glm-intro-glm.html#putting-the-g-into-glm",
    "href": "materials/glm-intro-glm.html#putting-the-g-into-glm",
    "title": "5  Generalising your model",
    "section": "5.1 Putting the “G” into GLM",
    "text": "5.1 Putting the “G” into GLM\nIn the previous linear model example all the assumptions were met. But what if we have data where that isn’t the case? For example, what if we have data where we can’t describe the relationship between the predictor and response variables in a linear way?\nOne of the ways we can deal with this is by using a generalised linear model, also abbreviated as GLM. In a way it’s an extension of the linear model we discussed in the previous section. As with the normal linear model, the predictor variables in the model are in a linear combination, such as:\n\\[\n\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_3 + ...\n\\]\nHere, the \\(\\beta_0\\) value is the constant or intercept, whereas each subsequent \\(\\beta_i\\) is a unique regression coefficient for each \\(X_i\\) predictor variable. So far so good.\nHowever, the GLM makes the linear model more flexible in two ways:\n\n\n\n\n\n\nImportant\n\n\n\n\nIn a standard linear model the linear combination (e.g. like we see above) becomes the predicted outcome value. With a GLM a transformation is specified, which turns the linear combination into the predicted outcome value. This is called a link function.\nA standard linear model assumes a continuous, normally distributed outcome, whereas with GLM the outcome can be both continuous or integer. Furthermore, the outcome does not have to be normally distributed. Indeed, the outcome can follow a different kind of distribution, such as binomial, Poisson, exponential etc.\n\n\n\nWe’ll introduce each of these elements below, then illustrate how they are used in practice, using different types of data.\nThe link function and different distributions are closely…err, linked. To make sense of what the link function is doing it’s useful to understand the different distributional assumptions. So we’ll start with those."
  },
  {
    "objectID": "materials/glm-intro-glm.html#distributions",
    "href": "materials/glm-intro-glm.html#distributions",
    "title": "5  Generalising your model",
    "section": "5.2 Distributions",
    "text": "5.2 Distributions\nIn the examples of a standard linear model we’ve seen that the residuals needed to be normally distributed. We’ve mainly used the Q-Q plot to assess this assumption of normality.\nBut what does “normal” actually mean? It assumes that the residuals are coming from a normal or Gaussian distribution. This distribution has a symmetrical bell-shape, where the centre is the mean, and half of the data are on either side of this.\nWe can see this in Figure 5.1. The mean of the normal distribution is indicated with the dashed blue line.\n\n\n\n\n\nFigure 5.1: Normal distribution\n\n\n\n\nWe can use the linear model we created previously, where we looked at the possible linear relationship between beak depth and beak length. This is based on measurements of G. fortis beaks in 1975.\nThe individual values of the residuals from this linear model are shown in Figure 5.1, panel B (in red), with the corresponding theoretical normal distribution in the background. We can see that the residuals follow this distribution reasonably well, which matches our conclusions from the Q-Q plot (see Figure 4.6).\nAll this means is that assuming that these residuals may come from a normal distribution isn’t such a daft suggestion after all.\nNow look at the example in Figure 5.2. This shows the classification of beak shape for a number of finches. Their beaks are either classed as blunt or pointed. Various (continuous) measurements were taken from each bird, with the beak length shown here.\n\n\n\n\n\nFigure 5.2: Classification in beak shape\n\n\n\n\nWe’ll look into this example in more detail later. For now it’s important to note that the response variable (the beak shape classification) is not continuous. Here it is a binary response (blunt or pointed). As a result, the assumptions for a regular linear model go out of the window. If we were foolish enough to fit a linear model to these data (see blue line in A), then the residuals would look rather non-normal (Figure 5.2 B).\nSo what do we do? Well, the normal distribution is not the only one there is. In Figure 5.3 there are a few examples of distributions (including the normal one).\n\n\n\n\n\nFigure 5.3: Different distributions\n\n\n\n\nDifferent distributions are useful for different types of data. For example, a logistic distribution is particularly useful in the context of binary or proportional response data. The Poisson distribution is useful when we have count data as a response.\nIn order to understand how this can help us, we need to be aware of two more concepts: linear predictors and link functions."
  },
  {
    "objectID": "materials/glm-intro-glm.html#linear-predictors",
    "href": "materials/glm-intro-glm.html#linear-predictors",
    "title": "5  Generalising your model",
    "section": "5.3 Linear predictors",
    "text": "5.3 Linear predictors\nThe nice thing about linear models is that the predictors are, well, linear. Straight lines make for easy interpretation of any potential relationship between predictor and response.\nAs mentioned before, predictors are in the form of a linear combination, where each predictor variable is multiplied by a coefficient and all the terms are added together:\n\\[\n\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_3 + ...\n\\]\nFortunately, this is no different for generalised linear models! We still have a linear combination but, as we’ll see, if the relationship is not linear then we need an additional step before we can model the data in this way.\nAt this point, we have two options at our disposal (well, there are more, but let’s not muddy the waters too much).\n\n\n\n\n\n\nImportant\n\n\n\n\nTransform our data and use a normal linear model on the transformed data\nTransform the linear predictor\n\n\n\nThe first option, to transform our data, seems like a useful option and can work. It keeps things familiar (we’d still use a standard linear model) and so all is well with the world. Up to the point of interpreting the data. If we, for example, log-transform our data, how do we interpret this? After all, the predictions of the linear model are directly related to the outcome or response variable. Transforming the data is usually done so that the residuals of the linear model resemble a more normal distribution. An unwanted side-effect of this is that this also changes the ratio scale properties of the measured variables [@stevens1946].\nThe second option would be to transform the linear predictor. This enables us to map a non-linear outcome (or response variable) to a linear model. This transformation is done using a link function."
  },
  {
    "objectID": "materials/glm-intro-glm.html#link-functions",
    "href": "materials/glm-intro-glm.html#link-functions",
    "title": "5  Generalising your model",
    "section": "5.4 Link functions",
    "text": "5.4 Link functions\nSimply put: link functions connect the predictors in our model to the response variables in a linear way.\nHowever, and similar to the standard linear model, there are two parts to each model:\n\nthe coefficients for each predictor (linking each parameter to a predictor)\nthe error or random component (which specifies a probability distribution)\n\nWhich link function you use depends on your analysis. Some common link functions and corresponding distributions are (adapted from [@glen2021]):\n\n\n\ndistribution\ndata type\nlink name\n\n\n\n\nbinomial\nbinary / proportion\nlogit\n\n\nnormal\nany real number\nidentity\n\n\npoisson\ncount data\nlog\n\n\n\nLet’s again look at the earlier example of beak shape.\n\n\n\n\n\nFigure 5.4: Beak shape classification\n\n\n\n\nWe’ve seen the data in Figure 5.4 A before, where we had information on what beak shape our observed finches had, plotted against their beak length.\nLet’s say we now want to make some predictions about what beak shape we would expect, given a certain beak length. In this scenario we’d need some way of modelling the response variable (beak shape; blunt or pointed) as a function of the predictor variable (beak length).\nThe issue we have is that the response variable is not continuous, but binary! We could fit a standard linear model to these data (blue line in Figure 5.2 A) but this is really bad practice. Why? Well, what such a linear model represents is the probability - or how likely it is - that an observed finch has a pointed beak, given a certain beak length (Figure 5.4 B).\nSimply fitting a linear line through those data suggests that it is possible to have a higher than 1 and lower-than-zero probability that a beak would be pointed! That, of course, makes no sense. So, we can’t describe these data as a linear relationship.\nInstead, we’ll use a logistic model to analyse these data. We’ll cover the practicalities of how to do this in more detail in a later chapter, but for now it’s sufficient to realise that one of the ways we could model these data could look like this:\n\n\n\n\n\nFigure 5.5: Logistic model for beak classification\n\n\n\n\nUsing this sigmoidal curve ensures that our predicted probabilities do not exceed the \\([0, 1]\\) range.\nNow, what happened behind the scenes is that the generalised linear model has taken the linear predictor and transformed it using the logit link function. This links the non-linear response variable (beak shape) to a linear model, using beak length as a predictor.\nWe’ll practice how to perform this analysis in the next section."
  },
  {
    "objectID": "materials/glm-intro-glm.html#summary",
    "href": "materials/glm-intro-glm.html#summary",
    "title": "5  Generalising your model",
    "section": "5.5 Summary",
    "text": "5.5 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nGLMs allow us to map a non-linear outcome to a linear model\nThe link function determines how this occurs, transforming the linear predictor"
  },
  {
    "objectID": "materials/cs3_practical_linear-regression.html#libraries-and-functions",
    "href": "materials/cs3_practical_linear-regression.html#libraries-and-functions",
    "title": "6  Linear regression",
    "section": "6.1 Libraries and functions",
    "text": "6.1 Libraries and functions\n\n\n\n\n\n\nClick to expand\n\n\n\n\n\n\nRPython\n\n\n\n6.1.1 Libraries\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n# Converts stats functions to a tidyverse-friendly format\nlibrary(rstatix)\n\n# Creates diagnostic plots using ggplot2\nlibrary(ggResidpanel)\n\n\n\n6.1.2 Functions\n\n# Creates diagnostic plots\nggResidpanel::resid_panel()\n\n# Creates a linear model\nstats::lm()\n\n# Creates an ANOVA table for a linear model\nstats::anova()\n\n\n\n\n\n6.1.3 Libraries\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Simple yet exhaustive stats functions.\nimport pingouin as pg\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Statistical models, conducting tests and statistical data exploration\nimport statsmodels.api as sm\n\n# Convenience interface for specifying models using formula strings and DataFrames\nimport statsmodels.formula.api as smf\n\n\n\n6.1.4 Functions\n\n# Summary statistics\npandas.DataFrame.describe()\n\n# Plots the first few rows of a DataFrame\npandas.DataFrame.head()\n\n# Query the columns of a DataFrame with a boolean expression\npandas.DataFrame.query()\n\n# Reads in a .csv file\npandas.read_csv()\n\n# Creates a model from a formula and data frame\nstatsmodels.formula.api.ols()\n\n# Creates an ANOVA table for one or more fitted linear models\nstatsmodels.stats.anova.anova_lm()"
  },
  {
    "objectID": "materials/cs3_practical_linear-regression.html#purpose-and-aim",
    "href": "materials/cs3_practical_linear-regression.html#purpose-and-aim",
    "title": "6  Linear regression",
    "section": "6.2 Purpose and aim",
    "text": "6.2 Purpose and aim\nRegression analysis not only tests for an association between two or more variables, but also allows you to investigate quantitatively the nature of any relationship which is present. This can help you determine if one variable may be used to predict values of another. Simple linear regression essentially models the dependence of a scalar dependent variable (\\(y\\)) on an independent (or explanatory) variable (\\(x\\)) according to the relationship:\n\\[\\begin{equation*}\ny = \\beta_0 + \\beta_1 x\n\\end{equation*}\\]\nwhere \\(\\beta_0\\) is the value of the intercept and \\(\\beta_1\\) is the slope of the fitted line. A linear regression analysis assesses if the coefficient of the slope, \\(\\beta_1\\), is actually different from zero. If it is different from zero then we can say that \\(x\\) has a significant effect on \\(y\\) (since changing \\(x\\) leads to a predicted change in \\(y\\)). If it isn’t significantly different from zero, then we say that there isn’t sufficient evidence of such a relationship. To assess whether the slope is significantly different from zero we first need to calculate the values of \\(\\beta_0\\) and \\(\\beta_1\\)."
  },
  {
    "objectID": "materials/cs3_practical_linear-regression.html#data-and-hypotheses",
    "href": "materials/cs3_practical_linear-regression.html#data-and-hypotheses",
    "title": "6  Linear regression",
    "section": "6.3 Data and hypotheses",
    "text": "6.3 Data and hypotheses\nWe will perform a simple linear regression analysis on the two variables murder and assault from the USArrests data set. This rather bleak data set contains statistics on arrests per 100,000 residents for assault, murder and robbery in each of the 50 US states in 1973, alongside the proportion of the population who lived in urban areas at that time. We wish to determine whether the assault variable is a significant predictor of the murder variable. This means that we will need to find the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) that best fit the following macabre equation:\n\\[\\begin{equation*}\nMurder  = \\beta_0 + \\beta_1 \\times Assault\n\\end{equation*}\\]\nAnd then will be testing the following null and alternative hypotheses:\n\n\\(H_0\\): assault is not a significant predictor of murder, \\(\\beta_1 = 0\\)\n\\(H_1\\): assault is a significant predictor of murder, \\(\\beta_1 \\neq 0\\)"
  },
  {
    "objectID": "materials/cs3_practical_linear-regression.html#summarise-and-visualise",
    "href": "materials/cs3_practical_linear-regression.html#summarise-and-visualise",
    "title": "6  Linear regression",
    "section": "6.4 Summarise and visualise",
    "text": "6.4 Summarise and visualise\n\nRPython\n\n\nFirst, we read in the data:\n\nUSArrests &lt;- read_csv(\"data/CS3-usarrests.csv\")\n\nYou can visualise the data with:\n\n# create scatterplot of the data\nggplot(USArrests,\n       aes(x = assault, y = murder)) +\n  geom_point()\n\n\n\n\n\n\nFirst, we read in the data:\n\nUSArrests_py = pd.read_csv(\"data/CS3-usarrests.csv\")\n\nYou can visualise the data with:\n\n# create scatterplot of the data\n(ggplot(USArrests_py,\n         aes(x = \"assault\",\n             y = \"murder\")) +\n     geom_point())\n\n\n\n\n\n\n\nPerhaps unsurprisingly, there appears to be a relatively strong positive relationship between these two variables. Whilst there is a reasonable scatter of the points around any trend line, we would probably expect a significant result in this case."
  },
  {
    "objectID": "materials/cs3_practical_linear-regression.html#assumptions",
    "href": "materials/cs3_practical_linear-regression.html#assumptions",
    "title": "6  Linear regression",
    "section": "6.5 Assumptions",
    "text": "6.5 Assumptions\nIn order for a linear regression analysis to be valid 4 key assumptions need to be met:\n\n\n\n\n\n\nImportant\n\n\n\n\nThe data must be linear (it is entirely possible to calculate a straight line through data that is not straight - it doesn’t mean that you should!)\nThe residuals must be normally distributed\nThe residuals must not be correlated with their fitted values (i.e. they should be independent)\nThe fit should not depend overly much on a single point (no point should have high leverage).\n\n\n\nWhether these assumptions are met can easily be checked visually by producing four key diagnostic plots.\n\nRPython\n\n\nFirst we need to define the linear model:\n\nlm_1 &lt;- lm(murder ~ assault,\n           data = USArrests)\n\n\nThe first argument to lm is a formula saying that murder depends on assault. As we have seen before, the syntax is generally dependent variable ~ independent variable.\nThe second argument specifies which data to use.\n\nNext, we can create diagnostic plots for the model:\n\nresid_panel(lm_1,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\nThe top left graph plots the Residuals plot. If the data are best explained by a straight line then there should be a uniform distribution of points above and below the horizontal blue line (and if there are sufficient points then the red line, which is a smoother line, should be on top of the blue line). This plot is pretty good.\nThe top right graph shows the Q-Q plot which allows a visual inspection of normality. If the residuals are normally distributed, then the points should lie on the diagonal dotted line. This isn’t too bad but there is some slight snaking towards the upper end and there appears to be an outlier.\nThe bottom left Location-scale graph allows us to investigate whether there is any correlation between the residuals and the predicted values and whether the variance of the residuals changes significantly. If not, then the red line should be horizontal. If there is any correlation or change in variance then the red line will not be horizontal. This plot is fine.\nThe last graph shows the Cook’s distance and tests if any one point has an unnecessarily large effect on the fit. The important aspect here is to see if any points are larger than 0.5 (meaning you’d have to be careful) or 1.0 (meaning you’d definitely have to check if that point has an large effect on the model). If not, then no point has undue influence. This plot is good.\n\n\n\nIf you haven’t loaded statsmodels yet, run the following:\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nNext, we create a linear model and get the .fit():\n\n# create a linear model\nmodel = smf.ols(formula= \"murder ~ assault\", data = USArrests_py)\n# and get the fitted parameters of the model\nlm_USArrests_py = model.fit()\n\nThen we use dgplots() to create the diagnostic plots:\n\ndgplots(lm_USArrests_py)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFormally, if there is any concern after looking at the diagnostic plots then a linear regression is not valid. However, disappointingly, very few people ever check whether the linear regression assumptions have been met before quoting the results.\nLet’s change this through leading by example!"
  },
  {
    "objectID": "materials/cs3_practical_linear-regression.html#implement-and-interpret-test",
    "href": "materials/cs3_practical_linear-regression.html#implement-and-interpret-test",
    "title": "6  Linear regression",
    "section": "6.6 Implement and interpret test",
    "text": "6.6 Implement and interpret test\nWe have already defined the linear model, so we can have a closer look at it:\n\nRPython\n\n\n\n# show the linear model\nlm_1\n\n\nCall:\nlm(formula = murder ~ assault, data = USArrests)\n\nCoefficients:\n(Intercept)      assault  \n    0.63168      0.04191  \n\n\nThe lm() function returns a linear model object which is essentially a list containing everything necessary to understand and analyse a linear model. However, if we just type the model name (as we have above) then it just prints to the screen the actual coefficients of the model i.e. the intercept and the slope of the line.\n\n\n\n\n\n\nThe linear model object: would you like to know more?\n\n\n\n\n\nIf you wanted to know more about the lm object we created, then type in:\n\nView(lm_1)\n\nThis shows a list (a type of object in R), containing all of the information associated with the linear model. The most relevant ones at the moment are:\n\ncoefficients contains the values of the coefficients we found earlier.\nresiduals contains the residual associated for each individual data point.\nfitted.values contains the values that the linear model predicts for each individual data point.\n\n\n\n\n\n\n\nprint(lm_USArrests_py.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 murder   R-squared:                       0.643\nModel:                            OLS   Adj. R-squared:                  0.636\nMethod:                 Least Squares   F-statistic:                     86.45\nDate:                Thu, 25 Jan 2024   Prob (F-statistic):           2.60e-12\nTime:                        08:33:13   Log-Likelihood:                -118.26\nNo. Observations:                  50   AIC:                             240.5\nDf Residuals:                      48   BIC:                             244.4\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.6317      0.855      0.739      0.464      -1.087       2.350\nassault        0.0419      0.005      9.298      0.000       0.033       0.051\n==============================================================================\nOmnibus:                        4.799   Durbin-Watson:                   1.796\nProb(Omnibus):                  0.091   Jarque-Bera (JB):                3.673\nSkew:                           0.598   Prob(JB):                        0.159\nKurtosis:                       3.576   Cond. No.                         436.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nA rather large table, but the values we’re interested in can be found more or less in the middle. We are after the coef values, where the intercept is 0.6317 and the slope is 0.0419.\n\n\n\nSo here we have found that the line of best fit is given by:\n\\[\\begin{equation*}\nMurder = 0.63 + 0.042 \\times Assault\n\\end{equation*}\\]\nNext we can assess whether the slope is significantly different from zero:\n\nRPython\n\n\n\nanova(lm_1)\n\nAnalysis of Variance Table\n\nResponse: murder\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nassault    1 597.70  597.70  86.454 2.596e-12 ***\nResiduals 48 331.85    6.91                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere, we again use the anova() command to assess significance. This shouldn’t be too surprising at this stage if the introductory lectures made any sense. From a mathematical perspective, one-way ANOVA and simple linear regression are exactly the same as each other and it makes sense that we should use the same command to analyse them in R.\nThis is exactly the same format as the table we saw for one-way ANOVA:\n\nThe 1st line just tells you the that this is an ANOVA test\nThe 2nd line tells you what the response variable is (in this case Murder)\nThe 3rd, 4th and 5th lines are an ANOVA table which contain some useful values:\n\nThe Df column contains the degrees of freedom values on each row, 1 and 48\nThe F value column contains the F statistic, 86.454\nThe p-value is 2.596e-12 and is the number directly under the Pr(&gt;F) on the 4th line.\nThe other values in the table (in the Sum Sq and Mean Sq) column are used to calculate the F statistic itself and we don’t need to know these.\n\n\n\n\nWe can perform an ANOVA on the lm_USArrests_py object using the anova_lm() function from the statsmodels package.\n\nsm.stats.anova_lm(lm_USArrests_py, typ = 2)\n\n              sum_sq    df          F        PR(&gt;F)\nassault   597.703202   1.0  86.454086  2.595761e-12\nResidual  331.849598  48.0        NaN           NaN\n\n\n\n\n\nAgain, the p-value is what we’re most interested in here and shows us the probability of getting data such as ours if the null hypothesis were actually true and the slope of the line were actually zero. Since the p-value is excruciatingly tiny we can reject our null hypothesis and state that:\n\nA simple linear regression showed that the assault rate in US states was a significant predictor of the number of murders (p = 2.59x10-12).\n\n\n6.6.1 Plotting the regression line\nIt can be very helpful to plot the regression line with the original data to see how far the data are from the predicted linear values. We can do this as follows:\n\nRPython\n\n\n\n# plot the data\nggplot(USArrests,\n       aes(x = assault, y = murder)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\nWe plot all the data using geom_point()\nNext, we add the linear model using geom_smooth(method = \"lm\"), hiding the confidence intervals (se = FALSE)\n\n\n\n\n(ggplot(USArrests_py,\n        aes(x = \"assault\", y = \"murder\")) +\n     geom_point() +\n     geom_smooth(method = \"lm\",\n                 se = False,\n                 colour = \"blue\"))"
  },
  {
    "objectID": "materials/cs3_practical_linear-regression.html#exercises",
    "href": "materials/cs3_practical_linear-regression.html#exercises",
    "title": "6  Linear regression",
    "section": "6.7 Exercises",
    "text": "6.7 Exercises\n\n6.7.1 State data: Life expectancy and murder\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nWe will use the data from the file data/CS3-statedata.csv data set for this exercise. This rather more benign data set contains information on more general properties of each US state, such as population (1975), per capita income (1974), illiteracy proportion (1970), life expectancy (1969), murder rate per 100,000 people (there’s no getting away from it), percentage of the population who are high-school graduates, average number of days where the minimum temperature is below freezing between 1931 and 1960, and the state area in square miles. The data set contains 50 rows and 8 columns, with column names: population, income, illiteracy, life_exp, murder, hs_grad, frost and area.\nPerform a linear regression on the variables life_exp and murder and do the following:\n\nFind the value of the slope and intercept coefficients for both regressions\nDetermine if the slope is significantly different from zero (i.e. is there a relationship between the two variables)\nProduce a scatter plot of the data with the line of best fit superimposed on top.\nProduce diagnostic plots and discuss with your (virtual) neighbour if you should have carried out a simple linear regression in each case\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n6.8 Answer\n\nLoad and visualise the data\n\nRPython\n\n\nFirst, we read in the data:\n\nUSAstate &lt;- read_csv(\"data/CS3-statedata.csv\")\n\nNext, we visualise the murder variable against the life_exp variable. We also add a regression line.\n\n# plot the data and add the regression line\nggplot(USAstate,\n       aes(x = murder, y = life_exp)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\nFirst, we read in the data:\n\nUSAstate_py = pd.read_csv(\"data/CS3-statedata.csv\")\n\nNext, we visualise the murder variable against the life_exp variable. We also add a regression line.\n\n(ggplot(USAstate_py,\n        aes(x = \"life_exp\", y = \"murder\")) +\n     geom_point() +\n     geom_smooth(method = \"lm\",\n                 se = False,\n                 colour = \"blue\"))\n\n\n\n\n\n\n\nWe visualise for the same reasons as before:\n\nWe check that the data aren’t obviously wrong. Here we have sensible values for life expectancy (nothing massively large or small), and plausible values for murder rates (not that I’m that au fait with US murder rates in 1973 but small positive numbers seem plausible).\nWe check to see what we would expect from the statistical analysis. Here there does appear to be a reasonable downward trend to the data. I would be surprised if we didn’t get a significant result given the amount of data and the spread of the data about the line\nWe check the assumptions (only roughly though as we’ll be doing this properly in a minute). Nothing immediately gives me cause for concern; the data appear linear, the spread of the data around the line appears homogeneous and symmetrical. No outliers either.\n\n\n\nCheck assumptions\nNow, let’s check the assumptions with the diagnostic plots.\n\nRPython\n\n\n\n# create a linear model\nlm_murder &lt;- lm(life_exp ~ murder,\n           data = USAstate)\n\n# create the diagnostic plots\nresid_panel(lm_murder,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\nThe Residuals plot appears symmetric enough (similar distribution of points above and below the horizontal blue line) for me to happy with linearity. Similarly the red line in the Location-Scale plot looks horizontal enough for me to be happy with homogeneity of variance. There aren’t any influential points in the Cook’s distance plot. The only plot that does give me a bit of concern is the Q-Q plot. Here we see clear evidence of snaking, although the degree of snaking isn’t actually that bad. This just means that we can be pretty certain that the distribution of residuals isn’t normal, but also that it isn’t very non-normal.\n\n\nFirst, we create a linear model:\n\n# create a linear model\nmodel = smf.ols(formula= \"life_exp ~ murder\", data = USAstate_py)\n# and get the fitted parameters of the model\nlm_USAstate_py = model.fit()\n\nNext, we can create the diagnostic plots:\n\ndgplots(lm_USAstate_py)\n\n\n\n\n\n\nThe Residuals plot appears symmetric enough (similar distribution of points above and below the horizontal blue line) for me to happy with linearity. Similarly the red line in the Location-Scale plot looks horizontal enough for me to be happy with homogeneity of variance. There aren’t any influential points in the Influential points plot. The only plot that does give me a bit of concern is the Q-Q plot. Here we see clear evidence of snaking, although the degree of snaking isn’t actually that bad. This just means that we can be pretty certain that the distribution of residuals isn’t normal, but also that it isn’t very non-normal.\n\n\n\nWhat do we do in this situation? Well, there are three possible options:\n\nAppeal to the Central Limit Theorem. This states that if we have a large enough sample size we don’t have to worry about whether the distribution of the residuals are normally distributed. Large enough is a bit of a moving target here and to be honest it depends on how non-normal the underlying data are. If the data are only a little bit non-normal then we can get away with using a smaller sample than if the data are massively skewed (for example). This is not an exact science, but anything over 30 data points is considered a lot for mild to moderate non-normality (as we have in this case). If the data were very skewed then we would be looking for more data points (50-100). So, for this example we can legitimately just carry on with our analysis without worrying.\nTry transforming the data. Here we would try applying some mathematical functions to the response variable (life_exp) in the hope that repeating the analysis with this transformed variable would make things better. To be honest with you it might not work and we won’t know until we try. Dealing with transformed variables is legitimate as an approach but it can make interpreting the model a bit more challenging. In this particular example none of the traditional transformations (log, square-root, reciprocal) do anything to fix the slight lack of normality.\nGo with permutation methods / bootstrapping. This approach would definitely work. I don’t have time to explain it here (it’s the subject of an entire other practical). This approach also requires us to have a reasonably large sample size to work well as we have to assume that the distribution of the sample is a good approximation for the distribution of the entire data set.\n\nSo in this case, because we have a large enough sample size and our deviation from normality isn’t too bad, we can just crack on with the standard analysis.\n\n\nImplement and interpret test\nSo, let’s actually do the analysis:\n\nRPython\n\n\n\nanova(lm_murder)\n\nAnalysis of Variance Table\n\nResponse: life_exp\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nmurder     1 53.838  53.838  74.989 2.26e-11 ***\nResiduals 48 34.461   0.718                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nsm.stats.anova_lm(lm_USAstate_py, typ = 2)\n\n             sum_sq    df          F        PR(&gt;F)\nmurder    53.837675   1.0  74.988652  2.260070e-11\nResidual  34.461327  48.0        NaN           NaN\n\n\n\n\n\nAnd after all of that we find that the murder rate is a statistically significant predictor of life expectancy in US states. Woohoo!\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.8.1 State data: Graduation and frost days\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nNow let’s investigate the relationship between the proportion of High School Graduates a state has (hs_grad) and the mean number of days below freezing (frost) within each state.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n6.9 Answer\nWe’ll run through this a bit quicker:\n\nRPython\n\n\n\n# plot the data\nggplot(USAstate,\n       aes(x = frost, y = hs_grad)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n(ggplot(USAstate_py,\n        aes(x = \"hs_grad\", y = \"frost\")) +\n     geom_point() +\n     geom_smooth(method = \"lm\",\n                 se = False,\n                 colour = \"blue\"))\n\n\n\n\n\n\n\nOnce again, we look at the data.\n\nThere doesn’t appear to be any ridiculous errors with the data; High School graduation proportions are in the 0-100% range and the mean number of sub-zero days for each state are between 0 and 365, so these numbers are plausible.\nWhilst there is a trend upwards, which wouldn’t surprise me if it came back as being significant, I’m a bit concerned about…\nThe assumptions. I’m mainly concerned that the data aren’t very linear. There appears to be a noticeable pattern to the data with some sort of minimum around 50-60 Frost days. This means that it’s hard to assess the other assumptions.\n\nLet’s check these out properly\nNow, let’s check the assumptions with the diagnostic plots.\n\nRPython\n\n\n\n# create a linear model\nlm_frost &lt;- lm(hs_grad ~ frost,\n               data = USAstate)\n\n# create the diagnostic plots\nresid_panel(lm_frost,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\nFirst, we create a linear model:\n\n# create a linear model\nmodel = smf.ols(formula= \"hs_grad ~ frost\", data = USAstate_py)\n# and get the fitted parameters of the model\nlm_frost_py = model.fit()\n\nNext, we can create the diagnostic plots:\n\ndgplots(lm_frost_py)\n\n\n\n\n\n\n\n\n\nWe can see that what we suspected from before is backed up by the residuals plot. The data aren’t linear and there appears to be some sort of odd down-up pattern here. Given the lack of linearity it just isn’t worth worrying about the other plots because our model is misspecified: a straight line just doesn’t represent our data at all.\nJust for reference, and as practice for looking at diagnostic plots, if we ignore the lack of linearity then we can say that\n\nNormality is pretty good from the Q-Q plot\nHomogeneity of variance isn’t very good and there appears to be a noticeable drop in variance as we go from left to right (from consideration of the Location-Scale plot)\nThere don’t appear to be any influential points (by looking at the Cook’s distance plot)\n\nHowever, none of that is relevant in this particular case since the data aren’t linear and a straight line would be the wrong model to fit.\nSo what do we do in this situation?\nWell actually, this is a bit tricky as there aren’t any easy fixes here. There are two broad solutions for dealing with a misspecified model.\n\nThe most common solution is that we need more predictor variables in the model. Here we’re trying to explain/predict high school graduation only using the number of frost days. Obviously there are many more things that would affect the proportion of high school graduates than just how cold it is in a State (which is a weird potential predictor when you think about it) and so what we would need is a statistical approach that allows us to look at multiple predictor variables. We’ll cover that approach in the next two sessions.\nThe other potential solution is to say that high school graduation can in fact be predicted only by the number of frost days but that the relationship between them isn’t linear. We would then need to specify a relationship (a curve basically) and then try to fit the data to the new, non-linear, curve. This process is called, unsurprisingly, non-linear regression and we don’t cover that in this course. This process is best used when there is already a strong theoretical reason for a non-linear relationship between two variables (such as sigmoidal dose-response curves in pharmacology or exponential relationships in cell growth). In this case we don’t have any such preconceived notions and so it wouldn’t really be appropriate in this case.\n\nNeither of these solutions can be tackled with the knowledge that we have so far in the course but we can definitely say that based upon this data set, there isn’t a linear relationship (significant or otherwise) between frosty days and high school graduation rates."
  },
  {
    "objectID": "materials/cs3_practical_linear-regression.html#answer",
    "href": "materials/cs3_practical_linear-regression.html#answer",
    "title": "6  Linear regression",
    "section": "6.8 Answer",
    "text": "6.8 Answer\n\nLoad and visualise the data\n\nRPython\n\n\nFirst, we read in the data:\n\nUSAstate &lt;- read_csv(\"data/CS3-statedata.csv\")\n\nNext, we visualise the murder variable against the life_exp variable. We also add a regression line.\n\n# plot the data and add the regression line\nggplot(USAstate,\n       aes(x = murder, y = life_exp)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\nFirst, we read in the data:\n\nUSAstate_py = pd.read_csv(\"data/CS3-statedata.csv\")\n\nNext, we visualise the murder variable against the life_exp variable. We also add a regression line.\n\n(ggplot(USAstate_py,\n        aes(x = \"life_exp\", y = \"murder\")) +\n     geom_point() +\n     geom_smooth(method = \"lm\",\n                 se = False,\n                 colour = \"blue\"))\n\n\n\n\n\n\n\nWe visualise for the same reasons as before:\n\nWe check that the data aren’t obviously wrong. Here we have sensible values for life expectancy (nothing massively large or small), and plausible values for murder rates (not that I’m that au fait with US murder rates in 1973 but small positive numbers seem plausible).\nWe check to see what we would expect from the statistical analysis. Here there does appear to be a reasonable downward trend to the data. I would be surprised if we didn’t get a significant result given the amount of data and the spread of the data about the line\nWe check the assumptions (only roughly though as we’ll be doing this properly in a minute). Nothing immediately gives me cause for concern; the data appear linear, the spread of the data around the line appears homogeneous and symmetrical. No outliers either.\n\n\n\nCheck assumptions\nNow, let’s check the assumptions with the diagnostic plots.\n\nRPython\n\n\n\n# create a linear model\nlm_murder &lt;- lm(life_exp ~ murder,\n           data = USAstate)\n\n# create the diagnostic plots\nresid_panel(lm_murder,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\nThe Residuals plot appears symmetric enough (similar distribution of points above and below the horizontal blue line) for me to happy with linearity. Similarly the red line in the Location-Scale plot looks horizontal enough for me to be happy with homogeneity of variance. There aren’t any influential points in the Cook’s distance plot. The only plot that does give me a bit of concern is the Q-Q plot. Here we see clear evidence of snaking, although the degree of snaking isn’t actually that bad. This just means that we can be pretty certain that the distribution of residuals isn’t normal, but also that it isn’t very non-normal.\n\n\nFirst, we create a linear model:\n\n# create a linear model\nmodel = smf.ols(formula= \"life_exp ~ murder\", data = USAstate_py)\n# and get the fitted parameters of the model\nlm_USAstate_py = model.fit()\n\nNext, we can create the diagnostic plots:\n\ndgplots(lm_USAstate_py)\n\n\n\n\n\n\nThe Residuals plot appears symmetric enough (similar distribution of points above and below the horizontal blue line) for me to happy with linearity. Similarly the red line in the Location-Scale plot looks horizontal enough for me to be happy with homogeneity of variance. There aren’t any influential points in the Influential points plot. The only plot that does give me a bit of concern is the Q-Q plot. Here we see clear evidence of snaking, although the degree of snaking isn’t actually that bad. This just means that we can be pretty certain that the distribution of residuals isn’t normal, but also that it isn’t very non-normal.\n\n\n\nWhat do we do in this situation? Well, there are three possible options:\n\nAppeal to the Central Limit Theorem. This states that if we have a large enough sample size we don’t have to worry about whether the distribution of the residuals are normally distributed. Large enough is a bit of a moving target here and to be honest it depends on how non-normal the underlying data are. If the data are only a little bit non-normal then we can get away with using a smaller sample than if the data are massively skewed (for example). This is not an exact science, but anything over 30 data points is considered a lot for mild to moderate non-normality (as we have in this case). If the data were very skewed then we would be looking for more data points (50-100). So, for this example we can legitimately just carry on with our analysis without worrying.\nTry transforming the data. Here we would try applying some mathematical functions to the response variable (life_exp) in the hope that repeating the analysis with this transformed variable would make things better. To be honest with you it might not work and we won’t know until we try. Dealing with transformed variables is legitimate as an approach but it can make interpreting the model a bit more challenging. In this particular example none of the traditional transformations (log, square-root, reciprocal) do anything to fix the slight lack of normality.\nGo with permutation methods / bootstrapping. This approach would definitely work. I don’t have time to explain it here (it’s the subject of an entire other practical). This approach also requires us to have a reasonably large sample size to work well as we have to assume that the distribution of the sample is a good approximation for the distribution of the entire data set.\n\nSo in this case, because we have a large enough sample size and our deviation from normality isn’t too bad, we can just crack on with the standard analysis.\n\n\nImplement and interpret test\nSo, let’s actually do the analysis:\n\nRPython\n\n\n\nanova(lm_murder)\n\nAnalysis of Variance Table\n\nResponse: life_exp\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nmurder     1 53.838  53.838  74.989 2.26e-11 ***\nResiduals 48 34.461   0.718                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nsm.stats.anova_lm(lm_USAstate_py, typ = 2)\n\n             sum_sq    df          F        PR(&gt;F)\nmurder    53.837675   1.0  74.988652  2.260070e-11\nResidual  34.461327  48.0        NaN           NaN\n\n\n\n\n\nAnd after all of that we find that the murder rate is a statistically significant predictor of life expectancy in US states. Woohoo!"
  },
  {
    "objectID": "materials/cs3_practical_linear-regression.html#answer-1",
    "href": "materials/cs3_practical_linear-regression.html#answer-1",
    "title": "6  Linear regression",
    "section": "6.9 Answer",
    "text": "6.9 Answer\nWe’ll run through this a bit quicker:\n\nRPython\n\n\n\n# plot the data\nggplot(USAstate,\n       aes(x = frost, y = hs_grad)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n(ggplot(USAstate_py,\n        aes(x = \"hs_grad\", y = \"frost\")) +\n     geom_point() +\n     geom_smooth(method = \"lm\",\n                 se = False,\n                 colour = \"blue\"))\n\n\n\n\n\n\n\nOnce again, we look at the data.\n\nThere doesn’t appear to be any ridiculous errors with the data; High School graduation proportions are in the 0-100% range and the mean number of sub-zero days for each state are between 0 and 365, so these numbers are plausible.\nWhilst there is a trend upwards, which wouldn’t surprise me if it came back as being significant, I’m a bit concerned about…\nThe assumptions. I’m mainly concerned that the data aren’t very linear. There appears to be a noticeable pattern to the data with some sort of minimum around 50-60 Frost days. This means that it’s hard to assess the other assumptions.\n\nLet’s check these out properly\nNow, let’s check the assumptions with the diagnostic plots.\n\nRPython\n\n\n\n# create a linear model\nlm_frost &lt;- lm(hs_grad ~ frost,\n               data = USAstate)\n\n# create the diagnostic plots\nresid_panel(lm_frost,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\nFirst, we create a linear model:\n\n# create a linear model\nmodel = smf.ols(formula= \"hs_grad ~ frost\", data = USAstate_py)\n# and get the fitted parameters of the model\nlm_frost_py = model.fit()\n\nNext, we can create the diagnostic plots:\n\ndgplots(lm_frost_py)\n\n\n\n\n\n\n\n\n\nWe can see that what we suspected from before is backed up by the residuals plot. The data aren’t linear and there appears to be some sort of odd down-up pattern here. Given the lack of linearity it just isn’t worth worrying about the other plots because our model is misspecified: a straight line just doesn’t represent our data at all.\nJust for reference, and as practice for looking at diagnostic plots, if we ignore the lack of linearity then we can say that\n\nNormality is pretty good from the Q-Q plot\nHomogeneity of variance isn’t very good and there appears to be a noticeable drop in variance as we go from left to right (from consideration of the Location-Scale plot)\nThere don’t appear to be any influential points (by looking at the Cook’s distance plot)\n\nHowever, none of that is relevant in this particular case since the data aren’t linear and a straight line would be the wrong model to fit.\nSo what do we do in this situation?\nWell actually, this is a bit tricky as there aren’t any easy fixes here. There are two broad solutions for dealing with a misspecified model.\n\nThe most common solution is that we need more predictor variables in the model. Here we’re trying to explain/predict high school graduation only using the number of frost days. Obviously there are many more things that would affect the proportion of high school graduates than just how cold it is in a State (which is a weird potential predictor when you think about it) and so what we would need is a statistical approach that allows us to look at multiple predictor variables. We’ll cover that approach in the next two sessions.\nThe other potential solution is to say that high school graduation can in fact be predicted only by the number of frost days but that the relationship between them isn’t linear. We would then need to specify a relationship (a curve basically) and then try to fit the data to the new, non-linear, curve. This process is called, unsurprisingly, non-linear regression and we don’t cover that in this course. This process is best used when there is already a strong theoretical reason for a non-linear relationship between two variables (such as sigmoidal dose-response curves in pharmacology or exponential relationships in cell growth). In this case we don’t have any such preconceived notions and so it wouldn’t really be appropriate in this case.\n\nNeither of these solutions can be tackled with the knowledge that we have so far in the course but we can definitely say that based upon this data set, there isn’t a linear relationship (significant or otherwise) between frosty days and high school graduation rates."
  },
  {
    "objectID": "materials/cs3_practical_linear-regression.html#summary",
    "href": "materials/cs3_practical_linear-regression.html#summary",
    "title": "6  Linear regression",
    "section": "6.10 Summary",
    "text": "6.10 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nLinear regression tests if a linear relationship exists between two or more variables\nIf so, we can use one variable to predict another\nA linear model has an intercept and slope and we test if the slope differs from zero\nWe create linear models and perform an ANOVA to assess the slope coefficient\nWe can only use a linear regression if these four assumptions are met:\n\nThe data are linear\nResiduals are normally distributed\nResiduals are not correlated with their fitted values\nNo single point should have a large influence on the linear model\n\nWe can use diagnostic plots to evaluate these assumptions"
  },
  {
    "objectID": "materials/cs4_practical_two-way-anova.html#libraries-and-functions",
    "href": "materials/cs4_practical_two-way-anova.html#libraries-and-functions",
    "title": "7  Two-way ANOVA",
    "section": "7.1 Libraries and functions",
    "text": "7.1 Libraries and functions\n\n\n\n\n\n\nClick to expand\n\n\n\n\n\n\nRPython\n\n\n\n7.1.1 Libraries\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n# Converts stats functions to a tidyverse-friendly format\nlibrary(rstatix)\n\n# Creates diagnostic plots using ggplot2\nlibrary(ggResidpanel)\n\n\n\n7.1.2 Functions\n\n# Creates diagnostic plots\nggResidpanel::resid_panel()\n\n# Creates a linear model\nstats::lm()\n\n# Creates an ANOVA table for a linear model\nstats::anova()\n\n\n\n\n\n7.1.3 Libraries\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Statistical models, conducting tests and statistical data exploration\nimport statsmodels.api as sm\n\n# Convenience interface for specifying models using formula strings and DataFrames\nimport statsmodels.formula.api as smf\n\n\n\n7.1.4 Functions\n\n# Summary statistics\npandas.DataFrame.describe()\n\n# Plots the first few rows of a DataFrame\npandas.DataFrame.head()\n\n# Reads in a .csv file\npandas.read_csv()\n\n# Creates a model from a formula and data frame\nstatsmodels.formula.api.ols()\n\n# Creates an ANOVA table for one or more fitted linear models\nstatsmodels.stats.anova.anova_lm()"
  },
  {
    "objectID": "materials/cs4_practical_two-way-anova.html#purpose-and-aim",
    "href": "materials/cs4_practical_two-way-anova.html#purpose-and-aim",
    "title": "7  Two-way ANOVA",
    "section": "7.2 Purpose and aim",
    "text": "7.2 Purpose and aim\nA two-way analysis of variance is used when we have two categorical predictor variables (or factors) and a single continuous response variable. For example, when we are looking at how body weight (continuous response variable in kilograms) is affected by sex (categorical variable, male or female) and exercise type (categorical variable, control or runner).\n\n\n\n\n\nWhen analysing these type of data there are two things we want to know:\n\nDoes either of the predictor variables have an effect on the response variable i.e. does sex affect body weight? Or does being a runner affect body weight?\nIs there any interaction between the two predictor variables? An interaction would mean that the effect that exercise has on your weight depends on whether you are male or female rather than being independent of your sex. For example if being male means that runners weigh more than non-runners, but being female means that runners weight less than non-runners then we would say that there was an interaction.\n\nWe will first consider how to visualise the data before then carrying out an appropriate statistical test."
  },
  {
    "objectID": "materials/cs4_practical_two-way-anova.html#data-and-hypotheses",
    "href": "materials/cs4_practical_two-way-anova.html#data-and-hypotheses",
    "title": "7  Two-way ANOVA",
    "section": "7.3 Data and hypotheses",
    "text": "7.3 Data and hypotheses\nWe will recreate the example analysis used in the lecture. The data are stored as a .csv file called data/CS4-exercise.csv."
  },
  {
    "objectID": "materials/cs4_practical_two-way-anova.html#summarise-and-visualise",
    "href": "materials/cs4_practical_two-way-anova.html#summarise-and-visualise",
    "title": "7  Two-way ANOVA",
    "section": "7.4 Summarise and visualise",
    "text": "7.4 Summarise and visualise\nexercise is a data frame with three variables; weight, sex and exercise. weight is the continuous response variable, whereas sex and exercise are the categorical predictor variables.\n\nRPython\n\n\nFirst, we read in the data:\n\nexercise &lt;- read_csv(\"data/CS4-exercise.csv\")\n\nYou can visualise the data with:\n\n# visualise the data, sex vs weight\nggplot(exercise,\n       aes(x = sex, y = weight)) +\n  geom_boxplot()\n\n\n\n# visualise the data, exercise vs weight\nggplot(exercise,\n       aes(x = exercise, y = weight)) +\n  geom_boxplot()\n\n\n\n\n\n\nFirst, we read in the data:\n\nexercise_py = pd.read_csv(\"data/CS4-exercise.csv\")\n\nYou can visualise the data with:\n\n# visualise the data, sex vs weight\n(ggplot(exercise_py,\n        aes(x = \"sex\", y = \"weight\")) +\n  geom_boxplot())\n\n\n\n# visualise the data, exercise vs weight\n(ggplot(exercise_py,\n        aes(x = \"exercise\", y = \"weight\")) +\n  geom_boxplot())\n\n\n\n\n\n\n\nThese produce box plots showing the response variable (weight) only in terms of one of the predictor variables. The values of the other predictor variable in each case aren’t taken into account.\nA better way would be to visualise both variables at the same time. We can do this as follows:\n\nRPython\n\n\n\nggplot(exercise,\n       aes(x = sex, y = weight, fill = exercise)) +\n  geom_boxplot() +\n  scale_fill_brewer(palette = \"Dark2\")\n\n\n\n\nThis produces box plots for all (four) combinations of the predictor variables. We are plotting sex on the x-axis; weight on the y-axis and filling the box plot by exercise regime.\nHere I’ve also changed the default colouring scheme, by using scale_fill_brewer(palette = \"Dark2\"). This uses a colour-blind friendly colour palette (more about the Brewer colour pallete here).\n\n\n\n(ggplot(exercise_py,\n        aes(x = \"sex\",\n            y = \"weight\", fill = \"exercise\")) +\n     geom_boxplot() +\n     scale_fill_brewer(type = \"qual\", palette = \"Dark2\"))\n\n\n\n\nThis produces box plots for all (four) combinations of the predictor variables. We are plotting sex on the x-axis; weight on the y-axis and filling the box plot by exercise regime.\nHere I’ve also changed the default colouring scheme, by using scale_fill_brewer(type = \"qual\", palette = \"Dark2\"). This uses a colour-blind friendly colour palette (more about the Brewer colour pallete here).\n\n\n\nIn this example there are only four box plots and so it is relatively easy to compare them and look for any interactions between variables, but if there were more than two groups per categorical variable, it would become harder to spot what was going on.\nTo compare categorical variables more easily we can just plot the group means which aids our ability to look for interactions and the main effects of each predictor variable. This is called an interaction plot.\nCreate an interaction plot:\n\nRPython\n\n\nWe’re adding a bit of jitter to the data, to avoid too much overlap between the data points. We can do this with geom_jitter().\n\nggplot(exercise,\n       aes(x = sex, y = weight,\n           colour = exercise, group = exercise)) +\n  geom_jitter(width = 0.05) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  scale_colour_brewer(palette = \"Dark2\")\n\n\n\n\nHere we plot weight on the y-axis, by sex on the x-axis.\n\nwe colour the data by exercise regime and group the data by exercise to work out the mean values of each group\ngeom_jitter(width = 0.05) displays the data, with a tiny bit of random noise, to separate the data points a bit for visualisation\nstat_summary(fun = mean)calculates the mean for each group\nscale_colour_brewer() lets us define the colour palette\n\nThe choice of which categorical factor is plotted on the horizontal axis and which is plotted as different lines is completely arbitrary. Looking at the data both ways shouldn’t add anything but often you’ll find that you prefer one plot to another.\nPlot the interaction plot the other way round:\n\nggplot(exercise,\n       aes(x = exercise, y = weight,\n           colour = sex, group = sex)) +\n  geom_jitter(width = 0.05) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  scale_colour_brewer(palette = \"Dark2\")\n\n\n\n\n\n\nWe’re adding a bit of jitter to the data, to avoid too much overlap between the data points. We can do this with geom_jitter().\n\n(ggplot(exercise_py,\n        aes(x = \"sex\", y = \"weight\",\n            colour = \"exercise\", group = \"exercise\")) +\n     geom_jitter(width = 0.05) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"point\", size = 3) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"line\") +\n     scale_colour_brewer(type = \"qual\", palette = \"Dark2\"))\n\n\n\n\nHere we plot weight on the y-axis, by sex on the x-axis.\n\nwe colour the data by exercise regime and group the data by exercise to work out the mean values of each group\ngeom_jitter(width = 0.05) displays the data, with a tiny bit of random noise, to separate the data points a bit for visualisation\nstat_summary(fun_data = \"mean_cl_boot\")calculates the mean for each group\nscale_colour_brewer() lets us define the colour palette\n\nThe choice of which categorical factor is plotted on the horizontal axis and which is plotted as different lines is completely arbitrary. Looking at the data both ways shouldn’t add anything but often you’ll find that you prefer one plot to another.\nPlot the interaction plot the other way round:\n\n(ggplot(exercise_py,\n        aes(x = \"exercise\", y = \"weight\",\n            colour = \"sex\", group = \"sex\")) +\n     geom_jitter(width = 0.05) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"point\", size = 3) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"line\") +\n  scale_colour_brewer(type = \"qual\", palette = \"Dark2\"))\n\n\n\n\n\n\n\nBy now you should have a good feeling for the data and could already provide some guesses to the following three questions:\n\nDoes there appear to be any interaction between the two categorical variables?\nIf not:\n\nDoes exercise have an effect on weight?\nDoes sex have an effect on weight?\n\n\nWe can now attempt to answer these three questions more formally using an ANOVA test. We have to test for three things: the interaction, the effect of exercise and the effect of sex."
  },
  {
    "objectID": "materials/cs4_practical_two-way-anova.html#assumptions",
    "href": "materials/cs4_practical_two-way-anova.html#assumptions",
    "title": "7  Two-way ANOVA",
    "section": "7.5 Assumptions",
    "text": "7.5 Assumptions\nBefore we can formally test these things we first need to define the model and check the underlying assumptions. We use the following code to define the model:\n\nRPython\n\n\n\n# define the linear model\nlm_exercise &lt;- lm(weight ~ sex + exercise + sex:exercise,\n                  data = exercise)\n\nThe sex:exercise term is how R represents the concept of an interaction between these two variables.\n\n\n\n# create a linear model\nmodel = smf.ols(formula = \"weight ~ exercise * sex\", data = exercise_py)\n# and get the fitted parameters of the model\nlm_exercise_py = model.fit()\n\nThe formula weight ~ exercise * sex can be read as “weight depends on exercise and sex and the interaction between exercise and sex.\n\n\n\nAs the two-way ANOVA is a type of linear model we need to satisfy pretty much the same assumptions as we did for a simple linear regression or a one-way ANOVA:\n\nThe data must not have any systematic pattern to it\nThe residuals must be normally distributed\nThe residuals must have homogeneity of variance\nThe fit should not depend overly much on a single point (no point should have high leverage).\n\nAgain, we will check these assumptions visually by producing four key diagnostic plots.\n\nRPython\n\n\n\nresid_panel(lm_exercise,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\nThe Residual plot shows the residuals against the predicted values. There is no systematic pattern here and this plot is pretty good.\nThe Q-Q plot allows a visual inspection of normality. Again, this looks OK (not perfect but OK).\nThe Location-Scale plot allows us to investigate whether there is homogeneity of variance. This plot is fine (not perfect but fine).\nThe Cook’s D plot shows that no individual point has a high influence on the model (all values are well below 0.5)\n\n\nThere is a shorthand way of writing:\nweight ~ sex + exercise + sex:exercise\nIf you use the following syntax:\nweight ~ sex * exercise\nThen R interprets it exactly the same way as writing all three terms. You can see this if you compare the output of the following two commands:\n\nanova(lm(weight ~ sex + exercise + sex:exercise,\n         data = exercise))\n\nAnalysis of Variance Table\n\nResponse: weight\n              Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nsex            1 4510.1  4510.1 366.911 &lt; 2.2e-16 ***\nexercise       1 1312.0  1312.0 106.733 &lt; 2.2e-16 ***\nsex:exercise   1  404.4   404.4  32.902 4.889e-08 ***\nResiduals    156 1917.6    12.3                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm(weight ~ sex * exercise,\n         data = exercise))\n\nAnalysis of Variance Table\n\nResponse: weight\n              Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nsex            1 4510.1  4510.1 366.911 &lt; 2.2e-16 ***\nexercise       1 1312.0  1312.0 106.733 &lt; 2.2e-16 ***\nsex:exercise   1  404.4   404.4  32.902 4.889e-08 ***\nResiduals    156 1917.6    12.3                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\ndgplots(lm_exercise_py)"
  },
  {
    "objectID": "materials/cs4_practical_two-way-anova.html#implement-and-interpret-test",
    "href": "materials/cs4_practical_two-way-anova.html#implement-and-interpret-test",
    "title": "7  Two-way ANOVA",
    "section": "7.6 Implement and interpret test",
    "text": "7.6 Implement and interpret test\nThe assumptions appear to be met well enough, meaning we can implement the ANOVA. We do this as follows (this is probably the easiest bit!):\n\nRPython\n\n\n\n# perform the ANOVA\nanova(lm_exercise)\n\nAnalysis of Variance Table\n\nResponse: weight\n              Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nsex            1 4510.1  4510.1 366.911 &lt; 2.2e-16 ***\nexercise       1 1312.0  1312.0 106.733 &lt; 2.2e-16 ***\nsex:exercise   1  404.4   404.4  32.902 4.889e-08 ***\nResiduals    156 1917.6    12.3                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe have a row in the table for each of the different effects that we’ve asked R to consider. The last column is the important one as this contains the p-values. We need to look at the interaction row first.\n\n\n\nsm.stats.anova_lm(lm_exercise_py, typ = 2)\n\n                   sum_sq     df           F        PR(&gt;F)\nexercise      1311.970522    1.0  106.733448  2.177106e-19\nsex           4636.450232    1.0  377.191645  1.760076e-43\nexercise:sex   404.434414    1.0   32.902172  4.889216e-08\nResidual      1917.556353  156.0         NaN           NaN\n\n\nWe have a row in the table for each of the different effects that we’ve asked Python to consider. The last column is the important one as this contains the p-values. We need to look at the interaction row first.\n\n\n\nsex:exercise has a p-value of about 4.89e-08 (which is smaller than 0.05) and so we can conclude that the interaction between sex and exercise is significant.\nThis is where we must stop.\nThe top two lines (corresponding to the effects of sex and exercise) are meaningless now. This is because the interaction means that we cannot interpret the main effects independently.\nIn this case, weight depends on and the sex and the exercise regime. This means the effect of sex on weight is dependent on exercise (and vice-versa).\nWe would report this as follows:\n\nA two-way ANOVA test showed that there was a significant interaction between the effects of sex and exercise on weight (p = 4.89e-08). Exercise was associated with a small loss of weight in males but a larger loss of weight in females."
  },
  {
    "objectID": "materials/cs4_practical_two-way-anova.html#exercises",
    "href": "materials/cs4_practical_two-way-anova.html#exercises",
    "title": "7  Two-way ANOVA",
    "section": "7.7 Exercises",
    "text": "7.7 Exercises\n\n7.7.1 Auxin response\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nPlant height responses to auxin in different genotypes\nThese data/CS4-auxin.csv data are from a simulated experiment that looks at the effect of the plant hormone auxin on plant height.\nThe experiment consists of two genotypes: a wild type control and a mutant (genotype). The plants are treated with auxin at different concentrations: none, low and high, which are stored in the concentration column.\nThe response variable plant height (plant_height) is then measured at the end of their life cycle, in centimeters.\nQuestions to answer:\n\nVisualise the data using boxplots and interaction plots.\nDoes there appear to be any interaction between genotype and concentration?\nCarry out a two-way ANOVA test.\nCheck the assumptions.\nWhat can you conclude? (Write a sentence to summarise).\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n7.8 Answer\n\nRPython\n\n\n\nLoad the data\n\n# read in the data\nauxin_response &lt;- read_csv(\"data/CS4-auxin.csv\")\n\n# let's have a peek at the data\nhead(auxin_response)\n\n# A tibble: 6 × 3\n  genotype concentration plant_height\n  &lt;chr&gt;    &lt;chr&gt;                &lt;dbl&gt;\n1 control  high                  33.7\n2 control  high                  27.1\n3 control  high                  22.9\n4 control  high                  28.7\n5 control  high                  30.7\n6 control  high                  26.9\n\n\n\n\nVisualise the data\n\nggplot(auxin_response,\n       aes(x = genotype, y = plant_height)) +\n  geom_boxplot()\n\n\n\nggplot(auxin_response,\n       aes(x = concentration, y = plant_height)) +\n  geom_boxplot()\n\n\n\n\nLet’s look at the interaction plots. We’re only plotting the mean values here, but feel free to explore the data itself by adding another geom_.\n\n# by genotype\nggplot(auxin_response,\n       aes(x = concentration,\n          y = plant_height,\n          colour = genotype, group = genotype)) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  geom_jitter(alpha = 0.3, width = 0.1) +\n  scale_colour_brewer(palette = \"Dark2\")\n\n\n\n# by concentration\nggplot(auxin_response,\n       aes(x = genotype,\n           y = plant_height,\n           colour = concentration, group = concentration)) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  geom_jitter(alpha = 0.3, width = 0.1) +\n  scale_colour_brewer(palette = \"Dark2\")\n\n\n\n\nWe’ve constructed both box plots and two interaction plots. We only needed to do one interaction plot but I find it can be quite useful to look at the data from different angles.\nThe interaction plots show the mean values for each group, so I prefer to overlay this with the actual data. Both interaction plots suggest that there is an interaction here as the lines in the plots aren’t parallel. Looking at the interaction plot with concentration on the x-axis, it appears that there is non-difference between genotypes when the concentration is low, but that there is a difference between genotypes when the concentration is none or high.\n\n\nAssumptions\nFirst we need to define the model:\n\n# define the linear model, with interaction term\nlm_auxin &lt;- lm(plant_height ~ concentration * genotype,\n               data = auxin_response)\n\nNext, we check the assumptions:\n\nresid_panel(lm_auxin,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\nSo, these actually all look pretty good, with the data looking normally distributed (Q-Q plot), linearity OK (Residual plot), homogeneity of variance looking sharp (Location-scale plot) and no clear influential points (Cook’s D plot).\n\n\nImplement the test\nLet’s carry out a two-way ANOVA:\n\n# perform the ANOVA\nanova(lm_auxin)\n\nAnalysis of Variance Table\n\nResponse: plant_height\n                        Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nconcentration            2 4708.2 2354.12 316.333 &lt; 2.2e-16 ***\ngenotype                 1   83.8   83.84  11.266 0.0009033 ***\nconcentration:genotype   2 1034.9  517.45  69.531 &lt; 2.2e-16 ***\nResiduals              269 2001.9    7.44                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nInterpret the output and report the results\nThere is a significant interaction between concentration and genotype.\n\n\n\n\nLoad the data\n\n# read in the data\nauxin_response_py = pd.read_csv(\"data/CS4-auxin.csv\")\n\n# let's have a peek at the data\nauxin_response_py.head()\n\n  genotype concentration  plant_height\n0  control          high          33.7\n1  control          high          27.1\n2  control          high          22.9\n3  control          high          28.7\n4  control          high          30.7\n\n\n\n\nVisualise the data\n\n(ggplot(auxin_response_py,\n       aes(x = \"genotype\", y = \"plant_height\")) +\n  geom_boxplot())\n\n\n\n(ggplot(auxin_response_py,\n       aes(x = \"concentration\", y = \"plant_height\")) +\n  geom_boxplot())\n\n\n\n\nLet’s look at the interaction plots. We’re also including the data itself here with geom_jitter().\n\n# by genotype\n(ggplot(auxin_response_py,\n        aes(x = \"concentration\",\n            y = \"plant_height\",\n            colour = \"genotype\", group = \"genotype\")) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"point\", size = 3) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"line\") +\n     geom_jitter(alpha = 0.3, width = 0.1) +\n     scale_colour_brewer(type = \"qual\", palette = \"Dark2\"))\n\n\n\n# by concentration\n(ggplot(auxin_response_py,\n        aes(x = \"genotype\",\n            y = \"plant_height\",\n            colour = \"concentration\", group = \"concentration\")) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"point\", size = 3) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"line\") +\n     geom_jitter(alpha = 0.3, width = 0.1) +\n     scale_colour_brewer(type = \"qual\", palette = \"Dark2\"))\n\n\n\n\nWe’ve constructed both box plots and two interaction plots. We only needed to do one interaction plot but I find it can be quite useful to look at the data from different angles.\nThe interaction plots show the mean values for each group, so I prefer to overlay this with the actual data. Both interaction plots suggest that there is an interaction here as the lines in the plots aren’t parallel. Looking at the interaction plot with concentration on the x-axis, it appears that there is non-difference between genotypes when the concentration is low, but that there is a difference between genotypes when the concentration is none or high.\n\n\nAssumptions\nFirst we need to define the model:\n\n# create a linear model\nmodel = smf.ols(formula= \"plant_height ~ C(genotype) * concentration\", data = auxin_response_py)\n# and get the fitted parameters of the model\nlm_auxin_py = model.fit()\n\nNext, we check the assumptions:\n\ndgplots(lm_auxin_py)\n\n\n\n\n\n\nSo, these actually all look pretty good, with the data looking normally distributed (Q-Q plot), linearity OK (Residual plot), homogeneity of variance looking sharp (Location-scale plot) and no clear influential points (Influential points plot).\n\n\nImplement the test\nLet’s carry out a two-way ANOVA:\n\nsm.stats.anova_lm(lm_auxin_py, typ = 2)\n\n                                sum_sq     df           F        PR(&gt;F)\nC(genotype)                  83.839560    1.0   11.265874  9.033241e-04\nconcentration              4578.890410    2.0  307.642376  3.055198e-70\nC(genotype):concentration  1034.894263    2.0   69.531546  4.558874e-25\nResidual                   2001.872330  269.0         NaN           NaN\n\n\n\n\nInterpret the output and report the results\nThere is definitely a significant interaction between concentration and genotype.\n\n\n\n\nSo, we can conclude the following:\n\nA two-way ANOVA showed that there is a significant interaction between genotype and auxin concentration on plant height (p = 4.56e-25). Increasing auxin concentration appears to result in a reduction of plant height in both wild type and mutant genotypes. The response in the mutant genotype seems to be less pronounced than in wild type.\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.8.1 Tulips\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nBlooms and growing conditions\nWe’re sticking with the plant theme and using the data/CS4-tulip.csv data set, which contains information on an experiment to determine the best conditions for growing tulips (well someone has to care about these sorts of things!). The average number of flower heads (blooms) were recorded for 27 different plots. Each plot experienced one of three different watering regimes and one of three different shade regimes.\n\nInvestigate how the number of blooms is affected by different growing conditions.\n\nNote: have a look at the data and make sure that they are in the correct format!\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nRPython\n\n\n\nLoad the data\n\n# read in the data\ntulip &lt;- read_csv(\"data/CS4-tulip.csv\")\n\nRows: 27 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): water, shade, blooms\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# have a quick look at the data\ntulip\n\n# A tibble: 27 × 3\n   water shade blooms\n   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1     1     1    0  \n 2     1     2    0  \n 3     1     3  111. \n 4     2     1  183. \n 5     2     2   59.2\n 6     2     3   76.8\n 7     3     1  225. \n 8     3     2   83.8\n 9     3     3  135. \n10     1     1   80.1\n# ℹ 17 more rows\n\n\nIn this data set the watering regime (water) and shading regime (shade) are encoded with numerical values. However, these numbers are actually categories, representing the amount of water/shade.\nAs such, we don’t want to treat these as numbers but as factors. At the moment they are numbers, which we can tell with &lt;dbl&gt;, which stands for double.\nWe can convert the columns using the as_factor() function. Because we’d like to keep referring to these columns as factors, we will update our existing data set.\n\n# convert watering and shade regimes to factor\ntulip &lt;- tulip %&gt;% \n  mutate(water = as_factor(water),\n         shade = as_factor(shade))\n\nThis data set has three variables; blooms (which is the response variable) and water and shade (which are the two potential predictor variables).\n\n\nVisualise the data\nAs always we’ll visualise the data first:\n\n# by watering regime\nggplot(tulip,\n       aes(x = water, y = blooms)) +\n  geom_boxplot()\n\n\n\n# by shading regime\nggplot(tulip,\n       aes(x = shade, y = blooms)) +\n  geom_boxplot()\n\n\n\n# interaction plot by watering regime\nggplot(tulip,\n       aes(x = shade,\n           y = blooms,\n           colour = water, group = water)) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  geom_jitter(alpha = 0.3, width = 0.1) +\n  scale_colour_brewer(palette = \"Dark2\")\n\n\n\n# interaction plot by shade regime\nggplot(tulip,\n       aes(x = water,\n           y = blooms,\n           colour = shade, group = shade)) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  geom_jitter(alpha = 0.3, width = 0.1) +\n  scale_colour_brewer(palette = \"Dark2\")\n\n\n\n\nAgain, both interaction plots suggest that there might be an interaction here. Digging in a little deeper from a descriptive perspective, it looks as though that water regime 1 is behaving differently to water regimes 2 and 3 under different shade conditions.\n\n\nAssumptions\nFirst we need to define the model:\n\n# define the linear model\nlm_tulip &lt;- lm(blooms ~ water * shade,\n               data = tulip)\n\nNext, we check the assumptions:\n\nresid_panel(lm_tulip,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\nThese are actually all OK. A two-way ANOVA analysis is on the cards.\n\n\nImplement the test\nLet’s carry out the two-way ANOVA.\n\n# perform the ANOVA\nanova(lm_tulip)\n\nAnalysis of Variance Table\n\nResponse: blooms\n            Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nwater        2 103626   51813 22.0542 1.442e-05 ***\nshade        2  36376   18188  7.7417   0.00375 ** \nwater:shade  4  41058   10265  4.3691   0.01211 *  \nResiduals   18  42288    2349                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nInterpret the output and report results\nSo we do appear to have a significant interaction between water and shade as expected.\n\n\n\n\nLoad the data\n\n# read in the data\ntulip_py = pd.read_csv(\"data/CS4-tulip.csv\")\n\n# have a quick look at the data\ntulip_py.head()\n\n   water  shade  blooms\n0      1      1    0.00\n1      1      2    0.00\n2      1      3  111.04\n3      2      1  183.47\n4      2      2   59.16\n\n\nIn this data set the watering regime (water) and shading regime (shade) are encoded with numerical values. However, these numbers are actually categories, representing the amount of water/shade.\nAs such, we don’t want to treat these as numbers but as factors. We can convert the columns using astype(object). Because we’d like to keep referring to these columns as factors, we will update our existing data set.\n\n# convert watering and shade regimes to factor\ntulip_py['water'] = tulip_py['water'].astype(object)\ntulip_py['shade'] = tulip_py['shade'].astype(object)\n\nThis data set has three variables; blooms (which is the response variable) and water and shade (which are the two potential predictor variables).\n\n\nVisualise the data\nAs always we’ll visualise the data first:\n\n# by watering regime\n(ggplot(tulip_py,\n        aes(x = \"water\", y = \"blooms\")) +\n     geom_boxplot())\n\n\n\n  \n# by shading regime\n(ggplot(tulip_py,\n        aes(x = \"shade\", y = \"blooms\")) +\n     geom_boxplot())\n\n\n\n# interaction plot by watering regime\n(ggplot(tulip_py,\n        aes(x = \"shade\", y = \"blooms\",\n            colour = \"water\", group = \"water\")) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"point\", size = 3) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"line\") +\n     geom_jitter(alpha = 0.3, width = 0.1) +\n     scale_colour_brewer(type = \"qual\", palette = \"Dark2\"))\n\n\n\n# interaction plot by shade regime\n(ggplot(tulip_py,\n        aes(x = \"water\", y = \"blooms\",\n            colour = \"shade\", group = \"shade\")) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"point\", size = 3) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"line\") +\n     geom_jitter(alpha = 0.3, width = 0.1) +\n     scale_colour_brewer(type = \"qual\", palette = \"Dark2\"))\n\n\n\n\nAgain, both interaction plots suggest that there might be an interaction here. Digging in a little deeper from a descriptive perspective, it looks as though that water regime 1 is behaving differently to water regimes 2 and 3 under different shade conditions.\n\n\nAssumptions\nFirst we need to define the model:\n\n# create a linear model\nmodel = smf.ols(formula= \"blooms ~ water * shade\", data = tulip_py)\n# and get the fitted parameters of the model\nlm_tulip_py = model.fit()\n\nNext, we check the assumptions:\n\ndgplots(lm_tulip_py)\n\n\n\n\n\n\nThese are actually all OK. A two-way ANOVA analysis is on the cards.\n\n\nImplement the test\nLet’s carry out the two-way ANOVA.\n\nsm.stats.anova_lm(lm_tulip_py, typ = 2)\n\n                    sum_sq    df          F    PR(&gt;F)\nwater        103625.786985   2.0  22.054200  0.000014\nshade         36375.936807   2.0   7.741723  0.003750\nwater:shade   41058.139437   4.0   4.369108  0.012108\nResidual      42288.185200  18.0        NaN       NaN\n\n\n\n\nInterpret the output and report results\nSo we do appear to have a significant interaction between water and shade as expected.\n\n\n\n\n\nA two-way ANOVA showed that there is a significant interaction between watering and shading regimes on number of blooms (p = 1.21e-02)."
  },
  {
    "objectID": "materials/cs4_practical_two-way-anova.html#answer",
    "href": "materials/cs4_practical_two-way-anova.html#answer",
    "title": "7  Two-way ANOVA",
    "section": "7.8 Answer",
    "text": "7.8 Answer\n\nRPython\n\n\n\nLoad the data\n\n# read in the data\nauxin_response &lt;- read_csv(\"data/CS4-auxin.csv\")\n\n# let's have a peek at the data\nhead(auxin_response)\n\n# A tibble: 6 × 3\n  genotype concentration plant_height\n  &lt;chr&gt;    &lt;chr&gt;                &lt;dbl&gt;\n1 control  high                  33.7\n2 control  high                  27.1\n3 control  high                  22.9\n4 control  high                  28.7\n5 control  high                  30.7\n6 control  high                  26.9\n\n\n\n\nVisualise the data\n\nggplot(auxin_response,\n       aes(x = genotype, y = plant_height)) +\n  geom_boxplot()\n\n\n\nggplot(auxin_response,\n       aes(x = concentration, y = plant_height)) +\n  geom_boxplot()\n\n\n\n\nLet’s look at the interaction plots. We’re only plotting the mean values here, but feel free to explore the data itself by adding another geom_.\n\n# by genotype\nggplot(auxin_response,\n       aes(x = concentration,\n          y = plant_height,\n          colour = genotype, group = genotype)) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  geom_jitter(alpha = 0.3, width = 0.1) +\n  scale_colour_brewer(palette = \"Dark2\")\n\n\n\n# by concentration\nggplot(auxin_response,\n       aes(x = genotype,\n           y = plant_height,\n           colour = concentration, group = concentration)) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  geom_jitter(alpha = 0.3, width = 0.1) +\n  scale_colour_brewer(palette = \"Dark2\")\n\n\n\n\nWe’ve constructed both box plots and two interaction plots. We only needed to do one interaction plot but I find it can be quite useful to look at the data from different angles.\nThe interaction plots show the mean values for each group, so I prefer to overlay this with the actual data. Both interaction plots suggest that there is an interaction here as the lines in the plots aren’t parallel. Looking at the interaction plot with concentration on the x-axis, it appears that there is non-difference between genotypes when the concentration is low, but that there is a difference between genotypes when the concentration is none or high.\n\n\nAssumptions\nFirst we need to define the model:\n\n# define the linear model, with interaction term\nlm_auxin &lt;- lm(plant_height ~ concentration * genotype,\n               data = auxin_response)\n\nNext, we check the assumptions:\n\nresid_panel(lm_auxin,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\nSo, these actually all look pretty good, with the data looking normally distributed (Q-Q plot), linearity OK (Residual plot), homogeneity of variance looking sharp (Location-scale plot) and no clear influential points (Cook’s D plot).\n\n\nImplement the test\nLet’s carry out a two-way ANOVA:\n\n# perform the ANOVA\nanova(lm_auxin)\n\nAnalysis of Variance Table\n\nResponse: plant_height\n                        Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nconcentration            2 4708.2 2354.12 316.333 &lt; 2.2e-16 ***\ngenotype                 1   83.8   83.84  11.266 0.0009033 ***\nconcentration:genotype   2 1034.9  517.45  69.531 &lt; 2.2e-16 ***\nResiduals              269 2001.9    7.44                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nInterpret the output and report the results\nThere is a significant interaction between concentration and genotype.\n\n\n\n\nLoad the data\n\n# read in the data\nauxin_response_py = pd.read_csv(\"data/CS4-auxin.csv\")\n\n# let's have a peek at the data\nauxin_response_py.head()\n\n  genotype concentration  plant_height\n0  control          high          33.7\n1  control          high          27.1\n2  control          high          22.9\n3  control          high          28.7\n4  control          high          30.7\n\n\n\n\nVisualise the data\n\n(ggplot(auxin_response_py,\n       aes(x = \"genotype\", y = \"plant_height\")) +\n  geom_boxplot())\n\n\n\n(ggplot(auxin_response_py,\n       aes(x = \"concentration\", y = \"plant_height\")) +\n  geom_boxplot())\n\n\n\n\nLet’s look at the interaction plots. We’re also including the data itself here with geom_jitter().\n\n# by genotype\n(ggplot(auxin_response_py,\n        aes(x = \"concentration\",\n            y = \"plant_height\",\n            colour = \"genotype\", group = \"genotype\")) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"point\", size = 3) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"line\") +\n     geom_jitter(alpha = 0.3, width = 0.1) +\n     scale_colour_brewer(type = \"qual\", palette = \"Dark2\"))\n\n\n\n# by concentration\n(ggplot(auxin_response_py,\n        aes(x = \"genotype\",\n            y = \"plant_height\",\n            colour = \"concentration\", group = \"concentration\")) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"point\", size = 3) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"line\") +\n     geom_jitter(alpha = 0.3, width = 0.1) +\n     scale_colour_brewer(type = \"qual\", palette = \"Dark2\"))\n\n\n\n\nWe’ve constructed both box plots and two interaction plots. We only needed to do one interaction plot but I find it can be quite useful to look at the data from different angles.\nThe interaction plots show the mean values for each group, so I prefer to overlay this with the actual data. Both interaction plots suggest that there is an interaction here as the lines in the plots aren’t parallel. Looking at the interaction plot with concentration on the x-axis, it appears that there is non-difference between genotypes when the concentration is low, but that there is a difference between genotypes when the concentration is none or high.\n\n\nAssumptions\nFirst we need to define the model:\n\n# create a linear model\nmodel = smf.ols(formula= \"plant_height ~ C(genotype) * concentration\", data = auxin_response_py)\n# and get the fitted parameters of the model\nlm_auxin_py = model.fit()\n\nNext, we check the assumptions:\n\ndgplots(lm_auxin_py)\n\n\n\n\n\n\nSo, these actually all look pretty good, with the data looking normally distributed (Q-Q plot), linearity OK (Residual plot), homogeneity of variance looking sharp (Location-scale plot) and no clear influential points (Influential points plot).\n\n\nImplement the test\nLet’s carry out a two-way ANOVA:\n\nsm.stats.anova_lm(lm_auxin_py, typ = 2)\n\n                                sum_sq     df           F        PR(&gt;F)\nC(genotype)                  83.839560    1.0   11.265874  9.033241e-04\nconcentration              4578.890410    2.0  307.642376  3.055198e-70\nC(genotype):concentration  1034.894263    2.0   69.531546  4.558874e-25\nResidual                   2001.872330  269.0         NaN           NaN\n\n\n\n\nInterpret the output and report the results\nThere is definitely a significant interaction between concentration and genotype.\n\n\n\n\nSo, we can conclude the following:\n\nA two-way ANOVA showed that there is a significant interaction between genotype and auxin concentration on plant height (p = 4.56e-25). Increasing auxin concentration appears to result in a reduction of plant height in both wild type and mutant genotypes. The response in the mutant genotype seems to be less pronounced than in wild type."
  },
  {
    "objectID": "materials/cs4_practical_two-way-anova.html#summary",
    "href": "materials/cs4_practical_two-way-anova.html#summary",
    "title": "7  Two-way ANOVA",
    "section": "7.9 Summary",
    "text": "7.9 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nA two-way ANOVA is used when there are two categorical variables and a single continuous variable\nWe can visually check for interactions between the categorical variables by using interaction plots\nThe two-way ANOVA is a type of linear model and assumes the following:\n\nthe data have no systematic pattern\nthe residuals are normally distributed\nthe residuals have homogeneity of variance\nthe fit does not depend on a single point (no single point has high leverage)"
  },
  {
    "objectID": "materials/cs4_practical_linear-regression-grouped-data.html#libraries-and-functions",
    "href": "materials/cs4_practical_linear-regression-grouped-data.html#libraries-and-functions",
    "title": "8  Linear regression with grouped data",
    "section": "8.1 Libraries and functions",
    "text": "8.1 Libraries and functions\n\n\n\n\n\n\nClick to expand\n\n\n\n\n\n\nRPython\n\n\n\n8.1.1 Libraries\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n# Converts stats functions to a tidyverse-friendly format\nlibrary(rstatix)\n\n# Creates diagnostic plots using ggplot2\nlibrary(ggResidpanel)\n\n# Helper functions for tidying data\nlibrary(broom)\n\n\n\n8.1.2 Functions\n\n# Gets underlying data out of model object\nbroom::augment()\n\n# Creates diagnostic plots\nggResidpanel::resid_panel()\n\n# Performs an analysis of variance\nstats::anova()\n\n# Creates a linear model\nstats::lm()\n\n\n\n\n\n8.1.3 Libraries\n\n# A fundamental package for scientific computing in Python\nimport numpy as np\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Simple yet exhaustive stats functions.\nimport pingouin as pg\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Statistical models, conducting tests and statistical data exploration\nimport statsmodels.api as sm\n\n# Convenience interface for specifying models using formula strings and DataFrames\nimport statsmodels.formula.api as smf\n\n\n\n8.1.4 Functions\n\n# Computes natural logarithm of value\nnumpy.log()\n\n# Plots the first few rows of a DataFrame\npandas.DataFrame.head()\n\n# Reads in a .csv file\npandas.read_csv()\n\n# Creates a model from a formula and data frame\nstatsmodels.formula.api.ols()"
  },
  {
    "objectID": "materials/cs4_practical_linear-regression-grouped-data.html#purpose-and-aim",
    "href": "materials/cs4_practical_linear-regression-grouped-data.html#purpose-and-aim",
    "title": "8  Linear regression with grouped data",
    "section": "8.2 Purpose and aim",
    "text": "8.2 Purpose and aim\nA linear regression analysis with grouped data is used when we have one categorical predictor variable (or factor), and one continuous predictor variable. The response variable must still be continuous however.\nFor example in an experiment that looks at light intensity in woodland, how is light intensity (continuous: lux) affected by the height at which the measurement is taken, recorded as depth measured from the top of the canopy (continuous: meters) and by the type of woodland (categorical: Conifer or Broad leaf).\n\n\n\n\n\nWhen analysing these type of data we want to know:\n\nIs there a difference between the groups?\nDoes the continuous predictor variable affect the continuous response variable (does canopy depth affect measured light intensity?)\nIs there any interaction between the two predictor variables? Here an interaction would display itself as a difference in the slopes of the regression lines for each group, so for example perhaps the conifer data set has a significantly steeper line than the broad leaf woodland data set.\n\nIn this case, no interaction means that the regression lines will have the same slope. Essentially the analysis is identical to two-way ANOVA.\n\nWe will plot the data and visually inspect it.\nWe will test for an interaction and if it doesn’t exist then:\n\nWe can test to see if either predictor variable has an effect (i.e. do the regression lines have different intercepts? and is the common gradient significantly different from zero?)\n\n\nWe will first consider how to visualise the data before then carrying out an appropriate statistical test."
  },
  {
    "objectID": "materials/cs4_practical_linear-regression-grouped-data.html#data-and-hypotheses",
    "href": "materials/cs4_practical_linear-regression-grouped-data.html#data-and-hypotheses",
    "title": "8  Linear regression with grouped data",
    "section": "8.3 Data and hypotheses",
    "text": "8.3 Data and hypotheses\nThe data are stored in data/CS4-treelight.csv. This is a data frame with four variables; id, light, depth and species. light is the continuous response variable, depth is the continuous predictor variable and species is the categorical predictor variable.\nRead in the data and inspect them:\n\nRPython\n\n\n\n# read in the data\ntreelight &lt;- read_csv(\"data/CS4-treelight.csv\")\n\n# inspect the data\ntreelight\n\n# A tibble: 23 × 4\n      id light depth species\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  \n 1     1 4106.  1    Conifer\n 2     2 4934.  1.75 Conifer\n 3     3 4417.  2.5  Conifer\n 4     4 4529.  3.25 Conifer\n 5     5 3443.  4    Conifer\n 6     6 4640.  4.75 Conifer\n 7     7 3082.  5.5  Conifer\n 8     8 2368.  6.25 Conifer\n 9     9 2777.  7    Conifer\n10    10 2419.  7.75 Conifer\n# ℹ 13 more rows\n\n\n\n\n\n# load the data\ntreelight_py = pd.read_csv(\"data/CS4-treelight.csv\")\n\n# and have a look\ntreelight_py.head()\n\n   id        light  depth  species\n0   1  4105.646110   1.00  Conifer\n1   2  4933.925144   1.75  Conifer\n2   3  4416.527443   2.50  Conifer\n3   4  4528.618186   3.25  Conifer\n4   5  3442.610306   4.00  Conifer"
  },
  {
    "objectID": "materials/cs4_practical_linear-regression-grouped-data.html#summarise-and-visualise",
    "href": "materials/cs4_practical_linear-regression-grouped-data.html#summarise-and-visualise",
    "title": "8  Linear regression with grouped data",
    "section": "8.4 Summarise and visualise",
    "text": "8.4 Summarise and visualise\n\nRPython\n\n\n\n# plot the data\nggplot(treelight,\n       aes(x = depth, y = light, colour = species)) +\n  geom_point() +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(x = \"Depth (m)\",\n       y = \"Light intensity (lux)\")\n\n\n\n\n\n\n\n# plot the data\n(ggplot(treelight_py,\n        aes(x = \"depth\",\n            y = \"light\",\n            colour = \"species\")) +\n     geom_point() +\n     scale_color_brewer(type = \"qual\", palette = \"Dark2\") +\n     labs(x = \"Depth (m)\",\n         y = \"Light intensity (lux)\")\n)\n\n\n\n\n\n\n\nIt looks like there is a slight negative correlation between depth and light intensity, with light intensity reducing as the canopy depth increases. It would be useful to plot the regression lines in this plot.\n\nRPython\n\n\n\n# plot the data\nggplot(treelight,\n       aes(x = depth, y = light, colour = species)) +\n  geom_point() +\n  # add regression lines\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(x = \"Depth (m)\",\n       y = \"Light intensity (lux)\")\n\n\n\n\n\n\n\n# plot the data\n(ggplot(treelight_py,\n        aes(x = \"depth\",\n            y = \"light\",\n            colour = \"species\")) +\n     geom_point() +\n     # add regression lines\n     geom_smooth(method = \"lm\", se = False) +\n     scale_color_brewer(type = \"qual\", palette = \"Dark2\") +\n     labs(x = \"Depth (m)\",\n          y = \"Light intensity (lux)\"))\n\n\n\n\n\n\n\nLooking at this plot, there doesn’t appear to be any significant interaction between the woodland type (Broadleaf and Conifer) and the depth at which light measurements were taken (depth) on the amount of light intensity getting through the canopy as the gradients of the two lines appear to be very similar. There does appear to be a noticeable slope to both lines and both lines look as though they have very different intercepts. All of this suggests that there isn’t any interaction but that both depth and species have a significant effect on light independently."
  },
  {
    "objectID": "materials/cs4_practical_linear-regression-grouped-data.html#implement-and-interpret-the-test",
    "href": "materials/cs4_practical_linear-regression-grouped-data.html#implement-and-interpret-the-test",
    "title": "8  Linear regression with grouped data",
    "section": "8.5 Implement and interpret the test",
    "text": "8.5 Implement and interpret the test\nIn this case we’re going to implement the test before checking the assumptions (I know, let’s live a little!). You’ll find out why soon…\nWe can test for a possible interaction more formally:\n\nRPython\n\n\n\nanova(lm(light ~ depth * species,\n         data = treelight))\n\nAnalysis of Variance Table\n\nResponse: light\n              Df   Sum Sq  Mean Sq  F value    Pr(&gt;F)    \ndepth          1 30812910 30812910 107.8154 2.861e-09 ***\nspecies        1 51029543 51029543 178.5541 4.128e-11 ***\ndepth:species  1   218138   218138   0.7633    0.3932    \nResiduals     19  5430069   285793                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nRemember that depth * species is a shorthand way of writing the full set of depth + species + depth:species terms in R i.e. both main effects and the interaction effect.\n\n\nUnfortunately there is no clear way of defining interaction models in pingouin. So we’re resorting back to statsmodels, just like we had to when we performed the Shapiro-Wilk test on the residuals.\nIf you haven’t loaded statsmodels yet, run the following:\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nNext, we create a linear model and get the .fit():\n\n# create a linear model\nmodel = smf.ols(formula = \"light ~ depth * C(species)\",\n                data = treelight_py)\n\n# and get the fitted parameters of the model\nlm_treelight_py = model.fit()\n\nTo get the relevant values, we can print the summary of the model fit. This gives us a rather huge table. Don’t be too daunted by it - there is a logic to the madness and for now we’re mainly interested in the P&gt;|t| column.\n\nprint(lm_treelight_py.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  light   R-squared:                       0.938\nModel:                            OLS   Adj. R-squared:                  0.928\nMethod:                 Least Squares   F-statistic:                     95.71\nDate:                Thu, 25 Jan 2024   Prob (F-statistic):           1.19e-11\nTime:                        08:33:58   Log-Likelihood:                -174.91\nNo. Observations:                  23   AIC:                             357.8\nDf Residuals:                      19   BIC:                             362.4\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n===============================================================================================\n                                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------------------\nIntercept                    7798.5655    298.623     26.115      0.000    7173.541    8423.590\nC(species)[T.Conifer]       -2784.5833    442.274     -6.296      0.000   -3710.274   -1858.893\ndepth                        -221.1256     61.802     -3.578      0.002    -350.478     -91.773\ndepth:C(species)[T.Conifer]   -71.0357     81.309     -0.874      0.393    -241.217      99.145\n==============================================================================\nOmnibus:                        1.435   Durbin-Watson:                   2.176\nProb(Omnibus):                  0.488   Jarque-Bera (JB):                1.269\nSkew:                           0.444   Prob(JB):                        0.530\nKurtosis:                       2.267   Cond. No.                         31.8\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nAs with two-way ANOVA we have a row in the table for each of the different effects. At this point we are particularly interested in the p-values. We need to look at the interaction first.\nThe interaction term between depth and species has a p-value of 0.393 (which is bigger than 0.05) and so we can conclude that the interaction between depth and species isn’t significant. As such we can now consider whether each of the predictor variables independently has an effect.\nBoth depth and species have very small p-values (2.86x10-9 and 4.13x10 -11) and so we can conclude that they do have a significant effect on light.\nThis means that the two regression lines should have the same non-zero slope, but different intercepts. We would now like to know what those values are.\n\n8.5.1 Finding intercept values\nFinding the intercept values is not entirely straightforward and there is some deciphering required to get this right.\nFor a simple straight line such as the linear regression for the conifer data by itself, the output is relatively straightforward:\n\nRPython\n\n\nI’m being a bit lazy here. Since I don’t want to save the filtered data in a separate object, I’m using the pipe to send the Conifer data to the lm() function. However, the first argument in the lm() function is not data, so we need to tell it specifically that the data is coming from the pipe. We do this with the . notation:\n\n# filter the Conifer data and fit a linear model\ntreelight %&gt;% \n  filter(species == \"Conifer\") %&gt;% \n  lm(light ~ depth, data = .)\n\n\nCall:\nlm(formula = light ~ depth, data = .)\n\nCoefficients:\n(Intercept)        depth  \n     5014.0       -292.2  \n\n\n\n\nWe have two options to obtain the intercept for conifers only. We could subset our data, keeping only the conifer values. We could then create a linear model of those data, and obtain the relevant intercept.\nHowever, since we already created a model for the entire data set (including the interaction term) and printed the summary of that, we can actually derive the intercept value with the information that we’ve got.\nIn the coef table of the summary there are several values:\nIntercept                      7798.5655\nC(species)[T.Conifer]         -2784.5833\ndepth                         -221.1256\ndepth:C(species)[T.Conifer]   -71.0357\nThis tells us that the overall intercept value for the model with the interaction term is 7798.5655. The C(species)[T.Conifer] term means that, to go from this overall intercept value to the intercept for conifer, we need to add -2784.5833.\nDoing the maths gives us an intercept of \\(7798.5655 + (-2784.5833) = 5014\\) if we round this.\nEqually, if we want to get the coefficient for depth, then we take the reference value of -221.1256 and add the value next to depth:C(species)[T.Conifer] to it. This gives us \\(-221.1256 + (-71.0357) = -292.2\\) if we round it.\n\n\n\nWe can interpret this as meaning that the intercept of the regression line is 5014 and the coefficient of the depth variable (the number in front of it in the equation) is -292.2.\nSo, the equation of the regression line for the conifer data is given by:\n\\[\\begin{equation}\nlight = 5014 + -292.2 \\times depth\n\\end{equation}\\]\nThis means that for every extra 1 m of depth of forest canopy we lose 292.2 lux of light.\nWhen we looked at the full data set, we found that interaction wasn’t important. This means that we will have a model with two distinct intercepts but only a single slope (that’s what you get for a linear regression without any interaction), so we need to calculate that specific combination. We do this is as follows:\n\nRPython\n\n\n\nlm(light ~ depth + species,\n   data = treelight)\n\n\nCall:\nlm(formula = light ~ depth + species, data = treelight)\n\nCoefficients:\n   (Intercept)           depth  speciesConifer  \n        7962.0          -262.2         -3113.0  \n\n\nNotice the + symbol in the argument, as opposed to the * symbol used earlier. This means that we are explicitly not including an interaction term in this fit, and consequently we are forcing R to calculate the equation of lines which have the same gradient.\nIdeally we would like R to give us two equations, one for each forest type, so four parameters in total. Unfortunately, R is parsimonious and doesn’t do that. Instead R gives you three coefficients, and these require a bit of interpretation.\nThe first two numbers that R returns (underneath Intercept and depth) are the exact intercept and slope coefficients for one of the lines (in this case they correspond to the data for Broadleaf woodlands).\nFor the coefficients belonging to the other line, R uses these first two coefficients as baseline values and expresses the other coefficients relative to these ones. R also doesn’t tell you explicitly which group it is using as its baseline reference group! (Did I mention that R can be very helpful at times 😉?)\nSo, how to decipher the above output?\nFirst, I need to work out which group has been used as the baseline.\n\nIt will be the group that comes first alphabetically, so it should be Broadleaf\nThe other way to check would be to look and see which group is not mentioned in the above table. Conifer is mentioned (in the SpeciesConifer heading) and so again the baseline group is Broadleaf.\n\nThis means that the intercept value and depth coefficient correspond to the Broadleaf group and as a result I know what the equation of one of my lines is:\nBroadleaf:\n\\[\\begin{equation}\nlight = 7962 + -262.2 \\times depth\n\\end{equation}\\]\nIn this example we know that the gradient is the same for both lines (because we explicitly asked to exclude an interaction), so all I need to do is find the intercept value for the Conifer group. Unfortunately, the final value given underneath SpeciesConifer does not give me the intercept for Conifer, instead it tells me the difference between the Conifer group intercept and the baseline intercept i.e. the equation for the regression line for conifer woodland is given by:\n\\[\\begin{equation}\nlight = (7962 + -3113) + -262.2 \\times depth\n\\end{equation}\\]\n\\[\\begin{equation}\nlight = 4829 + -262.2 \\times depth\n\\end{equation}\\]\n\n\nThe way we obtain the values for the model without the interaction is very similar to what we did for the conifer data. We need to update our model first, to remove the interaction:\n\n# create a linear model\nmodel = smf.ols(formula = \"light ~ depth + C(species)\",\n                data = treelight_py)\n\n# and get the fitted parameters of the model\nlm_treelight_add_py = model.fit()\n\nNotice the + symbol in the argument, as opposed to the * symbol used earlier. This means that we are explicitly not including an interaction term in this fit, and consequently we are forcing Python to calculate the equation of lines which have the same gradient.\nWe can get the relevant coefficients as follows:\n\nprint(lm_treelight_add_py.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  light   R-squared:                       0.935\nModel:                            OLS   Adj. R-squared:                  0.929\nMethod:                 Least Squares   F-statistic:                     144.9\nDate:                Thu, 25 Jan 2024   Prob (F-statistic):           1.26e-12\nTime:                        08:33:59   Log-Likelihood:                -175.37\nNo. Observations:                  23   AIC:                             356.7\nDf Residuals:                      20   BIC:                             360.1\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=========================================================================================\n                            coef    std err          t      P&gt;|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------------\nIntercept              7962.0316    231.356     34.415      0.000    7479.431    8444.633\nC(species)[T.Conifer] -3113.0265    231.586    -13.442      0.000   -3596.106   -2629.947\ndepth                  -262.1656     39.922     -6.567      0.000    -345.441    -178.891\n==============================================================================\nOmnibus:                        2.068   Durbin-Watson:                   2.272\nProb(Omnibus):                  0.356   Jarque-Bera (JB):                1.677\nSkew:                           0.633   Prob(JB):                        0.432\nKurtosis:                       2.618   Cond. No.                         13.9\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nAgain, I need to work out which group has been used as the baseline.\n\nIt will be the group that comes first alphabetically, so it should be Broadleaf\nThe other way to check would be to look and see which group is not mentioned in the above table. Conifer is mentioned (in the C(species)[T.Conifer] heading) and so again the baseline group is Broadleaf.\n\nThis means that the intercept value and depth coefficient correspond to the Broadleaf group and as a result I know what the equation of one of my lines is:\nBroadleaf:\n\\[\\begin{equation}\nlight = 7962 + -262.2 \\times depth\n\\end{equation}\\]\nIn this example we know that the gradient is the same for both lines (because we explicitly asked to exclude an interaction), so all I need to do is find the intercept value for the Conifer group. Unfortunately, the final value given in C(species)[T.Conifer] does not give me the intercept for Conifer, instead it tells me the difference between the Conifer group intercept and the baseline intercept i.e. the equation for the regression line for conifer woodland is given by:\n\\[\\begin{equation}\nlight = (7962 + -3113) + -262.2 \\times depth\n\\end{equation}\\]\n\\[\\begin{equation}\nlight = 4829 + -262.2 \\times depth\n\\end{equation}\\]\n\n\n\n\n\n8.5.2 Adding custom regression lines\nIn the example above we determined that the interaction term species:depth was not significant. It would be good to visualise the model without the interaction term.\n\nRPython\n\n\nThis is relatively straightforward if we understand the output of the model a bit better.\nFirst of all, we load the broom library. This is part of tidyverse, so you don’t have to install it. It is not loaded by default, hence us loading it. What broom does it changes the format of many common base R outputs into a more tidy format, so we can work with the output in our analyses more easily.\nThe function we use here is called augment(). What this does is take a model object and a dataset and adds information about each observation in the dataset.\n\n# define the model without interaction term\nlm_additive &lt;- lm(light ~ species + depth,\n                  data = treelight)\n\n# load the broom package\nlibrary(broom)\n\n# augment the model\nlm_additive %&gt;% augment()\n\n# A tibble: 23 × 9\n   light species depth .fitted .resid   .hat .sigma .cooksd .std.resid\n   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1 4106. Conifer  1      4587.  -481. 0.191    531. 0.0799      -1.01 \n 2 4934. Conifer  1.75   4390.   544. 0.156    528. 0.0766       1.11 \n 3 4417. Conifer  2.5    4194.   223. 0.128    542. 0.00985      0.449\n 4 4529. Conifer  3.25   3997.   532. 0.105    530. 0.0440       1.06 \n 5 3443. Conifer  4      3800.  -358. 0.0896   538. 0.0163      -0.706\n 6 4640. Conifer  4.75   3604.  1037. 0.0801   486. 0.120        2.03 \n 7 3082. Conifer  5.5    3407.  -325. 0.0769   540. 0.0113      -0.637\n 8 2368. Conifer  6.25   3210.  -842. 0.0801   507. 0.0793      -1.65 \n 9 2777. Conifer  7      3014.  -237. 0.0896   542. 0.00719     -0.468\n10 2419. Conifer  7.75   2817.  -398. 0.105    537. 0.0247      -0.792\n# ℹ 13 more rows\n\n\nThe output shows us lots of data. Our original light values are in the light column and it’s the same for species and depth. What has been added is information about the fitted (or predicted) values based on the light ~ depth + species model we defined.\nThe fitted or predicted values are in the .fitted column, with corresponding residuals in the .resid column. Remember, your data = predicted values + error, so if you would add .fitted + resid then you would end up with your original data again.\nUsing this information we can now plot the regression lines by species:\n\n# plot the regression lines by species\nlm_additive %&gt;%\n  augment() %&gt;% \n  ggplot(aes(x = depth, y = .fitted, colour = species)) +\n  geom_line() +\n  scale_color_brewer(palette = \"Dark2\")\n\n\n\n\nLastly, if we would want to plot the data and regression lines together, we could change the code as follows:\n\n# plot the regression lines\nlm_additive %&gt;%\n  augment() %&gt;% \n  ggplot(aes(x = depth, y = .fitted, colour = species)) +\n  # add the original data points\n  geom_point(data = treelight,\n             aes(x = depth, y = light, colour = species)) +\n  geom_line() +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(x = \"Depth (m)\",\n       y = \"Light intensity (lux)\")\n\n\n\n\n\n\nTo do this, we need to do the following:\n\ncreate a linear model without the interaction term (we did this previously)\nextract the predicted values of the model\nplot these against the original data\n\n\n# get predicted values\nlm_treelight_add_py.predict()\n\narray([4586.83958789, 4390.21542165, 4193.59125541, 3996.96708918,\n       3800.34292294, 3603.7187567 , 3407.09459046, 3210.47042422,\n       3013.84625799, 2817.22209175, 2620.59792551, 2423.97375927,\n       2227.34959303, 7322.87199901, 7047.59816627, 5781.86286681,\n       7543.35323075, 6319.56442008, 7267.03073579, 7773.27242247,\n       5822.76069339, 7766.98044915, 6532.70501628])\n\n\nWe can’t easily use the predicted values in this kind of format, so we’re adding them to the existing data, in a column called .fitted:\n\n# add predicted values to data set\ntreelight_py['.fitted'] = lm_treelight_add_py.predict()\n\n# have a peek at the data\ntreelight_py.head()\n\n   id        light  depth  species      .fitted\n0   1  4105.646110   1.00  Conifer  4586.839588\n1   2  4933.925144   1.75  Conifer  4390.215422\n2   3  4416.527443   2.50  Conifer  4193.591255\n3   4  4528.618186   3.25  Conifer  3996.967089\n4   5  3442.610306   4.00  Conifer  3800.342923\n\n\nNow we can simply plot the data:\n\n# plot the data\n(ggplot(treelight_py,\n        aes(x = \"depth\",\n            y = \"light\",\n            colour = \"species\")) +\n     geom_point() +\n     # add regression lines\n     geom_line(aes(x = \"depth\",\n                   y = \".fitted\",\n                   colour = \"species\")) +\n     scale_color_brewer(type = \"qual\", palette = \"Dark2\") +\n     labs(x = \"Depth (m)\",\n          y = \"Light intensity (lux)\"))"
  },
  {
    "objectID": "materials/cs4_practical_linear-regression-grouped-data.html#assumptions",
    "href": "materials/cs4_practical_linear-regression-grouped-data.html#assumptions",
    "title": "8  Linear regression with grouped data",
    "section": "8.6 Assumptions",
    "text": "8.6 Assumptions\nIn this case we first wanted to check if the interaction was significant, prior to checking the assumptions. If we would have checked the assumptions first, then we would have done that one the full model (with the interaction), then done the ANOVA if everything was OK. We would have then found out that the interaction was not significant, meaning we’d have to re-check the assumptions with the new model. In what order you do it is a bit less important here. The main thing is that you check the assumptions and report on it!\nAnyway, hopefully you’ve got the gist of checking assumptions for linear models by now: diagnostic plots!\n\nRPython\n\n\n\nresid_panel(lm_additive,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\nThe Residuals plot looks OK, no systematic pattern.\nThe Q-Q plot isn’t perfect, but I’m happy with the normality assumption.\nThe Location-Scale plot is OK, some very slight suggestion of heterogeneity of variance, but nothing to be too worried about.\nThe Cook’s D plot shows that all of the points are OK\n\n\n\n\ndgplots(lm_treelight_add_py)\n\n\n\n\n\n\n\n\n\n\n\n\nThe Residuals plot looks OK, no systematic pattern.\nThe Q-Q plot isn’t perfect, but I’m happy with the normality assumption.\nThe Location-Scale plot is OK, some very slight suggestion of heterogeneity of variance, but nothing to be too worried about.\nThe Influential points plot shows that all of the points are OK\n\n\n\n\nWoohoo!"
  },
  {
    "objectID": "materials/cs4_practical_linear-regression-grouped-data.html#dealing-with-interaction",
    "href": "materials/cs4_practical_linear-regression-grouped-data.html#dealing-with-interaction",
    "title": "8  Linear regression with grouped data",
    "section": "8.7 Dealing with interaction",
    "text": "8.7 Dealing with interaction\nIf there had been a significant interaction between the two predictor variables (for example, if light intensity had dropped off significantly faster in conifer woods than in broad leaf woods, in addition to being lower overall, then we would again be looking for two equations for the linear regression, but this time the gradients vary as well. In this case interaction is important and so we need the output from a linear regression that explicitly includes the interaction term:\n\nRPython\n\n\n\nlm(light ~ depth + species + depth:species,\n   data = treelight)\n\nor written using the short-hand:\n\nlm(light ~ depth * species,\n   data = treelight)\n\nThere really is absolutely no difference in the end result. Either way this gives us the following output:\n\n\n\nCall:\nlm(formula = light ~ depth * species, data = treelight)\n\nCoefficients:\n         (Intercept)                 depth        speciesConifer  \n             7798.57               -221.13              -2784.58  \ndepth:speciesConifer  \n              -71.04  \n\n\nAs before the Broadleaf line is used as the baseline regression and we can read off the values for its intercept and slope directly:\nBroadleaf: \\[\\begin{equation}\nlight = 7798.57 + -221.13 \\times depth\n\\end{equation}\\]\nNote that this is different from the previous section, by allowing for an interaction all fitted values will change.\nFor the conifer line we will have a different intercept value and a different gradient value. As before the value underneath speciesConifer gives us the difference between the intercept of the conifer line and the broad leaf line. The new, additional term depth:speciesConifer tells us how the coefficient of depth varies for the conifer line i.e. how the gradient is different. Putting these two together gives us the following equation for the regression line conifer woodland:\nConifer: \\[\\begin{equation}\nlight = (7798.57 + -2784.58) + (-221.13 + -71.04) \\times depth\n\\end{equation}\\]\n\\[\\begin{equation}\nlight = 5014 + -292.2 \\times depth\n\\end{equation}\\]\n\n\nWe’ve actually created this model before, but for clarity we’ll define it here again.\n\n# create a linear model\nmodel = smf.ols(formula = \"light ~ depth * C(species)\",\n                data = treelight_py)\n\n# and get the fitted parameters of the model\nlm_treelight_py = model.fit()\n\nWe get the model parameters as follows:\n\nprint(lm_treelight_py.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  light   R-squared:                       0.938\nModel:                            OLS   Adj. R-squared:                  0.928\nMethod:                 Least Squares   F-statistic:                     95.71\nDate:                Thu, 25 Jan 2024   Prob (F-statistic):           1.19e-11\nTime:                        08:34:03   Log-Likelihood:                -174.91\nNo. Observations:                  23   AIC:                             357.8\nDf Residuals:                      19   BIC:                             362.4\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n===============================================================================================\n                                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------------------\nIntercept                    7798.5655    298.623     26.115      0.000    7173.541    8423.590\nC(species)[T.Conifer]       -2784.5833    442.274     -6.296      0.000   -3710.274   -1858.893\ndepth                        -221.1256     61.802     -3.578      0.002    -350.478     -91.773\ndepth:C(species)[T.Conifer]   -71.0357     81.309     -0.874      0.393    -241.217      99.145\n==============================================================================\nOmnibus:                        1.435   Durbin-Watson:                   2.176\nProb(Omnibus):                  0.488   Jarque-Bera (JB):                1.269\nSkew:                           0.444   Prob(JB):                        0.530\nKurtosis:                       2.267   Cond. No.                         31.8\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nAs before the Broadleaf line is used as the baseline regression and we can read off the values for its intercept and slope directly:\nBroadleaf: \\[\\begin{equation}\nlight = 7798.57 + -221.13 \\times depth\n\\end{equation}\\]\nNote that this is different from the previous section, by allowing for an interaction all fitted values will change.\nFor the conifer line we will have a different intercept value and a different gradient value. As before the value next to C(species)[T.Conifer] gives us the difference between the intercept of the conifer line and the broad leaf line. The interaction term depth:C(species)[T.Conifer] tells us how the coefficient of depth varies for the conifer line i.e. how the gradient is different. Putting these two together gives us the following equation for the regression line conifer woodland:\nConifer: \\[\\begin{equation}\nlight = (7798.57 + -2784.58) + (-221.13 + -71.04) \\times depth\n\\end{equation}\\]\n\\[\\begin{equation}\nlight = 5014 + -292.2 \\times depth\n\\end{equation}\\]\n\n\n\nThese also happen to be exactly the regression lines that you would get by calculating a linear regression on each group’s data separately."
  },
  {
    "objectID": "materials/cs4_practical_linear-regression-grouped-data.html#exercises",
    "href": "materials/cs4_practical_linear-regression-grouped-data.html#exercises",
    "title": "8  Linear regression with grouped data",
    "section": "8.8 Exercises",
    "text": "8.8 Exercises\n\n8.8.1 Clover and yarrow\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nClover and yarrow field trials\nThe data/CS4-clover.csv data set contains information on field trials at three different farms (A, B and C). Each farm recorded the yield of clover in each of ten fields along with the density of yarrow stalks in each field.\n\nInvestigate how clover yield is affected by yarrow stalk density. Is there evidence of competition between the two species?\nIs there a difference between farms?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nRPython\n\n\n\nclover &lt;- read_csv(\"data/CS4-clover.csv\")\n\nThis data set has three variables; yield (which is the response variable), yarrow (which is a continuous predictor variable) and farm (which is the categorical predictor variables). As always we’ll visualise the data first:\n\n# plot the data\nggplot(clover,\n       aes(x = yarrow, y = yield,\n           colour = farm)) +\n  geom_point() +\n  scale_color_brewer(palette = \"Dark2\")\n\n\n\n\nLooking at this plot as it stands, it’s pretty clear that yarrow density has a significant effect on yield, but it’s pretty hard to see from the plot whether there is any effect of farm, or whether there is any interaction. In order to work that out we’ll want to add the regression lines for each farm separately.\n\n# plot the data\nggplot(clover,\n       aes(x = yarrow, y = yield,\n           colour = farm, group = farm)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_color_brewer(palette = \"Dark2\")\n\n\n\n\nThe regression lines are very close together. Although the line for Farm C crosses the other two, it doesn’t look like we’re going to see a very strong interaction - if there is one. There doesn’t seem to be an effect of farm in general, since the lines are not separate from each other.\nLet’s carry out the analysis:\n\nlm_clover &lt;- lm(yield ~ yarrow * farm,\n                data = clover)\n\nanova(lm_clover)\n\nAnalysis of Variance Table\n\nResponse: yield\n            Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nyarrow       1 8538.3  8538.3 28.3143 1.847e-05 ***\nfarm         2    3.8     1.9  0.0063    0.9937    \nyarrow:farm  2  374.7   187.4  0.6213    0.5457    \nResiduals   24 7237.3   301.6                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis confirms our suspicions from looking at the plot. There isn’t any interaction between yarrow and farm. yarrow density has a statistically significant effect on yield but there isn’t any difference between the different farms on the yields of clover.\nLet’s check the assumptions:\n\nresid_panel(lm_clover,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\nThis is a borderline case.\n\nNormality is fine (Q-Q plot)\nThere aren’t any highly influential points (Cook’s D plot)\nThere is a strong suggestion of heterogeneity of variance (Location-Scale plot). Most of the points are relatively close to the regression lines, but the there is a much great spread of points low yarrow density (which corresponds to high yield values, which is what the predicted values correspond to).\nFinally, there is a slight suggestion that the data might not be linear, that it might curve slightly (Residual plot).\n\n\n\n\nclover_py = pd.read_csv(\"data/CS4-clover.csv\")\n\nThis data set has three variables; yield (which is the response variable), yarrow (which is a continuous predictor variable) and farm (which is the categorical predictor variables). As always we’ll visualise the data first:\n\n# plot the data\n(ggplot(clover_py,\n        aes(x = \"yarrow\",\n            y = \"yield\", colour = \"farm\")) +\n     geom_point() +\n     scale_color_brewer(type = \"qual\", palette = \"Dark2\"))\n\n\n\n\nLooking at this plot as it stands, it’s pretty clear that yarrow density has a significant effect on yield, but it’s pretty hard to see from the plot whether there is any effect of farm, or whether there is any interaction. In order to work that out we’ll want to add the regression lines for each farm separately.\n\n# plot the data\n(ggplot(clover_py,\n        aes(x = \"yarrow\",\n            y = \"yield\", colour = \"farm\")) +\n     geom_point() +\n     geom_smooth(method = \"lm\", se = False) +\n     scale_color_brewer(type = \"qual\", palette = \"Dark2\"))\n\n\n\n\nThe regression lines are very close together. Although the line for Farm C crosses the other two, it doesn’t look like we’re going to see a very strong interaction - if there is one. There doesn’t seem to be an effect of farm in general, since the lines are not separate from each other.\nLet’s carry out the analysis. First, we need to change the name of the yield column, because yield is a keyword in Python, and we can’t use it inside the model formula. How annoying.\n\n# rename yield column\nclover_py = clover_py.rename(columns = {\"yield\": \"clover_yield\"})\n\n# create a linear model\nmodel = smf.ols(formula = \"clover_yield ~ yarrow * C(farm)\",\n                data = clover_py)\n\n# and get the fitted parameters of the model\nlm_clover_py = model.fit()\n\nPerform an ANOVA on the model:\n\nsm.stats.anova_lm(lm_clover_py, typ = 2)\n\n                     sum_sq    df          F    PR(&gt;F)\nC(farm)            3.808918   2.0   0.006315  0.993706\nyarrow          8540.833898   1.0  28.322674  0.000018\nyarrow:C(farm)   374.718749   2.0   0.621312  0.545659\nResidual        7237.311354  24.0        NaN       NaN\n\n\nThis confirms our suspicions from looking at the plot. There isn’t any interaction between yarrow and farm. yarrow density has a statistically significant effect on yield but there isn’t any difference between the different farms on the yields of clover.\nLet’s check the assumptions:\n\ndgplots(lm_clover_py)\n\n\n\n\n\n\nThis is a borderline case.\n\nNormality is fine (Q-Q plot)\nThere aren’t any highly influential points (Influential points plot)\nThere is a strong suggestion of heterogeneity of variance (Location-Scale plot). Most of the points are relatively close to the regression lines, but the there is a much great spread of points low yarrow density (which corresponds to high yield values, which is what the predicted values correspond to).\nFinally, there is a slight suggestion that the data might not be linear, that it might curve slightly (Residuals plot).\n\n\n\n\nWe have two options; both of which are arguably OK to do in real life.\n\nWe can claim that these assumptions are well enough met and just report the analysis that we’ve just done.\nWe can decide that the analysis is not appropriate and look for other options.\n\nWe can try to transform the data by taking logs of yield. This might fix both of our problems: taking logs of the response variable has the effect of improving heterogeneity of variance when the Residuals plot is more spread out on the right vs. the left (like ours). It also is appropriate if we think the true relationship between the response and predictor variables is exponential rather than linear (which we might have). We do have the capabilities to try this option.\nWe could try a permutation based approach (beyond the remit of this course, and actually a bit tricky in this situation). This wouldn’t address the non-linearity but it would deal with the variance assumption.\nWe could come up with a specific functional, mechanistic relationship between yarrow density and clover yield based upon other aspects of their biology. For example there might be a threshold effect such that for yarrow densities below a particular value, clover yields are unaffected, but as soon as yarrow values get above that threshold the clover yield decreases (maybe even linearly). This would require a much better understanding of clover-yarrow dynamics (of which I personally know very little).\n\n\nLet’s do a quick little transformation of the data, and repeat our analysis see if our assumptions are better met this time (just for the hell of it):\n\nRPython\n\n\n\n# plot log-transformed data\nggplot(clover,\n       aes(x = yarrow, y = log(yield), colour = farm)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_color_brewer(palette = \"Dark2\")\n\n\n\n\n\n\nTo log-transform our data, we require numpy.\n\n# import numpy\nimport numpy as np\n\nFirst, we create a new column containing the log-transformed data:\n\nclover_py[\"log_yield\"] = np.log(clover_py[\"clover_yield\"])\n\nThen we can plot them:\n\n# plot log-transformed data\n(ggplot(clover_py,\n         aes(x = \"yarrow\",\n             y = \"log_yield\", colour = \"farm\")) +\n     geom_point() +\n     geom_smooth(method = \"lm\", se = False) +\n     scale_color_brewer(type = \"qual\", palette = \"Dark2\"))\n\n\n\n\n\n\n\nAgain, this looks plausible. There’s a noticeable outlier from Farm B (data point at the bottom of the plot) but otherwise we see that: there probably isn’t an interaction; there is likely to be an effect of yarrow on log_yield; and there probably isn’t any difference between the farms.\nLet’s do the analysis:\n\nRPython\n\n\n\n# define linear model\nlm_log_clover &lt;- lm(log(yield) ~ yarrow * farm,\n                    data = clover)\n\nanova(lm_log_clover)\n\nAnalysis of Variance Table\n\nResponse: log(yield)\n            Df  Sum Sq Mean Sq F value   Pr(&gt;F)    \nyarrow       1 10.6815 10.6815 27.3233 2.34e-05 ***\nfarm         2  0.0862  0.0431  0.1103   0.8960    \nyarrow:farm  2  0.8397  0.4199  1.0740   0.3575    \nResiduals   24  9.3823  0.3909                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWoop. All good so far. We have the same conclusions as before in terms of what is significant and what isn’t. Now we just need to check the assumptions:\n\nresid_panel(lm_log_clover,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n# create a linear model\nmodel = smf.ols(formula = \"log_yield ~ yarrow * C(farm)\",\n                data = clover_py)\n\n# and get the fitted parameters of the model\nlm_log_clover_py = model.fit()\n\nPerform an ANOVA on the model:\n\nsm.stats.anova_lm(lm_clover_py, typ = 2)\n\n                     sum_sq    df          F    PR(&gt;F)\nC(farm)            3.808918   2.0   0.006315  0.993706\nyarrow          8540.833898   1.0  28.322674  0.000018\nyarrow:C(farm)   374.718749   2.0   0.621312  0.545659\nResidual        7237.311354  24.0        NaN       NaN\n\n\nWoop. All good so far. We have the same conclusions as before in terms of what is significant and what isn’t. Now we just need to check the assumptions:\n\ndgplots(lm_clover_py)\n\n\n\n\n\n\n\n\n\nWell, this is actually a better set of diagnostic plots. Whilst one data point (for example in the Q-Q plot) is a clear outlier, if we ignore that point then all of the other plots do look better.\nSo now we know that yarrow is a significant predictor of yield and we’re happy that the assumptions have been met."
  },
  {
    "objectID": "materials/cs4_practical_linear-regression-grouped-data.html#summary",
    "href": "materials/cs4_practical_linear-regression-grouped-data.html#summary",
    "title": "8  Linear regression with grouped data",
    "section": "8.9 Summary",
    "text": "8.9 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nA linear regression analysis with grouped data is used when we have one categorical and one continuous predictor variable, together with one continuous response variable\nWe can visualise the data by plotting a regression line together with the original data\nWhen performing an ANOVA, we need to check for interaction terms\nWe check the underlying assumptions using diagnostic plots\nWe can create an equation for the regression line for each group in the data using the parameter from the linear model output"
  },
  {
    "objectID": "materials/cs5_practical_multiple-linear-regression.html#libraries-and-functions",
    "href": "materials/cs5_practical_multiple-linear-regression.html#libraries-and-functions",
    "title": "9  Multiple linear regression",
    "section": "9.1 Libraries and functions",
    "text": "9.1 Libraries and functions\n\n\n\n\n\n\nClick to expand\n\n\n\n\n\n\nRPython\n\n\n\n9.1.1 Libraries\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n# Converts stats functions to a tidyverse-friendly format\nlibrary(rstatix)\n\n# Creates diagnostic plots using ggplot2\nlibrary(ggResidpanel)\n\n# Helper functions for tidying data\nlibrary(broom)\n\n\n\n9.1.2 Functions\n\n# Gets underlying data out of model object\nbroom::augment()\n\n# Creates diagnostic plots\nggResidpanel::resid_panel()\n\n# Performs an analysis of variance\nstats::anova()\n\n# Creates a linear model\nstats::lm()\n\n\n\n\n\n9.1.3 Libraries\n\n# A fundamental package for scientific computing in Python\nimport numpy as np\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Simple yet exhaustive stats functions.\nimport pingouin as pg\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Statistical models, conducting tests and statistical data exploration\nimport statsmodels.api as sm\n\n# Convenience interface for specifying models using formula strings and DataFrames\nimport statsmodels.formula.api as smf\n\n\n\n9.1.4 Functions\n\n# Reads in a .csv file\npandas.read_csv()\n\n# Creates a model from a formula and data frame\nstatsmodels.formula.api.ols()"
  },
  {
    "objectID": "materials/cs5_practical_multiple-linear-regression.html#purpose-and-aim",
    "href": "materials/cs5_practical_multiple-linear-regression.html#purpose-and-aim",
    "title": "9  Multiple linear regression",
    "section": "9.2 Purpose and aim",
    "text": "9.2 Purpose and aim\nRevisiting the linear model framework and expanding to systems with three predictor variables."
  },
  {
    "objectID": "materials/cs5_practical_multiple-linear-regression.html#data-and-hypotheses",
    "href": "materials/cs5_practical_multiple-linear-regression.html#data-and-hypotheses",
    "title": "9  Multiple linear regression",
    "section": "9.3 Data and hypotheses",
    "text": "9.3 Data and hypotheses\nThe data set we’ll be using is located in data/CS5-pm2_5.csv. It contains data on air pollution levels measured in London, in 2019. It also contains several meteorological measurements. Each variable was recorded on a daily basis.\nNote: some of the variables are based on simulations.\nIt contains the following variables:\n\n\n\nvariable\nexplanation\n\n\n\n\navg_temp\naverage daily temperature (\\(^\\circ C\\))\n\n\ndate\ndate of record\n\n\nlocation\nlocation in London (inner or outer)\n\n\npm2_5\nconcentration of PM2.5 (\\(\\mu g / m^3\\))\n\n\nrain_mm\ndaily rainfall in mm (same across both locations)\n\n\nwind_m_s\nwind speed in \\(m/s\\)"
  },
  {
    "objectID": "materials/cs5_practical_multiple-linear-regression.html#summarise-and-visualise",
    "href": "materials/cs5_practical_multiple-linear-regression.html#summarise-and-visualise",
    "title": "9  Multiple linear regression",
    "section": "9.4 Summarise and visualise",
    "text": "9.4 Summarise and visualise\n\nRPython\n\n\nLet’s first load the data:\n\npm2_5 &lt;- read_csv(\"data/CS5-pm2_5.csv\")\n\nRows: 730 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): date, location\ndbl (4): avg_temp, pm2_5, rain_mm, wind_m_s\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(pm2_5)\n\n# A tibble: 6 × 6\n  avg_temp date       location pm2_5 rain_mm wind_m_s\n     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1      4.5 01/01/2019 inner     17.1     2.3     3.87\n2      4.9 01/01/2019 outer     10.8     2.3     5.84\n3      4.3 02/01/2019 inner     14.9     2.3     3.76\n4      4.8 02/01/2019 outer     11.4     2.3     6   \n5      4   03/01/2019 inner     18.5     1.4     2.13\n6      4.5 03/01/2019 outer     15.0     1.4     2.57\n\n\nIt’s the pm2_5 response variable we’re interested in here. Let’s start by checking if there might be a difference between PM2.5 level in inner and outer London:\n\nggplot(pm2_5,\n       aes(x = location, y = pm2_5)) +\n    geom_boxplot() +\n    geom_jitter(width = 0.1, alpha = 0.3)\n\n\n\n\nI’ve added the (jittered) data to the plot, with some transparency (alpha = 0.3). It’s always good to look at the actual data and not just summary statistics (which is what the box plot is).\nThere seems to be quite a difference between the PM2.5 levels in the two London areas, with the levels in inner London being markedly higher. I’m not surprised by this! So when we do our statistical testing, I would expect to find a clear difference between the locations.\nApart from the location, there are quite a few numerical descriptor variables. We could plot them one-by-one, but that’s a bit tedious. So instead we use the pairs() function again. This only works on numerical data, so we select all the columns that are numeric with select_if(is.numeric):\n\npm2_5 %&gt;% \n    select_if(is.numeric) %&gt;% \n    pairs(lower.panel = NULL)\n\n\n\n\nWe can see that there is not much of a correlation between pm2_5 and avg_temp or rain_mm, whereas there might be something going on in relation to wind_m_s.\nOther notable things include that rainfall seems completely independent of wind speed (rain fall seems pretty constant). Nor does the average temperature seem in any way related to wind speed (it looks like a random collection of data points!).\nWe can visualise the relationship between pm2_5 and wind_m_s in a bit more detail, by plotting the data against each other and colouring by location:\n\nggplot(pm2_5,\n       aes(x = wind_m_s, y = pm2_5,\n           colour = location)) +\n    geom_point()\n\n\n\n\nThis seems to show that there might be some linear relationship between PM2.5 levels and wind speed.\nAnother way of looking at this would be to create a correlation matrix, like we did before in the correlations chapter:\n\npm2_5 %&gt;% \n    select_if(is.numeric) %&gt;% \n    cor()\n\n            avg_temp       pm2_5     rain_mm    wind_m_s\navg_temp  1.00000000  0.03349457  0.03149221 -0.01107855\npm2_5     0.03349457  1.00000000 -0.02184951 -0.41733945\nrain_mm   0.03149221 -0.02184951  1.00000000  0.04882097\nwind_m_s -0.01107855 -0.41733945  0.04882097  1.00000000\n\n\nThis confirms what we saw in the plots, there aren’t any very strong correlations between the different (numerical) variables, apart from a negative correlation between pm2_5 and wind_m_s, which has a Pearson’s r of \\(r\\) = -0.42.\n\n\nLet’s first load the data:\n\npm2_5_py = pd.read_csv(\"data/CS5-pm2_5.csv\")\n\npm2_5_py.head()\n\n   avg_temp        date location   pm2_5  rain_mm  wind_m_s\n0       4.5  01/01/2019    inner  17.126      2.3      3.87\n1       4.9  01/01/2019    outer  10.821      2.3      5.84\n2       4.3  02/01/2019    inner  14.884      2.3      3.76\n3       4.8  02/01/2019    outer  11.416      2.3      6.00\n4       4.0  03/01/2019    inner  18.471      1.4      2.13\n\n\nIt’s the pm2_5 response variable we’re interested in here. Let’s start by checking if there might be a difference between PM2.5 level in inner and outer London:\n\n(ggplot(pm2_5_py, aes(x = \"location\", y = \"pm2_5\")) +\n    geom_boxplot() +\n    geom_jitter(width = 0.1, alpha = 0.7))\n\n\n\n\nI’ve added the (jittered) data to the plot, with some transparency (alpha = 0.7). It’s always good to look at the actual data and not just summary statistics (which is what the box plot is).\nThere seems to be quite a difference between the PM2.5 levels in the two London areas, with the levels in inner London being markedly higher. I’m not surprised by this! So when we do our statistical testing, I would expect to find a clear difference between the locations.\nApart from the location, there are quite a few numerical descriptor variables. At this point I should probably bite the bullet and install seaborn, so I can use the pairplot() function.\nBut I’m not going to ;-)\nI’ll just tell you that there is not much of a correlation between pm2_5 and avg_temp or rain_mm, whereas there might be something going on in relation to wind_m_s. So I plot that instead and colour it by location:\n\n(ggplot(pm2_5_py,\n        aes(x = \"wind_m_s\",\n            y = \"pm2_5\",\n            colour = \"location\")) +\n     geom_point())\n\n\n\n\nThis seems to show that there might be some linear relationship between PM2.5 levels and wind speed.\nIf I would plot all the other variables against each other, then I would spot that rainfall seems completely independent of wind speed (rain fall seems pretty constant). Nor does the average temperature seem in any way related to wind speed (it looks like a random collection of data points!). You can check this yourself!\nAnother way of looking at this would be to create a correlation matrix, like we did before in the correlations chapter:\n\npm2_5_py.corr()\n\n          avg_temp     pm2_5   rain_mm  wind_m_s\navg_temp  1.000000  0.033495  0.031492 -0.011079\npm2_5     0.033495  1.000000 -0.021850 -0.417339\nrain_mm   0.031492 -0.021850  1.000000  0.048821\nwind_m_s -0.011079 -0.417339  0.048821  1.000000\n\n\nThis confirms what we saw in the plots, there aren’t any very strong correlations between the different (numerical) variables, apart from a negative correlation between pm2_5 and wind_m_s, which has a Pearson’s r of \\(r\\) = -0.42."
  },
  {
    "objectID": "materials/cs5_practical_multiple-linear-regression.html#implement-and-interpret-the-test",
    "href": "materials/cs5_practical_multiple-linear-regression.html#implement-and-interpret-the-test",
    "title": "9  Multiple linear regression",
    "section": "9.5 Implement and interpret the test",
    "text": "9.5 Implement and interpret the test\nFrom our initial observations we derived that there might be some relationship between PM2.5 levels and wind speed. We also noticed that this is likely to be different between inner and outer London.\nIf we would want to test for every variable and interaction, then we would end up with a rather huge model, which would even include 3-way and a 4-way interaction! To illustrate the point that the process of model testing applies to as many variables as you like, we’re adding the avg_temp and rain_mm variables to our model.\nSo in this case we create a model that takes into account all of the main effects (avg_temp, location, rain_mm, wind_m_s). We also include a potential two-way interaction (location:wind_m_s). The two-way interaction may be of interest since the PM2.5 levels in response to wind speed seem to differ between the two locations.\nOur model is then as follows:\npm2_5 ~ avg_temp + location + rain_mm + wind_m_s + wind_m_s:location\nSo let’s define and explore it!\n\nRPython\n\n\nWe write the model as follows:\n\nlm_pm2_5_full &lt;- lm(pm2_5 ~ avg_temp + location +\n                            rain_mm + wind_m_s +\n                            wind_m_s:location,\n                    data = pm2_5)\n\nLet’s look at the coefficients:\n\nlm_pm2_5_full\n\n\nCall:\nlm(formula = pm2_5 ~ avg_temp + location + rain_mm + wind_m_s + \n    wind_m_s:location, data = pm2_5)\n\nCoefficients:\n           (Intercept)                avg_temp           locationouter  \n              18.18286                 0.01045                -2.07084  \n               rain_mm                wind_m_s  locationouter:wind_m_s  \n              -0.02788                -0.28545                -0.42945  \n\n\n\n\n\n\n\n\nExtracting coefficients with tidy()\n\n\n\n\n\nThis will give us quite a few coefficients, so instead of just calling the lm object, I’m restructuring the output using the tidy() function from the broom package. It’s installed with tidyverse but you have to load it separately using library(broom).\n\nlm_pm2_5_full %&gt;%\n    tidy() %&gt;% \n    select(term, estimate)\n\n# A tibble: 6 × 2\n  term                   estimate\n  &lt;chr&gt;                     &lt;dbl&gt;\n1 (Intercept)             18.2   \n2 avg_temp                 0.0105\n3 locationouter           -2.07  \n4 rain_mm                 -0.0279\n5 wind_m_s                -0.285 \n6 locationouter:wind_m_s  -0.429 \n\n\n\n\n\nThe question is, are all of these terms statistically significant? To find out we perform an ANOVA:\n\nanova(lm_pm2_5_full)\n\nAnalysis of Variance Table\n\nResponse: pm2_5\n                   Df  Sum Sq Mean Sq   F value  Pr(&gt;F)    \navg_temp            1    5.29    5.29    5.0422 0.02504 *  \nlocation            1 3085.37 3085.37 2940.5300 &lt; 2e-16 ***\nrain_mm             1    2.48    2.48    2.3644 0.12457    \nwind_m_s            1  728.13  728.13  693.9481 &lt; 2e-16 ***\nlocation:wind_m_s   1  134.82  134.82  128.4912 &lt; 2e-16 ***\nResiduals         724  759.66    1.05                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFrom this we can see that the interaction between location and wind_m_s is statistically significant. Which means that we can’t just talk about the effect of location or wind_m_s on PM2.5 levels, without taking the other variable into account!\nThe p-value for the avg_temp is significant, whereas the rain_mm main effect is not. This means that rain fall is not contributing much to model’s ability to explain our data. This matches what we already saw when we visualised the data.\nWhat to do? We’ll explore this in more detail in the chapter on model comparisons, but for now the most sensible option would be to redefine the model, but exclude the rain_mm variable. Here I have rewritten the model and named it lm_pm2_5_red to indicate it is a reduced model (with fewer variables than our original full model):\n\nlm_pm2_5_red &lt;- lm(pm2_5 ~ avg_temp + location + wind_m_s + location:wind_m_s, data = pm2_5)\n\nLet’s look at the new model coefficients:\n\nlm_pm2_5_red\n\n\nCall:\nlm(formula = pm2_5 ~ avg_temp + location + wind_m_s + location:wind_m_s, \n    data = pm2_5)\n\nCoefficients:\n           (Intercept)                avg_temp           locationouter  \n              18.13930                 0.01031                -2.07380  \n              wind_m_s  locationouter:wind_m_s  \n              -0.28631                -0.42880  \n\n\nAs we did in the linear regression on grouped data, we end up with two linear equations, one for inner London and one for outer London.\nOur reference group is inner (remember, it takes a reference group in alphabetical order and we can see outer in the output).\nSo we end up with:\n\\(PM2.5_{inner} = 18.14 + 0.01 \\times avg\\_temp - 0.29 \\times wind\\_m\\_s\\)\n\\(PM2.5_{outer} = (18.14 - 2.07) + 0.01 \\times avg\\_temp + (-0.29 - 0.43) \\times wind\\_m\\_s\\)\nwhich gives\n\\(PM2.5_{outer} = 16.07 + 0.01 \\times avg\\_temp - 0.72 \\times wind\\_m\\_s\\)\nWe still need to check the assumptions of the model:\n\nresid_panel(lm_pm2_5_red,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThey all look pretty good, with the only weird thing being a small empty zone of predicted values just under 16. Nothing that is getting me worried though.\nIt’d be useful to visualise the model. We can take the model and use the augment() function to extract the fitted values (.fitted). These are the values for pm2_5 that the model is predicting. We can then plot these against the wind_m_s measurements, colouring by location:\n\nlm_pm2_5_red %&gt;% \n  augment() %&gt;% \n  ggplot(aes(x = wind_m_s,\n             y = pm2_5, colour = location)) +\n  geom_point() +\n  geom_smooth(aes(y = .fitted))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\nWe write the model as follows:\n\n# create a linear model\nmodel = smf.ols(formula = \"pm2_5 ~ avg_temp + C(location) + rain_mm + wind_m_s + wind_m_s:location\", data = pm2_5_py)\n# and get the fitted parameters of the model\nlm_pm2_5_full_py = model.fit()\n\nThis will give us quite a few coefficients, so instead of just printing the entire summary table, we’re extracting the parameters with .params:\n\nlm_pm2_5_full_py.params\n\nIntercept                     18.182858\nC(location)[T.outer]          -2.070843\navg_temp                       0.010451\nrain_mm                       -0.027880\nwind_m_s                      -0.285450\nwind_m_s:location[T.outer]    -0.429455\ndtype: float64\n\n\nThe question is, are all of these terms statistically significant? To find out we perform an ANOVA:\n\nsm.stats.anova_lm(lm_pm2_5_full_py, typ = 2)\n\n                       sum_sq     df           F         PR(&gt;F)\nC(location)        123.803830    1.0  117.991919   1.441933e-25\navg_temp             1.803685    1.0    1.719012   1.902360e-01\nrain_mm              0.350639    1.0    0.334179   5.633886e-01\nwind_m_s           728.129799    1.0  693.948101  8.928636e-108\nwind_m_s:location  134.820234    1.0  128.491164   1.567268e-27\nResidual           759.661960  724.0         NaN            NaN\n\n\nFrom this we can see that the interaction between location and wind_m_s is statistically significant. Which means that we can’t just talk about the effect of location or wind_m_s on PM2.5 levels, without taking the other variable into account!\nThe p-value for the avg_temp is significant, whereas the rain_mm main effect is not. This means that rain fall is not contributing much to model’s ability to explain our data. This matches what we already saw when we visualised the data.\nWhat to do? We’ll explore this in more detail in the chapter on model comparisons, but for now the most sensible option would be to redefine the model, but exclude the rain_mm variable. Here I have rewritten the model and named it lm_pm2_5_red to indicate it is a reduced model (with fewer variables than our original full model):\n\n# create a linear model\nmodel = smf.ols(formula = \"pm2_5 ~ avg_temp + C(location) * wind_m_s\", data = pm2_5_py)\n# and get the fitted parameters of the model\nlm_pm2_5_red_py = model.fit()\n\nLet’s look at the new model coefficients:\n\nlm_pm2_5_red_py.params\n\nIntercept                        18.139300\nC(location)[T.outer]             -2.073802\navg_temp                          0.010312\nwind_m_s                         -0.286313\nC(location)[T.outer]:wind_m_s    -0.428800\ndtype: float64\n\n\nAs we did in the linear regression on grouped data, we end up with two linear equations, one for inner London and one for outer London.\nOur reference group is inner (remember, it takes a reference group in alphabetical order and we can see outer in the output).\nSo we end up with:\n\\(PM2.5_{inner} = 18.14 + 0.01 \\times avg\\_temp - 0.29 \\times wind\\_m\\_s\\)\n\\(PM2.5_{outer} = (18.14 - 2.07) + 0.01 \\times avg\\_temp + (-0.29 - 0.43) \\times wind\\_m\\_s\\)\nwhich gives\n\\(PM2.5_{outer} = 16.07 + 0.01 \\times avg\\_temp - 0.72 \\times wind\\_m\\_s\\)\nWe still need to check the assumptions of the model:\n\ndgplots(lm_pm2_5_red_py)\n\n\n\n\n\n\n\n\n\n\n\nThey all look pretty good, with the only weird thing being a small empty zone of predicted values just under 16. Nothing that is getting me worried though.\nIt’d be useful to visualise the model. We can take the model and extract the fitted values (.fittedvalues). These are the pm2_5 that the model is predicting. We can then plot these against the wind_m_s measurements, colouring by location. We’re also adding the original values to the plot with geom_point():\n\n(ggplot(pm2_5_py, aes(x = \"wind_m_s\",\n                     y = \"pm2_5\",\n                     colour = \"location\")) +\n    geom_point() +\n    geom_smooth(aes(y = lm_pm2_5_red_py.fittedvalues)))"
  },
  {
    "objectID": "materials/cs5_practical_multiple-linear-regression.html#exploring-models",
    "href": "materials/cs5_practical_multiple-linear-regression.html#exploring-models",
    "title": "9  Multiple linear regression",
    "section": "9.6 Exploring models",
    "text": "9.6 Exploring models\nRather than stop here however, we will use the concept of the linear model to its full potential and show that we can construct and analyse any possible combination of predictor variables for this data set. Namely we will consider the following four extra models, where reduce the complexity to the model, step-by-step:\n\n\n\nModel\nDescription\n\n\n\n\n1. pm2_5 ~ wind_m_s + location\nAn additive model\n\n\n2. pm2_5 ~ wind_m_s\nEquivalent to a simple linear regression\n\n\n3. pm2_5 ~ location\nEquivalent to a one-way ANOVA\n\n\n4. pm2_5 ~ 1\nThe null model, where we have no predictors\n\n\n\n\n9.6.1 Additive model\nTo create the additive model, we drop the interaction term (keep in mind, this is to demonstrate the process - we would normally not do this because the interaction term is significant!).\n\nRPython\n\n\nFirst, we define the model:\n\nlm_pm2_5_add &lt;- lm(pm2_5 ~ avg_temp + location + wind_m_s,\n                   data = pm2_5)\n\nWe can visualise this as follows:\n\nlm_pm2_5_add %&gt;% \n    augment() %&gt;% \n    ggplot(aes(x = wind_m_s, y = pm2_5,\n               colour = location)) +\n    geom_point() +\n    geom_smooth(aes(y = .fitted))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nNext, we extract the coefficient estimates:\n\nlm_pm2_5_add\n\n\nCall:\nlm(formula = pm2_5 ~ avg_temp + location + wind_m_s, data = pm2_5)\n\nCoefficients:\n  (Intercept)       avg_temp  locationouter       wind_m_s  \n     19.04867        0.01587       -4.05339       -0.49868  \n\n\n\n\nFirst, we define the model\n\n# create a linear model\nmodel = smf.ols(formula = \"pm2_5 ~ avg_temp + C(location) + wind_m_s\",\n                data = pm2_5_py)\n                \n# and get the fitted parameters of the model\nlm_pm2_5_add_py = model.fit()\n\nWe can visualise this as follows:\n\n(ggplot(pm2_5_py, aes(x = \"wind_m_s\",\n                      y = \"pm2_5\",\n                      colour = \"location\")) +\n    geom_point() +\n    geom_smooth(aes(y = lm_pm2_5_add_py.fittedvalues)))\n\n\n\n\nNext, we extract the coefficient estimates:\n\nlm_pm2_5_add_py.params\n\nIntercept               19.048672\nC(location)[T.outer]    -4.053394\navg_temp                 0.015872\nwind_m_s                -0.498683\ndtype: float64\n\n\n\n\n\nSo our two equations would be as follows:\n\\(PM2.5_{inner} = 19.04 + 0.016 \\times avg\\_temp - 0.50 \\times wind\\_m\\_s\\)\n\\(PM2.5_{outer} = (19.22 - 4.05) + 0.016 \\times avg\\_temp - 0.50 \\times wind\\_m\\_s\\)\ngives\n\\(PM2.5_{outer} = 15.17 + 0.016 \\times avg\\_temp - 0.50 \\times wind\\_m\\_s\\)\n\n\n9.6.2 Revisiting linear regression\n\nRPython\n\n\nFirst, we define the model:\n\nlm_pm2_5_wind &lt;- lm(pm2_5 ~ wind_m_s,\n                   data = pm2_5)\n\nWe can visualise this as follows:\n\nlm_pm2_5_wind %&gt;% \n    augment() %&gt;% \n    ggplot(aes(x = wind_m_s, y = pm2_5)) +\n    geom_point() +\n    geom_smooth(aes(y = .fitted))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nAlternative using geom_smooth()\n\n\n\n\n\n\nggplot(pm2_5, aes(x = wind_m_s, y = pm2_5)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nNext, we extract the coefficient estimates:\n\nlm_pm2_5_wind\n\n\nCall:\nlm(formula = pm2_5 ~ wind_m_s, data = pm2_5)\n\nCoefficients:\n(Intercept)     wind_m_s  \n    17.3267      -0.5285  \n\n\n\n\nFirst, we define the model\n\n# create a linear model\nmodel = smf.ols(formula = \"pm2_5 ~ wind_m_s\",\n                data = pm2_5_py)\n                \n# and get the fitted parameters of the model\nlm_pm2_5_wind_py = model.fit()\n\nWe can visualise this as follows:\n\n(ggplot(pm2_5_py, aes(x = \"wind_m_s\",\n                      y = \"pm2_5\")) +\n    geom_point() +\n    geom_smooth(aes(y = lm_pm2_5_wind_py.fittedvalues), colour = \"blue\"))\n\n\n\n\n\n\n\n\n\n\nAlternative using geom_smooth()\n\n\n\n\n\n\n(ggplot(pm2_5_py, aes(x = \"wind_m_s\", y = \"pm2_5\")) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = False, colour = \"blue\"))\n\n\n\n\n\n\n\nNext, we extract the coefficient estimates:\n\nlm_pm2_5_wind_py.params\n\nIntercept    17.326708\nwind_m_s     -0.528510\ndtype: float64\n\n\n\n\n\nThis gives us the following equation:\n\\(PM2.5 = 17.33 - 0.53 \\times wind\\_m\\_s\\)\n\n\n9.6.3 Revisiting ANOVA\nIf we’re just looking at the effect of location, then we’re essentially doing a one-way ANOVA.\n\nRPython\n\n\nFirst, we define the model:\n\nlm_pm2_5_loc &lt;- lm(pm2_5 ~ location,\n                   data = pm2_5)\n\nWe can visualise this as follows:\n\nlm_pm2_5_loc %&gt;% \n    augment() %&gt;% \n    ggplot(aes(x = location, y = pm2_5)) +\n    geom_jitter(alpha = 0.3, width = 0.1) +\n    geom_point(aes(y = .fitted), colour = \"blue\", size = 3)\n\n\n\n\nOK, what’s going on here? I’ve plotted the .fitted values (the values predicted by the model) in blue and overlaid the original (with a little bit of jitter to avoid overplotting). However, there are only two predicted values!\nWe can check this and see that each unique fitted value occurs 365 times:\n\nlm_pm2_5_loc %&gt;% \n    augment() %&gt;% \n    count(location, .fitted)\n\n# A tibble: 2 × 3\n  location .fitted     n\n  &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;\n1 inner       16.9   365\n2 outer       12.8   365\n\n\nThis makes sense if we think back to our original ANOVA exploration. There we established that an ANOVA is just a special case of a linear model, where the fitted values are equal to the mean of each group.\nWe could even check this:\n\npm2_5 %&gt;% \n    group_by(location) %&gt;% \n    summarise(mean_pm2_5 = mean(pm2_5))\n\n# A tibble: 2 × 2\n  location mean_pm2_5\n  &lt;chr&gt;         &lt;dbl&gt;\n1 inner          16.9\n2 outer          12.8\n\n\nSo, that matches. We move on and extract the coefficient estimates:\n\nlm_pm2_5_loc\n\n\nCall:\nlm(formula = pm2_5 ~ location, data = pm2_5)\n\nCoefficients:\n  (Intercept)  locationouter  \n       16.943         -4.112  \n\n\nThese values match up exactly with the predicted values for each individual location.\n\n\nFirst, we define the model:\n\n# create a linear model\nmodel = smf.ols(formula = \"pm2_5 ~ C(location)\", data = pm2_5_py)\n# and get the fitted parameters of the model\nlm_pm2_5_loc_py = model.fit()\n\nWe can visualise this as follows:\n\n(ggplot(pm2_5_py, aes(x = \"location\",\n                     y = \"pm2_5\")) +\n    geom_jitter(alpha = 0.3, width = 0.1) +\n    geom_point(aes(y = lm_pm2_5_loc_py.fittedvalues), colour = \"blue\", size = 3))\n\n\n\n\nOK, what’s going on here? I’ve plotted the fittedvalues (the values predicted by the model) in blue and overlaid the original (with a little bit of jitter to avoid overplotting). However, there are only two predicted values!\nWe can check this and see that each unique fitted value occurs 365 times, using the value_counts() function on the fitted values:\n\nlm_pm2_5_loc_py.fittedvalues.value_counts()\n\n16.942926    365\n12.831356    365\ndtype: int64\n\n\nThis makes sense if we think back to our original ANOVA exploration. There we established that an ANOVA is just a special case of a linear model, where the fitted values are equal to the mean of each group.\nWe could even check this:\n\npm2_5_py.groupby(\"location\")[\"pm2_5\"].mean()\n\nlocation\ninner    16.942926\nouter    12.831356\nName: pm2_5, dtype: float64\n\n\nSo, that matches. We move on and extract the coefficient estimates:\n\nlm_pm2_5_loc_py.params\n\nIntercept               16.942926\nC(location)[T.outer]    -4.111570\ndtype: float64\n\n\nThese values match up exactly with the predicted values for each individual location.\n\n\n\nThis gives us the following equation:\n\\(\\bar{PM2.5_{inner}} = 16.94\\)\n\\(\\bar{PM2.5_{outer}} = 16.94 - 4.11 = 12.83\\)\n\n\n9.6.4 The null model\nThe null model by itself is rarely analysed for its own sake but is instead used a reference point for more sophisticated model selection techniques. It represents your data as an overal average value.\n\nRPython\n\n\nWe define the null model as follows:\n\nlm_pm2_5_null &lt;- lm(pm2_5 ~ 1, data = pm2_5)\n\nWe can just view the model:\n\nlm_pm2_5_null\n\n\nCall:\nlm(formula = pm2_5 ~ 1, data = pm2_5)\n\nCoefficients:\n(Intercept)  \n      14.89  \n\n\n\n\nWe define the null model as follows:\n\n# create a linear model\nmodel = smf.ols(formula = \"pm2_5 ~ 1\", data = pm2_5_py)\n# and get the fitted parameters of the model\nlm_pm2_5_null_py = model.fit()\n\nWe can just view the model parameters:\n\nlm_pm2_5_null_py.params\n\nIntercept    14.887141\ndtype: float64\n\n\n\n\n\nThis shows us that there is just one value: 14.89. This is the average across all the PM2.5 values in the data set.\nHere we’d predict the PM2.5 values as follows:\n\\(PM2.5 = 14.89\\)"
  },
  {
    "objectID": "materials/cs5_practical_multiple-linear-regression.html#exercises",
    "href": "materials/cs5_practical_multiple-linear-regression.html#exercises",
    "title": "9  Multiple linear regression",
    "section": "9.7 Exercises",
    "text": "9.7 Exercises\n\n9.7.1 Trees\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nTrees: an example with only continuous variables\nUse the data/CS5-trees.csv data set. This is a data frame with 31 observations of 3 continuous variables. The variables are the height height, diameter girth and timber volume volume of 31 felled black cherry trees.\nInvestigate the relationship between volume (as a dependent variable) and height and girth (as predictor variables).\n\nHere all variables are continuous and so there isn’t a way of producing a 2D plot of all three variables for visualisation purposes using R’s standard plotting functions.\nconstruct four linear models\n\nAssume volume depends on height, girth and an interaction between girth and height\nAssume volume depends on height and girth but that there isn’t any interaction between them.\nAssume volume only depends on girth (plot the result, with the regression line).\nAssume volume only depends on height (plot the result, with the regression line).\n\nFor each linear model write down the algebraic equation that the linear model produces that relates volume to the two continuous predictor variables.\nCheck the assumptions of each model. Do you have any concerns?\n\nNB: For two continuous predictors, the interaction term is simply the two values multiplied together (so girth:height means girth x height)\n\nUse the equations to calculate the predicted volume of a tree that has a diameter of 20 inches and a height of 67 feet in each case.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nLet’s construct the four linear models in turn.\n\nFull model\n\nRPython\n\n\nFirst, we read in the data:\n\ntrees &lt;- read_csv(\"data/CS5-trees.csv\")\n\n\n# define the model\nlm_trees_full &lt;- lm(volume ~ height * girth,\n                   data = trees)\n\n# view the model\nlm_trees_full\n\n\nCall:\nlm(formula = volume ~ height * girth, data = trees)\n\nCoefficients:\n (Intercept)        height         girth  height:girth  \n     69.3963       -1.2971       -5.8558        0.1347  \n\n\n\n\nFirst, we read in the data:\n\ntrees_py = pd.read_csv(\"data/CS5-trees.csv\")\n\n\n# create a linear model\nmodel = smf.ols(formula = \"volume ~ height * girth\",\n                data = trees_py)\n# and get the fitted parameters of the model\nlm_trees_full_py = model.fit()\n\nExtract the parameters:\n\nlm_trees_full_py.params\n\nIntercept       69.396316\nheight          -1.297083\ngirth           -5.855848\nheight:girth     0.134654\ndtype: float64\n\n\n\n\n\nWe can use this output to get the following equation:\nvolume = 69.40 + -1.30 \\(\\times\\) height + -5.86 \\(\\times\\) girth + 0.13 \\(\\times\\) height \\(\\times\\) girth\nIf we stick the numbers in (girth = 20 and height = 67) we get the following equation:\nvolume = 69.40 + -1.30 \\(\\times\\) 67 + -5.86 \\(\\times\\) 20 + 0.13 \\(\\times\\) 67 \\(\\times\\) 20\nvolume = 45.81\nHere we note that the interaction term just requires us to multiple the three numbers together (we haven’t looked at continuous predictors before in the examples and this exercise was included as a check to see if this whole process was making sense).\nIf we look at the diagnostic plots for the model using the following commands we get:\n\nRPython\n\n\n\nresid_panel(lm_trees_full,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\ndgplots(lm_trees_full_py)\n\n\n\n\n\n\n\n\n\nAll assumptions are OK.\n\nThere is some suggestion of heterogeneity of variance (with the variance being lower for small and large fitted (i.e. predicted volume) values), but that can be attributed to there only being a small number of data points at the edges, so I’m not overly concerned.\nSimilarly, there is a suggestion of snaking in the Q-Q plot (suggesting some lack of normality) but this is mainly due to the inclusion of one data point and overall the plot looks acceptable.\nThere are no highly influential points\n\n\n\nAdditive model\n\nRPython\n\n\n\n# define the model\nlm_trees_add &lt;- lm(volume ~ height + girth,\n                   data = trees)\n\n# view the model\nlm_trees_add\n\n\nCall:\nlm(formula = volume ~ height + girth, data = trees)\n\nCoefficients:\n(Intercept)       height        girth  \n   -57.9877       0.3393       4.7082  \n\n\n\n\n\n# create a linear model\nmodel = smf.ols(formula = \"volume ~ height + girth\",\n                data = trees_py)\n# and get the fitted parameters of the model\nlm_trees_add_py = model.fit()\n\nExtract the parameters:\n\nlm_trees_add_py.params\n\nIntercept   -57.987659\nheight        0.339251\ngirth         4.708161\ndtype: float64\n\n\n\n\n\nWe can use this output to get the following equation:\nvolume = -57.99 + 0.34 \\(\\times\\) height + 4.71 \\(\\times\\) girth\nIf we stick the numbers in (girth = 20 and height = 67) we get the following equation:\nvolume = -57.99 + 0.34 \\(\\times\\) 67 + 4.71 \\(\\times\\) 20\nvolume = 58.91\nIf we look at the diagnostic plots for the model using the following commands we get the following:\n\nRPython\n\n\n\nresid_panel(lm_trees_add,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\ndgplots(lm_trees_add_py)\n\n\n\n\n\n\n\n\n\nThis model isn’t great.\n\nThere is a worrying lack of linearity exhibited in the Residuals plot suggesting that this linear model isn’t appropriate.\nAssumptions of Normality seem OK\nEquality of variance is harder to interpret. Given the lack of linearity in the data it isn’t really sensible to interpret the Location-Scale plot as it stands (since the plot is generated assuming that we’ve fitted a straight line through the data), but for the sake of practising interpretation we’ll have a go. There is definitely suggestions of heterogeneity of variance here with a cluster of points with fitted values of around 20 having noticeably lower variance than the rest of the dataset.\nOne point is influential and if there weren’t issues with the linearity of the model I would remove this point and repeat the analysis. As it stands there isn’t much point.\n\n\n\nHeight-only model\n\nRPython\n\n\n\n# define the model\nlm_height &lt;- lm(volume ~ height,\n              data = trees)\n\n# view the model\nlm_height\n\n\nCall:\nlm(formula = volume ~ height, data = trees)\n\nCoefficients:\n(Intercept)       height  \n    -87.124        1.543  \n\n\n\n\n\n# create a linear model\nmodel = smf.ols(formula = \"volume ~ height\",\n                data = trees_py)\n# and get the fitted parameters of the model\nlm_height_py = model.fit()\n\nExtract the parameters:\n\nlm_height_py.params\n\nIntercept   -87.123614\nheight        1.543350\ndtype: float64\n\n\n\n\n\nWe can use this output to get the following equation:\nvolume = -87.12 + 1.54 \\(\\times\\) height\nIf we stick the numbers in (girth = 20 and height = 67) we get the following equation:\nvolume = -87.12 + 1.54 \\(\\times\\) 67\nvolume = 16.28\nIf we look at the diagnostic plots for the model using the following commands we get the following:\n\nRPython\n\n\n\nresid_panel(lm_height,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\ndgplots(lm_height_py)\n\n\n\n\n\n\n\n\n\nThis model also isn’t great.\n\nThe main issue here is the clear heterogeneity of variance. For trees with bigger volumes the data are much more spread out than for trees with smaller volumes (as can be seen clearly from the Location-Scale plot).\nApart from that, the assumption of Normality seems OK\nAnd there aren’t any hugely influential points in this model\n\n\n\nGirth-only model\n\nRPython\n\n\n\n# define the model\nlm_girth &lt;- lm(volume ~ girth,\n               data = trees)\n\n# view the model\nlm_girth\n\n\nCall:\nlm(formula = volume ~ girth, data = trees)\n\nCoefficients:\n(Intercept)        girth  \n    -36.943        5.066  \n\n\n\n\n\n# create a linear model\nmodel = smf.ols(formula = \"volume ~ girth\",\n                data = trees_py)\n# and get the fitted parameters of the model\nlm_girth_py = model.fit()\n\nExtract the parameters:\n\nlm_girth_py.params\n\nIntercept   -36.943459\ngirth         5.065856\ndtype: float64\n\n\n\n\n\nWe can use this output to get the following equation:\nvolume = -36.94 + 5.07 \\(\\times\\) girth\nIf we stick the numbers in (girth = 20 and height = 67) we get the following equation:\nvolume = -36.94 + 5.07 \\(\\times\\) 20\nvolume = 64.37\nIf we look at the diagnostic plots for the model using the following commands we get the following:\n\nRPython\n\n\n\nresid_panel(lm_girth,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\ndgplots(lm_girth_py)\n\n\n\n\n\n\n\n\n\nThe diagnostic plots here look rather similar to the ones we generated for the additive model and we have the same issue with a lack of linearity, heterogeneity of variance and one of the data points being influential."
  },
  {
    "objectID": "materials/cs5_practical_multiple-linear-regression.html#summary",
    "href": "materials/cs5_practical_multiple-linear-regression.html#summary",
    "title": "9  Multiple linear regression",
    "section": "9.8 Summary",
    "text": "9.8 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nWe can define a linear model with any combination of categorical and continuous predictor variables\nUsing the coefficients of the model we can construct the linear model equation\nThe underlying assumptions of a linear model with three (or more) predictor variables are the same as those of a two-way ANOVA"
  },
  {
    "objectID": "materials/cs5_practical_model-comparisons.html#libraries-and-functions",
    "href": "materials/cs5_practical_model-comparisons.html#libraries-and-functions",
    "title": "10  Model comparisons",
    "section": "10.1 Libraries and functions",
    "text": "10.1 Libraries and functions\n\n\n\n\n\n\nClick to expand\n\n\n\n\n\n\nRPython\n\n\n\n10.1.1 Libraries\n\n\n10.1.2 Functions\n\n\n\n\n10.1.3 Libraries\n\n\n10.1.4 Functions"
  },
  {
    "objectID": "materials/cs5_practical_model-comparisons.html#purpose-and-aim",
    "href": "materials/cs5_practical_model-comparisons.html#purpose-and-aim",
    "title": "10  Model comparisons",
    "section": "10.2 Purpose and aim",
    "text": "10.2 Purpose and aim\nIn the previous example we used a single data set and fitted five linear models to it depending on which predictor variables we used. Whilst this was fun (seriously, what else would you be doing right now?) it seems that there should be a “better way”. Well, thankfully there is! In fact there a several methods that can be used to compare different models in order to help identify “the best” model. More specifically, we can determine if a full model (which uses all available predictor variables and interactions) is necessary to appropriately describe the dependent variable, or whether we can throw away some of the terms (e.g. an interaction term) because they don’t offer any useful predictive power.\nHere we will use the Akaike Information Criterion in order to compare different models."
  },
  {
    "objectID": "materials/cs5_practical_model-comparisons.html#data-and-hypotheses",
    "href": "materials/cs5_practical_model-comparisons.html#data-and-hypotheses",
    "title": "10  Model comparisons",
    "section": "10.3 Data and hypotheses",
    "text": "10.3 Data and hypotheses\nThis section uses the data/CS5-ladybird.csv data set. This data set comprises of 20 observations of three variables (one dependent and two predictor). This records the clutch size (eggs) in a species of ladybird, alongside two potential predictor variables; the mass of the female (weight), and the colour of the male (male) which is a categorical variable."
  },
  {
    "objectID": "materials/cs5_practical_model-comparisons.html#backwards-stepwise-elimination",
    "href": "materials/cs5_practical_model-comparisons.html#backwards-stepwise-elimination",
    "title": "10  Model comparisons",
    "section": "10.4 Backwards Stepwise Elimination",
    "text": "10.4 Backwards Stepwise Elimination\nFirst, we load the data and visualise it:\n\nRPython\n\n\n\nladybird &lt;- read_csv(\"data/CS5-ladybird.csv\")\n\nhead(ladybird)\n\n# A tibble: 6 × 4\n     id male  weight  eggs\n  &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1     1 Wild    10.6    27\n2     2 Wild    10.6    26\n3     3 Wild    12.5    28\n4     4 Wild    11.4    24\n5     5 Wild    14.9    30\n6     6 Wild    10.4    21\n\n\nWe can visualise the data by male, so we can see if the eggs clutch size differs a lot between the two groups:\n\nggplot(ladybird,\n       aes(x = male, y = eggs)) +\n    geom_boxplot() +\n    geom_jitter(width = 0.05)\n\n\n\n\nWe can also plot the egg clutch size against the weight, again for each male colour group:\n\n# visualise the data\nggplot(ladybird,\n       aes(x = weight, y = eggs,\n           colour = male)) +\n    geom_point() +\n    scale_color_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\nladybird_py = pd.read_csv(\"data/CS5-ladybird.csv\")\n\nWe can visualise the data by male, so we can see if the eggs clutch size differs a lot between the two groups:\n\n(ggplot(ladybird_py,\n        aes(x = \"male\", y = \"eggs\")) +\n        geom_boxplot() +\n        geom_jitter(width = 0.05))\n\n\n\n\nWe can also plot the egg clutch size against the weight, again for each male colour group:\n\n(ggplot(ladybird_py,\n        aes(x = \"weight\", y = \"eggs\",\n            colour = \"male\")) +\n        geom_point() +\n        scale_color_brewer(type = \"qual\",\n                           palette = \"Dark2\"))\n\n\n\n\n\n\n\nWe can see a few things already:\n\nThere aren’t a huge number of data points in each group, so we need to be a bit cautious with drawing any firm conclusions.\nThere is quite some spread in the egg clutch sizes, with two observations in the Melanic group being rather low.\nFrom the box plot, there does not seem to be much difference in the egg clutch size between the two male colour groups.\nThe scatter plot suggests that egg clutch size seems to increase somewhat linearly as the weight of the female goes up. There does not seem to be much difference between the two male colour groups in this respect.\n\n\n10.4.1 Comparing models with AIC (step 1)\nWe start with the complete or full model, that takes into account any possible interaction between weight and male.\nNext, we define the reduced model. This is the next, most simple model. In this case we’re removing the interaction and constructing an additive model.\n\nRPython\n\n\n\n# define the full model\nlm_full &lt;- lm(eggs ~ weight * male,\n              data = ladybird)\n\n\n# define the additive model\nlm_add &lt;- lm(eggs ~ weight + male,\n             data = ladybird)\n\nWe then extract the AIC values for each of the models:\n\nAIC(lm_full)\n\n[1] 100.0421\n\nAIC(lm_add)\n\n[1] 99.19573\n\n\n\n\n\n# create the model\nmodel = smf.ols(formula= \"eggs ~ weight * C(male)\", data = ladybird_py)\n# and get the fitted parameters of the model\nlm_full_py = model.fit()\n\n\n# create the additive linear model\nmodel = smf.ols(formula= \"eggs ~ weight + C(male)\", data = ladybird_py)\n# and get the fitted parameters of the model\nlm_add_py = model.fit()\n\nWe then extract the AIC values for each of the models:\n\nlm_full_py.aic\n\n98.04206256815984\n\nlm_add_py.aic\n\n97.19572958073036\n\n\n\n\n\nEach line tells you the AIC score for that model. The full model has 4 parameters (the intercept, the coefficient for the continuous variable weight, the coefficient for the categorical variable male and a coefficient for the interaction term weight:male). The additive model has a lower AIC score with only 3 parameters (since we’ve dropped the interaction term). There are different ways of interpreting AIC scores but the most widely used interpretation says that:\n\nif the difference between two AIC scores is greater than 2, then the model with the smallest AIC score is more supported than the model with the higher AIC score\nif the difference between the two models’ AIC scores is less than 2 then both models are equally well supported\n\nThis choice of language (supported vs significant) is deliberate and there are areas of statistics where AIC scores are used differently from the way we are going to use them here (ask if you want a bit of philosophical ramble from me). However, in this situation we will use the AIC scores to decide whether our reduced model is at least as good as the full model. Here since the difference in AIC scores is less than 2, we can say that dropping the interaction term has left us with a model that is both simpler (fewer terms) and as least as good (AIC score) as the full model. As such our additive model eggs ~ weight + male is designated our current working minimal model.\n\n\n10.4.2 Comparing models with AIC (step 2)\nNext, we see which of the remaining terms can be dropped. We will look at the models where we have dropped both male and weight (i.e. eggs ~ weight and eggs ~ male) and compare their AIC values with the AIC of our current minimal model (eggs ~ weight + male). If the AIC values of at least one of our new reduced models is lower (or at least no more than 2 greater) than the AIC of our current minimal model, then we can drop the relevant term and get ourselves a new minimal model. If we find ourselves in a situation where we can drop more than one term we will drop the term that gives us the model with the lowest AIC.\n\nRPython\n\n\nDrop the variable weight and examine the AIC:\n\n# define the model\nlm_male &lt;- lm(eggs ~ male,\n              data = ladybird)\n\n# extract the AIC\nAIC(lm_male)\n\n[1] 118.7093\n\n\nDrop the variable male and examine the AIC:\n\n# define the model\nlm_weight &lt;- lm(eggs ~ weight,\n                data = ladybird)\n\n# extract the AIC\nAIC(lm_weight)\n\n[1] 97.52601\n\n\n\n\nDrop the variable weight and examine the AIC:\n\n# create the model\nmodel = smf.ols(formula= \"eggs ~ C(male)\", data = ladybird_py)\n# and get the fitted parameters of the model\nlm_male_py = model.fit()\n\n# extract the AIC\nlm_male_py.aic\n\n116.7092646564482\n\n\nDrop the variable male and examine the AIC:\n\n# create the model\nmodel = smf.ols(formula= \"eggs ~ weight\", data = ladybird_py)\n# and get the fitted parameters of the model\nlm_weight_py = model.fit()\n\n# extract the AIC\nlm_weight_py.aic\n\n95.52601286248304\n\n\n\n\n\nConsidering both outputs together and comparing with the AIC of our current minimal model, we can see that dropping male has decreased the AIC further, whereas dropping weight has actually increased the AIC and thus worsened the model quality.\nHence we can drop male and our new minimal model is eggs ~ weight.\n\n\n10.4.3 Comparing models with AIC (step 3)\nOur final comparison is to drop the variable weight and compare this simple model with a null model (eggs ~ 1), which assumes that the clutch size is constant across all parameters.\nDrop the variable weight and see if that has an effect:\n\nRPython\n\n\n\n# define the model\nlm_null &lt;- lm(eggs ~ 1,\n              data = ladybird)\n\n# extract the AIC\nAIC(lm_null)\n\n[1] 117.2178\n\n\n\n\n\n# create the model\nmodel = smf.ols(formula= \"eggs ~ 1\", data = ladybird_py)\n# and get the fitted parameters of the model\nlm_null_py = model.fit()\n\n# extract the AIC\nlm_null_py.aic\n\n115.21783038624153\n\n\n\n\n\nThe AIC of our null model is quite a bit larger than that of our current minimal model eggs ~ weight and so we conclude that weight is important. As such our minimal model is eggs ~ weight.\nSo, in summary, we could conclude that:\n\nFemale size is a useful predictor of clutch size, but male type is not so important.\n\nAt this point we can then continue analysing this minimal model, by checking the diagnostic plots and checking the assumptions. If they all pan out, then we can continue with an ANOVA."
  },
  {
    "objectID": "materials/cs5_practical_model-comparisons.html#notes-on-backwards-stepwise-elimination",
    "href": "materials/cs5_practical_model-comparisons.html#notes-on-backwards-stepwise-elimination",
    "title": "10  Model comparisons",
    "section": "10.5 Notes on Backwards Stepwise Elimination",
    "text": "10.5 Notes on Backwards Stepwise Elimination\nThis method of finding a minimal model by starting with a full model and removing variables is called backward stepwise elimination. Although regularly practised in data analysis, there is increasing criticism of this approach, with calls for it to be avoided entirely.\nWhy have we made you work through this procedure then? Given their prevalence in academic papers, it is very useful to be aware of these procedures and to know that there are issues with them. In other situations, using AIC for model comparisons are justified and you will come across them regularly. Additionally, there may be situations where you feel there are good reasons to drop a parameter from your model – using this technique you can justify that this doesn’t affect the model fit. Taken together, using backwards stepwise elimination for model comparison is still a useful technique.\n\n\n\n\n\n\nAutomatic BSE in R (but not Python)\n\n\n\nPerforming backwards stepwise elimination manually can be quite tedious. Thankfully R acknowledges this and there is a single inbuilt function called step() that can perform all of the necessary steps for you using AIC.\n\n# define the full model\nlm_full &lt;- lm(eggs ~ weight * male,\n              data = ladybird)\n\n# perform backwards stepwise elimination\nstep(lm_full)\n\nThis will perform a full backwards stepwise elimination process and will find the minimal model for you.\nYes, I could have told you this earlier, but where’s the fun in that? (it is also useful for you to understand the steps behind the technique I suppose…)\nWhen doing this in Python, you are a bit stuck. There does not seem to be an equivalent function. If you want to cobble something together yourself, then use this link as a starting point."
  },
  {
    "objectID": "materials/cs5_practical_model-comparisons.html#exercises",
    "href": "materials/cs5_practical_model-comparisons.html#exercises",
    "title": "10  Model comparisons",
    "section": "10.6 Exercises",
    "text": "10.6 Exercises\nWe are going to practice the backwards stepwise elimination technique on some of the data sets we analysed previously.\nFor each of the following data sets I would like you to:\n\nDefine the response variable\nDefine the relevant predictor variables\nDefine relevant interactions\nPerform a backwards stepwise elimination and discover the minimal model using AIC\n\nNB: if an interaction term is significant then any main factor that is part of the interaction term cannot be dropped from the model.\nPerform a BSE on the following data sets:\n\ndata/CS5-trees.csv\ndata/CS5-pm2_5.csv\n\n\n10.6.1 BSE: Trees\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nBSE on trees:\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n10.7 Answer\nLet’s start by reading in the data and checking which variables are in the data set.\n\nRPython\n\n\n\ntrees &lt;- read_csv(\"data/CS5-trees.csv\")\n\nhead(trees)\n\n# A tibble: 6 × 3\n  girth height volume\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1   8.3     70   10.3\n2   8.6     65   10.3\n3   8.8     63   10.2\n4  10.5     72   16.4\n5  10.7     81   18.8\n6  10.8     83   19.7\n\n\n\n\nFirst, we read in the data (if needed) and have a look at the variables:\n\ntrees_py = pd.read_csv(\"data/CS5-trees.csv\")\n\ntrees_py.head()\n\n   girth  height  volume\n0    8.3      70    10.3\n1    8.6      65    10.3\n2    8.8      63    10.2\n3   10.5      72    16.4\n4   10.7      81    18.8\n\n\n\n\n\n\nThe response variable is volume\nThe predictor variables are girth and height\nThe only possible interaction term is girth:height\nWe perform a BSE on the model using the step() function\n\nThe full model is volume ~ girth * height.\nWe perform the BSE as follows:\n\nRPython\n\n\nDefine the model and use the step() function:\n\n# define the full model\nlm_trees &lt;- lm(volume ~ girth * height,\n               data = trees)\n\n# perform BSE\nstep(lm_trees)\n\nStart:  AIC=65.49\nvolume ~ girth * height\n\n               Df Sum of Sq    RSS    AIC\n&lt;none&gt;                      198.08 65.495\n- girth:height  1    223.84 421.92 86.936\n\n\n\nCall:\nlm(formula = volume ~ girth * height, data = trees)\n\nCoefficients:\n (Intercept)         girth        height  girth:height  \n     69.3963       -5.8558       -1.2971        0.1347  \n\n\n\n\nWe first define the full model, then the model without the interaction (model_1).\nWe extract the AICs for both models. Because we do not have an automated way of performing the BSE, we’re stringing together a few operations to make the code a bit more concise (getting the .fit() of the model and immediately extracting the aic value):\n\n# define the models\nmodel_full = smf.ols(formula= \"volume ~ girth * height\",\n                     data = trees_py)\n\nmodel_1 = smf.ols(formula= \"volume ~ girth + height\",\n                  data = trees_py)\n\n# get the AIC of the model\nmodel_full.fit().aic\n\n153.46916240903528\n\nmodel_1.fit().aic\n\n174.9099729872703\n\n\n\n\n\nThis BSE approach only gets as far as the first step (trying to drop the interaction term). We see immediately that dropping the interaction term makes the model worse. This means that the best model is still the full model.\n\n\n\n\n\n\n\n\n\n\n\n\n10.7.1 BSE: Air pollution\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nPerform a BSE on pm2_5. Let’s start by reading in the data and checking which variables are in the data set.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n10.8 Answer\n\nRPython\n\n\n\npm2_5 &lt;- read_csv(\"data/CS5-pm2_5.csv\")\n\nhead(pm2_5)\n\n# A tibble: 6 × 6\n  avg_temp date       location pm2_5 rain_mm wind_m_s\n     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1      4.5 01/01/2019 inner     17.1     2.3     3.87\n2      4.9 01/01/2019 outer     10.8     2.3     5.84\n3      4.3 02/01/2019 inner     14.9     2.3     3.76\n4      4.8 02/01/2019 outer     11.4     2.3     6   \n5      4   03/01/2019 inner     18.5     1.4     2.13\n6      4.5 03/01/2019 outer     15.0     1.4     2.57\n\n\n\n\n\npm2_5_py = pd.read_csv(\"data/CS5-pm2_5.csv\")\n\npm2_5_py.head()\n\n   avg_temp        date location   pm2_5  rain_mm  wind_m_s\n0       4.5  01/01/2019    inner  17.126      2.3      3.87\n1       4.9  01/01/2019    outer  10.821      2.3      5.84\n2       4.3  02/01/2019    inner  14.884      2.3      3.76\n3       4.8  02/01/2019    outer  11.416      2.3      6.00\n4       4.0  03/01/2019    inner  18.471      1.4      2.13\n\n\n\n\n\n\nThe response variable is pm2_5\nThe predictor variables are all the variables, apart from date and pm2_5. It would be strange to try and create a model that relies on each individual measurement!\nWe can add the wind_m_s:location interaction, since it appeared that there is a difference between inner and outer London pollution levels, in relation to wind speed\nWe can start the backwards stepwise elimination with the following model:\n\npm2_5 ~ avg_temp + rain_mm + wind_m_s * location\n\nRPython\n\n\n\n# define the model\nlm_pm2_5 &lt;- lm(pm2_5 ~ avg_temp + rain_mm + wind_m_s * location,\n               data = pm2_5)\n\n# perform BSE\nstep(lm_pm2_5)\n\nStart:  AIC=41.08\npm2_5 ~ avg_temp + rain_mm + wind_m_s * location\n\n                    Df Sum of Sq    RSS     AIC\n- rain_mm            1     0.351 760.01  39.412\n- avg_temp           1     1.804 761.47  40.806\n&lt;none&gt;                           759.66  41.075\n- wind_m_s:location  1   134.820 894.48 158.336\n\nStep:  AIC=39.41\npm2_5 ~ avg_temp + wind_m_s + location + wind_m_s:location\n\n                    Df Sum of Sq    RSS     AIC\n- avg_temp           1     1.758 761.77  39.098\n&lt;none&gt;                           760.01  39.412\n- wind_m_s:location  1   134.530 894.54 156.385\n\nStep:  AIC=39.1\npm2_5 ~ wind_m_s + location + wind_m_s:location\n\n                    Df Sum of Sq    RSS     AIC\n&lt;none&gt;                           761.77  39.098\n- wind_m_s:location  1    136.95 898.72 157.788\n\n\n\nCall:\nlm(formula = pm2_5 ~ wind_m_s + location + wind_m_s:location, \n    data = pm2_5)\n\nCoefficients:\n           (Intercept)                wind_m_s           locationouter  \n               18.2422                 -0.2851                 -2.0597  \nwind_m_s:locationouter  \n               -0.4318  \n\n\n\n\nWe first define the full model, again stringing together a few operations to be more concise.\n\n# define the model\nmodel_full = smf.ols(formula= \"pm2_5 ~ avg_temp + rain_mm + wind_m_s * C(location)\", data = pm2_5_py)\n\n# get the AIC of the model\nmodel_full.fit().aic\n\n2112.725435984824\n\n\nCan we drop the interaction term or any of the remaining main effects?\n\n# define the model\nmodel_1 = smf.ols(\n    formula= \"pm2_5 ~ avg_temp + rain_mm + wind_m_s + C(location)\",\n    data = pm2_5_py)\n    \nmodel_2 = smf.ols(\n    formula= \"pm2_5 ~ avg_temp + wind_m_s * C(location)\",\n    data = pm2_5_py)\n    \nmodel_3 = smf.ols(\n    formula= \"pm2_5 ~ rain_mm + wind_m_s * C(location)\",\n    data = pm2_5_py)\n\n# get the AIC of the models\nmodel_1.fit().aic\n\n2229.9865962235162\n\nmodel_2.fit().aic\n\n2111.06230638372\n\nmodel_3.fit().aic\n\n2112.456639492829\n\n# compare to the full model\nmodel_full.fit().aic\n\n2112.725435984824\n\n\nThe AIC goes up quite a bit if we drop the interaction term. This means that we cannot drop the interaction term, nor any of the main effects that are included in the interaction. These are wind_m_s and location.\nThe model with the lowest AIC is the one without the rain_mm term, so our working model is:\n\nworking_model = smf.ols(\n    formula= \"pm2_5 ~ avg_temp + wind_m_s * C(location)\",\n    data = pm2_5_py)\n\nNow we can again check if dropping the avg_temp term or the wind_m_s:location interaction has any effect on the model performance:\n\nmodel_1 = smf.ols(\n    formula= \"pm2_5 ~ wind_m_s * C(location)\",\n    data = pm2_5_py)\n    \nmodel_2 = smf.ols(\n    formula= \"pm2_5 ~ avg_temp + wind_m_s + C(location)\",\n    data = pm2_5_py)\n\n# get the AIC of the models\nmodel_1.fit().aic\n\n2110.748543452394\n\nmodel_2.fit().aic\n\n2228.035595215867\n\nworking_model.fit().aic\n\n2111.06230638372\n\n\nThis shows that dropping the avg_temp term lowers the AIC, whereas dropping the interaction term makes the model markedly worse.\nSo, our new working model is:\n\nworking_model = smf.ols(\n    formula= \"pm2_5 ~ wind_m_s * C(location)\",\n    data = pm2_5_py)\n\nLastly, now we’ve dropped the avg_temp term we can do one final check on the interaction term:\n\nmodel_1 = smf.ols(\n    formula= \"pm2_5 ~ wind_m_s + C(location) + wind_m_s + C(location)\",\n    data = pm2_5_py)\n\nmodel_1.fit().aic\n\n2229.438631394105\n\nworking_model.fit().aic\n\n2110.748543452394\n\n\nDropping the interaction still makes the model worse.\n\n\n\nOur minimal model is thus:\npm2_5 ~ wind_m_s + location + wind_m_s:location"
  },
  {
    "objectID": "materials/cs5_practical_model-comparisons.html#answer",
    "href": "materials/cs5_practical_model-comparisons.html#answer",
    "title": "10  Model comparisons",
    "section": "10.7 Answer",
    "text": "10.7 Answer\nLet’s start by reading in the data and checking which variables are in the data set.\n\nRPython\n\n\n\ntrees &lt;- read_csv(\"data/CS5-trees.csv\")\n\nhead(trees)\n\n# A tibble: 6 × 3\n  girth height volume\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1   8.3     70   10.3\n2   8.6     65   10.3\n3   8.8     63   10.2\n4  10.5     72   16.4\n5  10.7     81   18.8\n6  10.8     83   19.7\n\n\n\n\nFirst, we read in the data (if needed) and have a look at the variables:\n\ntrees_py = pd.read_csv(\"data/CS5-trees.csv\")\n\ntrees_py.head()\n\n   girth  height  volume\n0    8.3      70    10.3\n1    8.6      65    10.3\n2    8.8      63    10.2\n3   10.5      72    16.4\n4   10.7      81    18.8\n\n\n\n\n\n\nThe response variable is volume\nThe predictor variables are girth and height\nThe only possible interaction term is girth:height\nWe perform a BSE on the model using the step() function\n\nThe full model is volume ~ girth * height.\nWe perform the BSE as follows:\n\nRPython\n\n\nDefine the model and use the step() function:\n\n# define the full model\nlm_trees &lt;- lm(volume ~ girth * height,\n               data = trees)\n\n# perform BSE\nstep(lm_trees)\n\nStart:  AIC=65.49\nvolume ~ girth * height\n\n               Df Sum of Sq    RSS    AIC\n&lt;none&gt;                      198.08 65.495\n- girth:height  1    223.84 421.92 86.936\n\n\n\nCall:\nlm(formula = volume ~ girth * height, data = trees)\n\nCoefficients:\n (Intercept)         girth        height  girth:height  \n     69.3963       -5.8558       -1.2971        0.1347  \n\n\n\n\nWe first define the full model, then the model without the interaction (model_1).\nWe extract the AICs for both models. Because we do not have an automated way of performing the BSE, we’re stringing together a few operations to make the code a bit more concise (getting the .fit() of the model and immediately extracting the aic value):\n\n# define the models\nmodel_full = smf.ols(formula= \"volume ~ girth * height\",\n                     data = trees_py)\n\nmodel_1 = smf.ols(formula= \"volume ~ girth + height\",\n                  data = trees_py)\n\n# get the AIC of the model\nmodel_full.fit().aic\n\n153.46916240903528\n\nmodel_1.fit().aic\n\n174.9099729872703\n\n\n\n\n\nThis BSE approach only gets as far as the first step (trying to drop the interaction term). We see immediately that dropping the interaction term makes the model worse. This means that the best model is still the full model."
  },
  {
    "objectID": "materials/cs5_practical_model-comparisons.html#answer-1",
    "href": "materials/cs5_practical_model-comparisons.html#answer-1",
    "title": "10  Model comparisons",
    "section": "10.8 Answer",
    "text": "10.8 Answer\n\nRPython\n\n\n\npm2_5 &lt;- read_csv(\"data/CS5-pm2_5.csv\")\n\nhead(pm2_5)\n\n# A tibble: 6 × 6\n  avg_temp date       location pm2_5 rain_mm wind_m_s\n     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1      4.5 01/01/2019 inner     17.1     2.3     3.87\n2      4.9 01/01/2019 outer     10.8     2.3     5.84\n3      4.3 02/01/2019 inner     14.9     2.3     3.76\n4      4.8 02/01/2019 outer     11.4     2.3     6   \n5      4   03/01/2019 inner     18.5     1.4     2.13\n6      4.5 03/01/2019 outer     15.0     1.4     2.57\n\n\n\n\n\npm2_5_py = pd.read_csv(\"data/CS5-pm2_5.csv\")\n\npm2_5_py.head()\n\n   avg_temp        date location   pm2_5  rain_mm  wind_m_s\n0       4.5  01/01/2019    inner  17.126      2.3      3.87\n1       4.9  01/01/2019    outer  10.821      2.3      5.84\n2       4.3  02/01/2019    inner  14.884      2.3      3.76\n3       4.8  02/01/2019    outer  11.416      2.3      6.00\n4       4.0  03/01/2019    inner  18.471      1.4      2.13\n\n\n\n\n\n\nThe response variable is pm2_5\nThe predictor variables are all the variables, apart from date and pm2_5. It would be strange to try and create a model that relies on each individual measurement!\nWe can add the wind_m_s:location interaction, since it appeared that there is a difference between inner and outer London pollution levels, in relation to wind speed\nWe can start the backwards stepwise elimination with the following model:\n\npm2_5 ~ avg_temp + rain_mm + wind_m_s * location\n\nRPython\n\n\n\n# define the model\nlm_pm2_5 &lt;- lm(pm2_5 ~ avg_temp + rain_mm + wind_m_s * location,\n               data = pm2_5)\n\n# perform BSE\nstep(lm_pm2_5)\n\nStart:  AIC=41.08\npm2_5 ~ avg_temp + rain_mm + wind_m_s * location\n\n                    Df Sum of Sq    RSS     AIC\n- rain_mm            1     0.351 760.01  39.412\n- avg_temp           1     1.804 761.47  40.806\n&lt;none&gt;                           759.66  41.075\n- wind_m_s:location  1   134.820 894.48 158.336\n\nStep:  AIC=39.41\npm2_5 ~ avg_temp + wind_m_s + location + wind_m_s:location\n\n                    Df Sum of Sq    RSS     AIC\n- avg_temp           1     1.758 761.77  39.098\n&lt;none&gt;                           760.01  39.412\n- wind_m_s:location  1   134.530 894.54 156.385\n\nStep:  AIC=39.1\npm2_5 ~ wind_m_s + location + wind_m_s:location\n\n                    Df Sum of Sq    RSS     AIC\n&lt;none&gt;                           761.77  39.098\n- wind_m_s:location  1    136.95 898.72 157.788\n\n\n\nCall:\nlm(formula = pm2_5 ~ wind_m_s + location + wind_m_s:location, \n    data = pm2_5)\n\nCoefficients:\n           (Intercept)                wind_m_s           locationouter  \n               18.2422                 -0.2851                 -2.0597  \nwind_m_s:locationouter  \n               -0.4318  \n\n\n\n\nWe first define the full model, again stringing together a few operations to be more concise.\n\n# define the model\nmodel_full = smf.ols(formula= \"pm2_5 ~ avg_temp + rain_mm + wind_m_s * C(location)\", data = pm2_5_py)\n\n# get the AIC of the model\nmodel_full.fit().aic\n\n2112.725435984824\n\n\nCan we drop the interaction term or any of the remaining main effects?\n\n# define the model\nmodel_1 = smf.ols(\n    formula= \"pm2_5 ~ avg_temp + rain_mm + wind_m_s + C(location)\",\n    data = pm2_5_py)\n    \nmodel_2 = smf.ols(\n    formula= \"pm2_5 ~ avg_temp + wind_m_s * C(location)\",\n    data = pm2_5_py)\n    \nmodel_3 = smf.ols(\n    formula= \"pm2_5 ~ rain_mm + wind_m_s * C(location)\",\n    data = pm2_5_py)\n\n# get the AIC of the models\nmodel_1.fit().aic\n\n2229.9865962235162\n\nmodel_2.fit().aic\n\n2111.06230638372\n\nmodel_3.fit().aic\n\n2112.456639492829\n\n# compare to the full model\nmodel_full.fit().aic\n\n2112.725435984824\n\n\nThe AIC goes up quite a bit if we drop the interaction term. This means that we cannot drop the interaction term, nor any of the main effects that are included in the interaction. These are wind_m_s and location.\nThe model with the lowest AIC is the one without the rain_mm term, so our working model is:\n\nworking_model = smf.ols(\n    formula= \"pm2_5 ~ avg_temp + wind_m_s * C(location)\",\n    data = pm2_5_py)\n\nNow we can again check if dropping the avg_temp term or the wind_m_s:location interaction has any effect on the model performance:\n\nmodel_1 = smf.ols(\n    formula= \"pm2_5 ~ wind_m_s * C(location)\",\n    data = pm2_5_py)\n    \nmodel_2 = smf.ols(\n    formula= \"pm2_5 ~ avg_temp + wind_m_s + C(location)\",\n    data = pm2_5_py)\n\n# get the AIC of the models\nmodel_1.fit().aic\n\n2110.748543452394\n\nmodel_2.fit().aic\n\n2228.035595215867\n\nworking_model.fit().aic\n\n2111.06230638372\n\n\nThis shows that dropping the avg_temp term lowers the AIC, whereas dropping the interaction term makes the model markedly worse.\nSo, our new working model is:\n\nworking_model = smf.ols(\n    formula= \"pm2_5 ~ wind_m_s * C(location)\",\n    data = pm2_5_py)\n\nLastly, now we’ve dropped the avg_temp term we can do one final check on the interaction term:\n\nmodel_1 = smf.ols(\n    formula= \"pm2_5 ~ wind_m_s + C(location) + wind_m_s + C(location)\",\n    data = pm2_5_py)\n\nmodel_1.fit().aic\n\n2229.438631394105\n\nworking_model.fit().aic\n\n2110.748543452394\n\n\nDropping the interaction still makes the model worse.\n\n\n\nOur minimal model is thus:\npm2_5 ~ wind_m_s + location + wind_m_s:location"
  },
  {
    "objectID": "materials/cs5_practical_model-comparisons.html#summary",
    "href": "materials/cs5_practical_model-comparisons.html#summary",
    "title": "10  Model comparisons",
    "section": "10.9 Summary",
    "text": "10.9 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nWe can use Backwards Stepwise Elimination (BSE) on a full model to see if certain terms add to the predictive power of the model or not\nThe AIC allows us to compare different models - if there is a difference in AIC of more than 2 between two models, then the smallest AIC score is more supported"
  },
  {
    "objectID": "materials/stats-week_glm-practical-logistic-binary.html#libraries-and-functions",
    "href": "materials/stats-week_glm-practical-logistic-binary.html#libraries-and-functions",
    "title": "11  Binary response",
    "section": "11.1 Libraries and functions",
    "text": "11.1 Libraries and functions\n\n\n\n\n\n\nClick to expand\n\n\n\n\n\n\nRPython\n\n\n\n11.1.1 Libraries\n\n\n11.1.2 Functions\n\n\n\n\n11.1.3 Libraries\n\n# A maths library\nimport math\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Statistical models, conducting tests and statistical data exploration\nimport statsmodels.api as sm\n\n# Convenience interface for specifying models using formula strings and DataFrames\nimport statsmodels.formula.api as smf\n\n\n\n11.1.4 Functions\n\n\n\n\n\n\n\nThe example in this section uses the following data set:\ndata/finches_early.csv\nThese data come from an analysis of gene flow across two finch species [@lamichhaney2020]. They are slightly adapted here for illustrative purposes.\nThe data focus on two species, Geospiza fortis and G. scandens. The original measurements are split by a uniquely timed event: a particularly strong El Niño event in 1983. This event changed the vegetation and food supply of the finches, allowing F1 hybrids of the two species to survive, whereas before 1983 they could not. The measurements are classed as early (pre-1983) and late (1983 onwards).\nHere we are looking only at the early data. We are specifically focussing on the beak shape classification, which we saw earlier in Figure 5.5."
  },
  {
    "objectID": "materials/stats-week_glm-practical-logistic-binary.html#load-and-visualise-the-data",
    "href": "materials/stats-week_glm-practical-logistic-binary.html#load-and-visualise-the-data",
    "title": "11  Binary response",
    "section": "11.2 Load and visualise the data",
    "text": "11.2 Load and visualise the data\nFirst we load the data, then we visualise it.\n\nRPython\n\n\n\nearly_finches &lt;- read_csv(\"data/finches_early.csv\")\n\n\n\n\nearly_finches_py = pd.read_csv(\"data/finches_early.csv\")\n\n\n\n\nLooking at the data, we can see that the pointed_beak column contains zeros and ones. These are actually yes/no classification outcomes and not numeric representations.\nWe’ll have to deal with this soon. For now, we can plot the data:\n\nRPython\n\n\n\nggplot(early_finches,\n       aes(x = factor(pointed_beak),\n          y = blength)) +\n  geom_boxplot()\n\n\n\n\n\n\nWe could just give Python the pointed_beak data directly, but then it would view the values as numeric. Which doesn’t really work, because we have two groups as such: those with a pointed beak (1), and those with a blunt one (0).\nWe can force Python to temporarily covert the data to a factor, by making the pointed_beak column an object type. We can do this directly inside the ggplot() function.\n\n(ggplot(early_finches_py,\n         aes(x = early_finches_py.pointed_beak.astype(object),\n             y = \"blength\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\nIt looks as though the finches with blunt beaks generally have shorter beak lengths.\nWe can visualise that differently by plotting all the data points as a classic binary response plot:\n\nRPython\n\n\n\nggplot(early_finches,\n       aes(x = blength, y = pointed_beak)) +\n  geom_point()\n\n\n\n\n\n\n\n(ggplot(early_finches_py,\n         aes(x = \"blength\",\n             y = \"pointed_beak\")) +\n     geom_point())\n\n\n\n\n\n\n\nThis presents us with a bit of an issue. We could fit a linear regression model to these data, although we already know that this is a bad idea…\n\nRPython\n\n\n\nggplot(early_finches,\n       aes(x = blength, y = pointed_beak)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n(ggplot(early_finches_py,\n         aes(x = \"blength\",\n             y = \"pointed_beak\")) +\n     geom_point() +\n     geom_smooth(method = \"lm\",\n                 colour = \"blue\",\n                 se = False))\n\n\n\n\n\n\n\nOf course this is rubbish - we can’t have a beak classification outside the range of \\([0, 1]\\). It’s either blunt (0) or pointed (1).\nBut for the sake of exploration, let’s look at the assumptions:\n\nRPython\n\n\n\nlm_bks &lt;- lm(pointed_beak ~ blength,\n             data = early_finches)\n\nresid_panel(lm_bks,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\nFirst, we create a linear model:\n\n# create a linear model\nmodel = smf.ols(formula = \"pointed_beak ~ blength\",\n                data = early_finches_py)\n# and get the fitted parameters of the model\nlm_bks_py = model.fit()\n\nNext, we can create the diagnostic plots:\n\ndgplots(lm_bks_py)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThey’re pretty extremely bad.\n\nThe response is not linear (Residual Plot, binary response plot, common sense).\nThe residuals do not appear to be distributed normally (Q-Q Plot)\nThe variance is not homogeneous across the predicted values (Location-Scale Plot)\nBut - there is always a silver lining - we don’t have influential data points."
  },
  {
    "objectID": "materials/stats-week_glm-practical-logistic-binary.html#creating-a-suitable-model",
    "href": "materials/stats-week_glm-practical-logistic-binary.html#creating-a-suitable-model",
    "title": "11  Binary response",
    "section": "11.3 Creating a suitable model",
    "text": "11.3 Creating a suitable model\nSo far we’ve established that using a simple linear model to describe a potential relationship between beak length and the probability of having a pointed beak is not a good idea. So, what can we do?\nOne of the ways we can deal with binary outcome data is by performing a logistic regression. Instead of fitting a straight line to our data, and performing a regression on that, we fit a line that has an S shape. This avoids the model making predictions outside the \\([0, 1]\\) range.\nWe described our standard linear relationship as follows:\n\\(Y = \\beta_0 + \\beta_1X\\)\nWe can now map this to our non-linear relationship via the logistic link function:\n\\(Y = \\frac{\\exp(\\beta_0 + \\beta_1X)}{1 + \\exp(\\beta_0 + \\beta_1X)}\\)\nNote that the \\(\\beta_0 + \\beta_1X\\) part is identical to the formula of a straight line.\nThe rest of the function is what makes the straight line curve into its characteristic S shape.\n\n\n\n\n\n\nEuler’s number (\\(\\exp\\)): would you like to know more?\n\n\n\n\n\nIn mathematics, \\(\\rm e\\) represents a constant of around 2.718. Another notation is \\(\\exp\\), which is often used when notations become a bit cumbersome. Here, I exclusively use the \\(\\exp\\) notation for consistency.\n\n\n\nWe can fit such an S-shaped curve to our early_finches data set, by creating a generalised linear model.\n\nRPython\n\n\nIn R we have a few options to do this, and by far the most familiar function would be glm(). Here we save the model in an object called glm_bks:\n\nglm_bks &lt;- glm(pointed_beak ~ blength,\n               family = binomial,\n               data = early_finches)\n\nThe format of this function is similar to that used by the lm() function for linear models. The important difference is that we must specify the family of error distribution to use. For logistic regression we must set the family to binomial.\nIf you forget to set the family argument, then the glm() function will perform a standard linear model fit, identical to what the lm() function would do.\n\n\n\n# create a linear model\nmodel = smf.glm(formula = \"pointed_beak ~ blength\",\n                family = sm.families.Binomial(),\n                data = early_finches_py)\n# and get the fitted parameters of the model\nglm_bks_py = model.fit()"
  },
  {
    "objectID": "materials/stats-week_glm-practical-logistic-binary.html#model-output",
    "href": "materials/stats-week_glm-practical-logistic-binary.html#model-output",
    "title": "11  Binary response",
    "section": "11.4 Model output",
    "text": "11.4 Model output\nThat’s the easy part done! The trickier part is interpreting the output. First of all, we’ll get some summary information.\n\nRPython\n\n\n\nsummary(glm_bks)\n\n\nCall:\nglm(formula = pointed_beak ~ blength, family = binomial, data = early_finches)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  -43.410     15.250  -2.847  0.00442 **\nblength        3.387      1.193   2.839  0.00452 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 84.5476  on 60  degrees of freedom\nResidual deviance:  9.1879  on 59  degrees of freedom\nAIC: 13.188\n\nNumber of Fisher Scoring iterations: 8\n\n\n\n\n\nprint(glm_bks_py.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:           pointed_beak   No. Observations:                   61\nModel:                            GLM   Df Residuals:                       59\nModel Family:                Binomial   Df Model:                            1\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -4.5939\nDate:                Thu, 25 Jan 2024   Deviance:                       9.1879\nTime:                        08:38:26   Pearson chi2:                     15.1\nNo. Iterations:                     8   Pseudo R-squ. (CS):             0.7093\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    -43.4096     15.250     -2.847      0.004     -73.298     -13.521\nblength        3.3866      1.193      2.839      0.005       1.049       5.724\n==============================================================================\n\n\n\n\n\nThere’s a lot to unpack here, but for the purpose of today we are mostly focussing on the coefficients.\n\nRPython\n\n\nThe coefficients can be found in the Coefficients block. The main numbers to extract from the output are the two numbers underneath Estimate.Std:\nCoefficients:\n            Estimate Std.\n(Intercept)  -43.410\nblength        3.387 \n\n\nRight at the bottom is a table showing the model coefficients. The main numbers to extract from the output are the two numbers in the coef column:\n======================\n                 coef\n----------------------\nIntercept    -43.4096\nblength        3.3866\n======================\n\n\n\nThese are the coefficients of the logistic model equation and need to be placed in the correct equation if we want to be able to calculate the probability of having a pointed beak for a given beak length.\nThe \\(p\\) values at the end of each coefficient row merely show whether that particular coefficient is significantly different from zero. This is similar to the \\(p\\) values obtained in the summary output of a linear model. As with continuous predictors in simple models, these \\(p\\) values can be used to decide whether that predictor is important (so in this case beak length appears to be significant). However, these \\(p\\) values aren’t great to work with when we have multiple predictor variables, or when we have categorical predictors with multiple levels (since the output will give us a \\(p\\) value for each level rather than for the predictor as a whole).\nWe can use the coefficients to calculate the probability of having a pointed beak for a given beak length:\n\\[ P(pointed \\ beak) = \\frac{\\exp(-43.41 + 3.39 \\times blength)}{1 + \\exp(-43.41 + 3.39 \\times blength)} \\]\nHaving this formula means that we can calculate the probability of having a pointed beak for any beak length. How do we work this out in practice?\n\nRPython\n\n\nWell, the probability of having a pointed beak if the beak length is large (for example 15 mm) can be calculated as follows:\n\nexp(-43.41 + 3.39 * 15) / (1 + exp(-43.41 + 3.39 * 15))\n\n[1] 0.9994131\n\n\nIf the beak length is small (for example 10 mm), the probability of having a pointed beak is extremely low:\n\nexp(-43.41 + 3.39 * 10) / (1 + exp(-43.41 + 3.39 * 10))\n\n[1] 7.410155e-05\n\n\n\n\nWell, the probability of having a pointed beak if the beak length is large (for example 15 mm) can be calculated as follows:\n\n# import the math library\nimport math\n\n\nmath.exp(-43.41 + 3.39 * 15) / (1 + math.exp(-43.41 + 3.39 * 15))\n\n0.9994130595039192\n\n\nIf the beak length is small (for example 10 mm), the probability of having a pointed beak is extremely low:\n\nmath.exp(-43.41 + 3.39 * 10) / (1 + math.exp(-43.41 + 3.39 * 10))\n\n7.410155028945912e-05\n\n\n\n\n\nWe can calculate the the probabilities for all our observed values and if we do that then we can see that the larger the beak length is, the higher the probability that a beak shape would be pointed. I’m visualising this together with the logistic curve, where the blue points are the calculated probabilities:\n\n\n\n\n\n\nCode available here\n\n\n\n\n\n\nRPython\n\n\n\nglm_bks %&gt;% \n  augment(type.predict = \"response\") %&gt;% \n  ggplot() +\n  geom_point(aes(x = blength, y = pointed_beak)) +\n  geom_line(aes(x = blength, y = .fitted),\n            linetype = \"dashed\",\n            colour = \"blue\") +\n  geom_point(aes(x = blength, y = .fitted),\n             colour = \"blue\", alpha = 0.5) +\n  labs(x = \"beak length (mm)\",\n       y = \"Probability\")\n\n\n\n\n(ggplot(early_finches_py) +\n  geom_point(aes(x = \"blength\", y = \"pointed_beak\")) +\n  geom_line(aes(x = \"blength\", y = glm_bks_py.fittedvalues),\n            linetype = \"dashed\",\n            colour = \"blue\") +\n  geom_point(aes(x = \"blength\", y = glm_bks_py.fittedvalues),\n             colour = \"blue\", alpha = 0.5) +\n  labs(x = \"beak length (mm)\",\n       y = \"Probability\"))\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.1: Predicted probabilities for beak classification\n\n\n\n\nThe graph shows us that, based on the data that we have and the model we used to make predictions about our response variable, the probability of seeing a pointed beak increases with beak length.\nShort beaks are more closely associated with the bluntly shaped beaks, whereas long beaks are more closely associated with the pointed shape. It’s also clear that there is a range of beak lengths (around 13 mm) where the probability of getting one shape or another is much more even.\nThere is an interesting genetic story behind all this, but that will be explained in more detail in the full-day Generalised linear models course."
  },
  {
    "objectID": "materials/stats-week_glm-practical-logistic-binary.html#exercises",
    "href": "materials/stats-week_glm-practical-logistic-binary.html#exercises",
    "title": "11  Binary response",
    "section": "11.5 Exercises",
    "text": "11.5 Exercises\n\n11.5.1 Diabetes\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nFor this exercise we’ll be using the data from data/diabetes.csv.\nThis is a data set comprising 768 observations of three variables (one dependent and two predictor variables). This records the results of a diabetes test result as a binary variable (1 is a positive result, 0 is a negative result), along with the result of a glucose tolerance test and the diastolic blood pressure for each of 768 women. The variables are called test_result, glucose and diastolic.\nWe want to see if the glucose tolerance is a meaningful predictor for predictions on a diabetes test. To investigate this, do the following:\n\nLoad and visualise the data\nCreate a suitable model\nDetermine if there are any statistically significant predictors\nCalculate the probability of a positive diabetes test result for a glucose tolerance test value of glucose = 150\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n11.6 Answer\n\nLoad and visualise the data\nFirst we load the data, then we visualise it.\n\nRPython\n\n\n\ndiabetes &lt;- read_csv(\"data/diabetes.csv\")\n\n\n\n\ndiabetes_py = pd.read_csv(\"data/diabetes.csv\")\n\n\n\n\nLooking at the data, we can see that the test_result column contains zeros and ones. These are yes/no test result outcomes and not actually numeric representations.\nWe’ll have to deal with this soon. For now, we can plot the data, by outcome:\n\nRPython\n\n\n\nggplot(diabetes,\n       aes(x = factor(test_result),\n           y = glucose)) +\n  geom_boxplot()\n\n\n\n\n\n\nWe could just give Python the test_result data directly, but then it would view the values as numeric. Which doesn’t really work, because we have two groups as such: those with a negative diabetes test result, and those with a positive one.\nWe can force Python to temporarily covert the data to a factor, by making the test_result column an object type. We can do this directly inside the ggplot() function.\n\n(ggplot(diabetes_py,\n         aes(x = diabetes_py.test_result.astype(object),\n             y = \"glucose\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\nIt looks as though the patients with a positive diabetes test have slightly higher glucose levels than those with a negative diabetes test.\nWe can visualise that differently by plotting all the data points as a classic binary response plot:\n\nRPython\n\n\n\nggplot(diabetes,\n       aes(x = glucose,\n           y = test_result)) +\n  geom_point()\n\n\n\n\n\n\n\n(ggplot(diabetes_py,\n         aes(x = \"glucose\",\n             y = \"test_result\")) +\n     geom_point())\n\n\n\n\n\n\n\n\n\nCreate a suitable model\n\nRPython\n\n\nWe’ll use the glm() function to create a generalised linear model. Here we save the model in an object called glm_dia:\n\nglm_dia &lt;- glm(test_result ~ glucose,\n               family = binomial,\n               data = diabetes)\n\nThe format of this function is similar to that used by the lm() function for linear models. The important difference is that we must specify the family of error distribution to use. For logistic regression we must set the family to binomial.\n\n\n\n# create a linear model\nmodel = smf.glm(formula = \"test_result ~ glucose\",\n                family = sm.families.Binomial(),\n                data = diabetes_py)\n# and get the fitted parameters of the model\nglm_dia_py = model.fit()\n\n\n\n\nLet’s look at the model parameters:\n\nRPython\n\n\n\nsummary(glm_dia)\n\n\nCall:\nglm(formula = test_result ~ glucose, family = binomial, data = diabetes)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.611732   0.442289  -12.69   &lt;2e-16 ***\nglucose      0.039510   0.003398   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 936.6  on 727  degrees of freedom\nResidual deviance: 752.2  on 726  degrees of freedom\nAIC: 756.2\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\nprint(glm_dia_py.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:            test_result   No. Observations:                  728\nModel:                            GLM   Df Residuals:                      726\nModel Family:                Binomial   Df Model:                            1\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -376.10\nDate:                Thu, 25 Jan 2024   Deviance:                       752.20\nTime:                        08:38:30   Pearson chi2:                     713.\nNo. Iterations:                     4   Pseudo R-squ. (CS):             0.2238\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -5.6117      0.442    -12.688      0.000      -6.479      -4.745\nglucose        0.0395      0.003     11.628      0.000       0.033       0.046\n==============================================================================\n\n\n\n\n\nWe can see that glucose is a significant predictor for the test_result (the \\(p\\) value is much smaller than 0.05).\nKnowing this, we’re interested in the coefficients. We have an intercept of -5.61 and 0.0395 for glucose. We can use these coefficients to write a formula that describes the potential relationship between the probability of having a positive test result, dependent on the glucose tolerance level value:\n\\[ P(positive \\ test\\ result) = \\frac{\\exp(-5.61 + 0.04 \\times glucose)}{1 + \\exp(-5.61 + 0.04 \\times glucose)} \\]\n\n\nCalculating probabilities\nUsing the formula above, we can now calculate the probability of having a positive test result, for a given glucose value. If we do this for glucose = 150, we get the following:\n\nRPython\n\n\n\nexp(-5.61 + 0.04 * 150) / (1 + exp(-5.61 + 0.04 * 145))\n\n[1] 0.6685441\n\n\n\n\n\nmath.exp(-5.61 + 0.04 * 150) / (1 + math.exp(-5.61 + 0.04 * 145))\n\n0.6685441044999503\n\n\n\n\n\nThis tells us that the probability of having a positive diabetes test result, given a glucose tolerance level of 150 is around 67%."
  },
  {
    "objectID": "materials/stats-week_glm-practical-logistic-binary.html#answer",
    "href": "materials/stats-week_glm-practical-logistic-binary.html#answer",
    "title": "11  Binary response",
    "section": "11.6 Answer",
    "text": "11.6 Answer\n\nLoad and visualise the data\nFirst we load the data, then we visualise it.\n\nRPython\n\n\n\ndiabetes &lt;- read_csv(\"data/diabetes.csv\")\n\n\n\n\ndiabetes_py = pd.read_csv(\"data/diabetes.csv\")\n\n\n\n\nLooking at the data, we can see that the test_result column contains zeros and ones. These are yes/no test result outcomes and not actually numeric representations.\nWe’ll have to deal with this soon. For now, we can plot the data, by outcome:\n\nRPython\n\n\n\nggplot(diabetes,\n       aes(x = factor(test_result),\n           y = glucose)) +\n  geom_boxplot()\n\n\n\n\n\n\nWe could just give Python the test_result data directly, but then it would view the values as numeric. Which doesn’t really work, because we have two groups as such: those with a negative diabetes test result, and those with a positive one.\nWe can force Python to temporarily covert the data to a factor, by making the test_result column an object type. We can do this directly inside the ggplot() function.\n\n(ggplot(diabetes_py,\n         aes(x = diabetes_py.test_result.astype(object),\n             y = \"glucose\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\nIt looks as though the patients with a positive diabetes test have slightly higher glucose levels than those with a negative diabetes test.\nWe can visualise that differently by plotting all the data points as a classic binary response plot:\n\nRPython\n\n\n\nggplot(diabetes,\n       aes(x = glucose,\n           y = test_result)) +\n  geom_point()\n\n\n\n\n\n\n\n(ggplot(diabetes_py,\n         aes(x = \"glucose\",\n             y = \"test_result\")) +\n     geom_point())\n\n\n\n\n\n\n\n\n\nCreate a suitable model\n\nRPython\n\n\nWe’ll use the glm() function to create a generalised linear model. Here we save the model in an object called glm_dia:\n\nglm_dia &lt;- glm(test_result ~ glucose,\n               family = binomial,\n               data = diabetes)\n\nThe format of this function is similar to that used by the lm() function for linear models. The important difference is that we must specify the family of error distribution to use. For logistic regression we must set the family to binomial.\n\n\n\n# create a linear model\nmodel = smf.glm(formula = \"test_result ~ glucose\",\n                family = sm.families.Binomial(),\n                data = diabetes_py)\n# and get the fitted parameters of the model\nglm_dia_py = model.fit()\n\n\n\n\nLet’s look at the model parameters:\n\nRPython\n\n\n\nsummary(glm_dia)\n\n\nCall:\nglm(formula = test_result ~ glucose, family = binomial, data = diabetes)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.611732   0.442289  -12.69   &lt;2e-16 ***\nglucose      0.039510   0.003398   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 936.6  on 727  degrees of freedom\nResidual deviance: 752.2  on 726  degrees of freedom\nAIC: 756.2\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\nprint(glm_dia_py.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:            test_result   No. Observations:                  728\nModel:                            GLM   Df Residuals:                      726\nModel Family:                Binomial   Df Model:                            1\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -376.10\nDate:                Thu, 25 Jan 2024   Deviance:                       752.20\nTime:                        08:38:30   Pearson chi2:                     713.\nNo. Iterations:                     4   Pseudo R-squ. (CS):             0.2238\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -5.6117      0.442    -12.688      0.000      -6.479      -4.745\nglucose        0.0395      0.003     11.628      0.000       0.033       0.046\n==============================================================================\n\n\n\n\n\nWe can see that glucose is a significant predictor for the test_result (the \\(p\\) value is much smaller than 0.05).\nKnowing this, we’re interested in the coefficients. We have an intercept of -5.61 and 0.0395 for glucose. We can use these coefficients to write a formula that describes the potential relationship between the probability of having a positive test result, dependent on the glucose tolerance level value:\n\\[ P(positive \\ test\\ result) = \\frac{\\exp(-5.61 + 0.04 \\times glucose)}{1 + \\exp(-5.61 + 0.04 \\times glucose)} \\]\n\n\nCalculating probabilities\nUsing the formula above, we can now calculate the probability of having a positive test result, for a given glucose value. If we do this for glucose = 150, we get the following:\n\nRPython\n\n\n\nexp(-5.61 + 0.04 * 150) / (1 + exp(-5.61 + 0.04 * 145))\n\n[1] 0.6685441\n\n\n\n\n\nmath.exp(-5.61 + 0.04 * 150) / (1 + math.exp(-5.61 + 0.04 * 145))\n\n0.6685441044999503\n\n\n\n\n\nThis tells us that the probability of having a positive diabetes test result, given a glucose tolerance level of 150 is around 67%."
  },
  {
    "objectID": "materials/stats-week_glm-practical-logistic-binary.html#summary",
    "href": "materials/stats-week_glm-practical-logistic-binary.html#summary",
    "title": "11  Binary response",
    "section": "11.7 Summary",
    "text": "11.7 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nWe use a logistic regression to model a binary response\nWe can feed new observations into the model and get probabilities for the outcome"
  }
]