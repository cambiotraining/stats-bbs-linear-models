---
title: "Generalising your model"
---
```{r}
#| echo: false
#| message: false
#| results: hide
source(file = "setup_files/setup.R")
```

```{python}
#| echo: false
#| message: false
import shutup;shutup.please()
exec(open('setup_files/setup.py').read())
```

```{r}
#| echo: false
#| message: false

# required data sets
finches <- read_csv("data/finches_beaks.csv")

fortis_1975 <- finches %>% filter(species == "fortis", year == 1975)
```

## Putting the "G" into GLM

In the previous linear model example all the assumptions were met. But what if we have data where that isn't the case? For example, what if we have data where we _can't_ describe the relationship between the predictor and response variables in a linear way?

One of the ways we can deal with this is by using a **generalised linear model**, also abbreviated as GLM. In a way it's an extension of the linear model we discussed in the previous section. As with the normal linear model, the predictor variables in the model are in a linear combination, such as:

$$
\beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + ...
$$

Here, the $\beta_0$ value is the constant or intercept, whereas each subsequent $\beta_i$ is a unique regression coefficient for each $X_i$ predictor variable. So far so good.

However, the GLM makes the linear model more flexible in two ways:

:::{.callout-important}
1. In a standard linear model the linear combination (e.g. like we see above) becomes the predicted outcome value. With a GLM a transformation is specified, which turns the linear combination into the predicted outcome value. This is called a **link function**.
2. A standard linear model assumes a continuous, normally distributed outcome, whereas **with GLM the outcome can be both continuous or integer**. Furthermore, the outcome does not have to be normally distributed. Indeed, **the outcome can follow a different kind of distribution**, such as binomial, Poisson, exponential etc.
:::

We'll introduce each of these elements below, then illustrate how they are used in practice, using different types of data.

The link function and different distributions are closely...err, _linked_. To make sense of what the link function is doing it's useful to understand the different distributional assumptions. So we'll start with those.

## Distributions

In the examples of a standard linear model we've seen that the residuals needed to be normally distributed. We've mainly used the Q-Q plot to assess this assumption of normality.

But what does "normal" actually mean? It assumes that the residuals are coming from a normal or Gaussian distribution. This distribution has a symmetrical bell-shape, where the centre is the mean, and half of the data are on either side of this.

We can see this in @fig-normdist. The mean of the normal distribution is indicated with the dashed blue line.

```{r}
#| echo: false
mean_resid <- fortis_1975 %>% 
  lm(bdepth ~ blength, data = .) %>%
  resid() %>%
  as_tibble() %>%
  summarise(mean = mean(value)) %>% 
  pull()

sd_resid <- fortis_1975 %>% 
  lm(bdepth ~ blength, data = .) %>%
  resid() %>%
  as_tibble() %>%
  summarise(sd = sd(value)) %>% 
  pull()
```

```{r}
#| echo: false
p1 <- fortis_1975 %>% 
  lm(bdepth ~ blength, data = .) %>%
  resid() %>%
  as_tibble() %>%
  #rstatix::get_summary_stats()
  ggplot(aes(x = value)) +
  stat_dist_halfeye(aes(dist = dist_normal(mean_resid, sd_resid)),
                    orientation = "horizontal") +
  geom_vline(xintercept = mean_resid, colour = "blue", linetype = "dashed") +
  labs(title = "A normal distribution",
       subtitle = "with mean indicated",
       x = NULL,
       y = "probability")
```

```{r}
#| echo: false
p2 <- fortis_1975 %>% 
  lm(bdepth ~ blength, data = .) %>%
  resid() %>%
  as_tibble() %>%
  #rstatix::get_summary_stats()
  ggplot(aes(x = value)) +
  stat_dist_halfeye(aes(dist = dist_normal(mean_resid, sd_resid)),
                    orientation = "horizontal") +
 stat_dotsinterval(aes(x = value),
                    orientation = "horizontal",
                    fill = "firebrick", scale = 1) +
  geom_vline(xintercept = mean_resid, colour = "blue", linetype = "dashed") +
  labs(title = "G. fortis (1975)",
       subtitle = "residuals and normal distribution",
       x = NULL,
       y = "probability")
```

```{r}
#| echo: false
#| label: fig-normdist
#| fig-cap: "Normal distribution"
p1 / p2 +
  plot_annotation(tag_levels = "A")
```

We can use the linear model we created previously, where we looked at the possible linear relationship between beak depth and beak length. This is based on measurements of _G. fortis_ beaks in 1975.

The individual values of the residuals from this linear model are shown in @fig-normdist, panel B (in red), with the corresponding theoretical normal distribution in the background. We can see that the residuals follow this distribution reasonably well, which matches our conclusions from the Q-Q plot (see @fig-fortis1975_lm_dgplots).

All this means is that assuming that these residuals may come from a normal distribution isn't such a daft suggestion after all.

Now look at the example in @fig-beak_shape. This shows the classification of beak shape for a number of finches. Their beaks are either classed as `blunt` or `pointed`. Various (continuous) measurements were taken from each bird, with the beak length shown here.

```{r}
#| echo: false
#| message: false
finches_2020 <- read_csv("data/41559_2020_1183_MOESM2_ESM.csv")
finches_2020 <- finches_2020 %>% 
  clean_names() %>% 
  mutate(timepoint = (str_detect(group, "Late")),
         timepoint = if_else(timepoint == FALSE, "early", "late"),
         timepoint = factor(timepoint))

early_finches <- finches_2020 %>% 
  filter(timepoint == "early") %>% 
  mutate(pointed_beak = if_else(str_detect(group, "blunt"), 0, 1))
```

```{r}
#| echo: false
p3 <- ggplot(early_finches,
       aes(x = blength, y = pointed_beak)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "beak length (mm)",
       y = "beak shape") +
  scale_y_continuous(breaks = c(0,1), labels = c("blunt", "pointed"))
```


```{r}
#| echo: false
lm_early <- lm(pointed_beak ~ bdepth,
   data = early_finches)

mean_resid_early <- lm_early %>% 
  resid() %>%
  as_tibble() %>%
  summarise(mean = mean(value)) %>% 
  pull()

sd_resid_early <- lm_early %>% 
  resid() %>%
  as_tibble() %>%
  summarise(sd = sd(value)) %>% 
  pull()

p4 <- lm_early %>% 
  resid() %>%
  as_tibble() %>%
  #rstatix::get_summary_stats()
  ggplot(aes(x = value)) +
  stat_dist_halfeye(aes(dist = dist_normal(mean_resid_early,
                                           sd_resid_early)),
                    orientation = "horizontal") +
  stat_dotsinterval(aes(x = value),
                    orientation = "horizontal",
                    fill = "firebrick", scale = 1) +
  geom_vline(xintercept = mean_resid_early,
             colour = "blue", linetype = "dashed") +
  labs(title = "default residuals",
       x = NULL,
       y = "probability")
```


```{r}
#| echo: false
#| message: false
#| label: fig-beak_shape
#| fig-cap: "Classification in beak shape"
(p3 + p4) +
  plot_annotation(tag_levels = "A")
```

We'll look into this example in more detail later. For now it's important to note that the response variable (the beak shape classification) is not continuous. Here it is a binary response (`blunt` or `pointed`). As a result, the assumptions for a regular linear model go out of the window. If we were foolish enough to fit a linear model to these data (see blue line in A), then the residuals would look rather non-normal (@fig-beak_shape B).

So what do we do? Well, the normal distribution is not the only one there is. In @fig-dist_examples there are a few examples of distributions (including the normal one).

```{r}
#| echo: false
#| label: fig-dist_examples
#| fig-cap: Different distributions

df <- data.frame()

pl_norm <- ggplot(df) +
  stat_halfeye(aes(xdist = dist_normal(0, 1))) +
  labs(title = "normal distribution")

pl_log <- ggplot(df) +
  stat_histinterval(aes(xdist = dist_logistic(0, 1))) +
  labs(title = "logistic distribution")


pl_pois <- ggplot(df) +
    stat_histinterval(aes(xdist = dist_poisson(10))) +
  labs(title = "poisson distribution")

pl_exp <- ggplot(df) +
  stat_halfeye(aes(xdist = dist_exponential(10))) +
  labs(title = "exponential distribution")


(pl_norm + pl_log) / (pl_pois + pl_exp)
```

Different distributions are useful for different types of data. For example, a logistic distribution is particularly useful in the context of binary or proportional response data. The Poisson distribution is useful when we have count data as a response.

In order to understand how this can help us, we need to be aware of two more concepts: **linear predictors** and **link functions**.

## Linear predictors

The nice thing about linear models is that the predictors are, well, linear. Straight lines make for easy interpretation of any potential relationship between predictor and response.

As mentioned before, predictors are in the form of a _linear combination_, where each predictor variable is multiplied by a coefficient and all the terms are added together:

$$
\beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + ...
$$

Fortunately, this is no different for generalised linear models! We still have a linear combination but, as we'll see, if the relationship is not linear then we need an additional step before we can model the data in this way.

At this point, we have two options at our disposal (well, there are more, but let's not muddy the waters too much).

:::{.callout-important}

1. Transform our data and use a normal linear model on the transformed data
2. Transform the linear predictor
:::

The first option, to **transform our data**, seems like a useful option and can work. It keeps things familiar (we'd still use a standard linear model) and so all is well with the world. Up to the point of interpreting the data. If we, for example, log-transform our data, how do we interpret this? After all, the predictions of the linear model are directly related to the outcome or response variable. Transforming the data is usually done so that the residuals of the linear model resemble a more normal distribution. An unwanted side-effect of this is that this also changes the ratio scale properties of the measured variables [@stevens1946].

The second option would be to **transform the linear predictor**. This enables us to map a *non-linear* outcome (or response variable) to a *linear* model. This transformation is done using a **link function**.

## Link functions

Simply put: link functions connect the predictors in our model to the response variables in a linear way. 

However, and similar to the standard linear model, there are two parts to each model:

1. the coefficients for each predictor (linking each parameter to a predictor)
2. the error or random component (which specifies a probability distribution)

Which link function you use depends on your analysis. Some common link functions and corresponding distributions are (adapted from [@glen2021]):

| distribution | data type           | link name |
|--------------|---------------------|-----------|
| binomial     | binary / proportion | logit     |
| normal       | any real number     | identity  |
| poisson      | count data          | log       |


Let's again look at the earlier example of beak shape.

```{r}
#| echo: false
#| message: false
#| label: fig-beak_shapeclass
#| fig-cap: "Beak shape classification"

p3a <- ggplot(early_finches,
       aes(x = blength, y = pointed_beak)) +
  geom_point() +
  labs(x = "beak length (mm)",
       y = "beak shape") +
  scale_y_continuous(breaks = c(0,1), labels = c("blunt", "pointed"))

p5 <- ggplot(early_finches, aes(x = blength, y = pointed_beak)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
    labs(x = "beak length (mm)",
         y = "probability of having a pointed beak")

(p3a + p5) +
  plot_annotation(tag_levels = "A")
```

We've seen the data in @fig-beak_shapeclass A before, where we had information on what beak shape our observed finches had, plotted against their beak length.

Let's say we now want to make some **predictions** about what beak shape we would expect, *given* a certain beak length. In this scenario we'd need some way of modelling the response variable (beak shape; `blunt` or `pointed`) as a function of the predictor variable (beak length).

The issue we have is that the response variable is not continuous, but binary! We could fit a standard linear model to these data (blue line in @fig-beak_shape A) but this is really bad practice. Why? Well, what such a linear model represents is *the probability* - or how likely it is - that an observed finch has a pointed beak, given a certain beak length (@fig-beak_shapeclass B).

Simply fitting a linear line through those data suggests that it is possible to have a higher than 1 and lower-than-zero probability that a beak would be pointed! That, of course, makes no sense. So, we can't describe these data as a linear relationship.

Instead, we'll use a *logistic model* to analyse these data. We'll cover the practicalities of how to do this in more detail in a later chapter, but for now it's sufficient to realise that one of the ways we could model these data could look like this:

```{r}
#| echo: false
#| message: false
#| label: fig-beak_shape_glm
#| fig-cap: "Logistic model for beak classification"
ggplot(early_finches, aes(x = blength, y = pointed_beak)) +
  geom_point() +
  geom_smooth(method = "glm",
              method.args = list(family = "binomial"),
              se = FALSE) +
  labs(x = "beak length (mm)",
       y = "probability of having a pointed beak")
```

Using this sigmoidal curve ensures that our predicted probabilities do not exceed the $[0, 1]$ range.

Now, what happened behind the scenes is that the generalised linear model has taken the linear predictor and transformed it using the *logit* link function. This links the  non-linear response variable (beak shape) to a linear model, using beak length as a predictor.

We'll practice how to perform this analysis in the next section.

## Summary

::: {.callout-tip}
#### Key points

- GLMs allow us to map a non-linear outcome to a linear model
- The link function determines *how* this occurs, transforming the linear predictor
:::
